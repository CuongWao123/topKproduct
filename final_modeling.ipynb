{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbffbed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Viz & Regular Expression Libraries :\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# Scikit-Learn ML Libraries :\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Boosting Algorithm Libraries :\n",
    "\n",
    "# from xgboost                          import XGBClassifier\n",
    "# from catboost                         import CatBoostClassifier\n",
    "# from lightgbm                         import LGBMClassifier\n",
    "from sklearn.ensemble                 import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics                  import accuracy_score\n",
    "from sklearn.model_selection          import StratifiedKFold,KFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004f7d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEGION\\AppData\\Local\\Temp\\ipykernel_9544\\1227308709.py:1: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/processed/train.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/processed/train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847f5c3",
   "metadata": {},
   "source": [
    "# Reduce memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b059ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0078533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 781.94 Mb (53.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = reduce_memory_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "463575dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4606311 entries, 0 to 4606310\n",
      "Data columns (total 48 columns):\n",
      " #   Column                                Dtype  \n",
      "---  ------                                -----  \n",
      " 0   date                                  object \n",
      " 1   customer_id                           int32  \n",
      " 2   employee_index                        object \n",
      " 3   country_of_residence                  object \n",
      " 4   gender                                object \n",
      " 5   age                                   object \n",
      " 6   registration_date                     object \n",
      " 7   new_customer                          float16\n",
      " 8   seniority                             object \n",
      " 9   primary_customer                      float16\n",
      " 10  last_primary_date                     object \n",
      " 11  customer_type                         object \n",
      " 12  relation_type                         object \n",
      " 13  residence_index                       object \n",
      " 14  foreigner_index                       object \n",
      " 15  spouse_index                          object \n",
      " 16  channel                               object \n",
      " 17  deceased_index                        object \n",
      " 18  address_type                          float16\n",
      " 19  province_code                         float16\n",
      " 20  province_name                         object \n",
      " 21  activity_index                        float16\n",
      " 22  income                                float32\n",
      " 23  segment                               object \n",
      " 24  savings_account_final_label           int8   \n",
      " 25  guarantees_final_label                int8   \n",
      " 26  current_accounts_final_label          int8   \n",
      " 27  deriv_investments_final_label         int8   \n",
      " 28  payroll_accounts_final_label          int8   \n",
      " 29  junior_accounts_final_label           int8   \n",
      " 30  more_particular_accounts_final_label  int8   \n",
      " 31  particular_accounts_final_label       int8   \n",
      " 32  particular_plus_accounts_final_label  int8   \n",
      " 33  short_term_deposits_final_label       int8   \n",
      " 34  medium_term_deposits_final_label      int8   \n",
      " 35  long_term_deposits_final_label        int8   \n",
      " 36  e_account_final_label                 int8   \n",
      " 37  funds_final_label                     int8   \n",
      " 38  mortgage_final_label                  int8   \n",
      " 39  pensions_final_label                  int8   \n",
      " 40  loans_final_label                     int8   \n",
      " 41  taxes_final_label                     int8   \n",
      " 42  credit_card_final_label               int8   \n",
      " 43  securities_final_label                int8   \n",
      " 44  home_account_final_label              int8   \n",
      " 45  payroll_final_label                   int8   \n",
      " 46  pensions_2_final_label                int8   \n",
      " 47  direct_debit_final_label              int8   \n",
      "dtypes: float16(5), float32(1), int32(1), int8(24), object(17)\n",
      "memory usage: 781.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c630c3",
   "metadata": {},
   "source": [
    "# Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e845db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the banking dataset\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame - Raw dataset\n",
    "    \n",
    "    Returns:\n",
    "    df: pandas DataFrame - Cleaned dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üßπ CLEANING DATASET...\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # =============================\n",
    "    # 1. HANDLE MISSING VALUES\n",
    "    # =============================\n",
    "    print(\"1Ô∏è‚É£ Handling missing values...\")\n",
    "    \n",
    "    # Fill missing values for payroll indicators\n",
    "    # These columns indicate if customer has payroll/pension products\n",
    "    missing_before = df[['payroll_final_label', 'pensions_2_final_label']].isnull().sum()\n",
    "    df.fillna(value={\n",
    "        'payroll_final_label': 0,\n",
    "        'pensions_2_final_label': 0\n",
    "    }, inplace=True)\n",
    "    \n",
    "    print(f\"   ‚úÖ Filled payroll_final_label: {missing_before['payroll_final_label']} missing ‚Üí 0\")\n",
    "    print(f\"   ‚úÖ Filled pensions_2_final_label: {missing_before['pensions_2_final_label']} missing ‚Üí 0\")\n",
    "    \n",
    "    # =============================\n",
    "    # 2. CREATE CUSTOMER TENURE FEATURE\n",
    "    # =============================\n",
    "    print(\"\\n2Ô∏è‚É£ Creating customer tenure feature...\")\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['registration_date'] = pd.to_datetime(df['registration_date'])\n",
    "    \n",
    "    # Calculate days since registration (customer tenure)\n",
    "    days_column = (df['date'] - df['registration_date']).dt.days\n",
    "    \n",
    "    # Insert the new column at position 6\n",
    "    df.insert(loc=6, column='customer_tenure_days', value=days_column)\n",
    "    \n",
    "    print(f\"   ‚úÖ Created 'customer_tenure_days' feature\")\n",
    "    print(f\"   üìä Range: {days_column.min()} to {days_column.max()} days\")\n",
    "    \n",
    "    # Drop the original registration_date column to save memory\n",
    "    df.drop(columns=['registration_date'], inplace=True)\n",
    "    print(f\"   üóëÔ∏è Dropped 'registration_date' column\")\n",
    "    \n",
    "    # =============================\n",
    "    # 3. CONVERT LAST_PRIMARY_DATE TO BINARY INDICATOR\n",
    "    # =============================\n",
    "    print(\"\\n3Ô∏è‚É£ Converting last_primary_date to binary indicator...\")\n",
    "    \n",
    "    # Convert last_primary_date to binary: 1 if date exists, 0 if null\n",
    "    # This indicates if customer was ever a primary customer\n",
    "    original_nulls = df['last_primary_date'].isnull().sum()\n",
    "    df['was_primary_customer'] = df['last_primary_date'].apply(\n",
    "        lambda x: 1 if pd.notnull(x) else 0\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Created 'was_primary_customer' binary feature\")\n",
    "    print(f\"   üìä {original_nulls:,} nulls ‚Üí 0, {len(df) - original_nulls:,} dates ‚Üí 1\")\n",
    "    \n",
    "    # Drop the original last_primary_date column\n",
    "    df.drop(columns=['last_primary_date'], inplace=True)\n",
    "    print(f\"   üóëÔ∏è Dropped 'last_primary_date' column\")\n",
    "    \n",
    "    # =============================\n",
    "    # 4. REMOVE CONSTANT/DUPLICATE COLUMNS\n",
    "    # =============================\n",
    "    print(\"\\n4Ô∏è‚É£ Removing constant and duplicate columns...\")\n",
    "    \n",
    "    # Remove address_type if it has the same value for all customers\n",
    "    if 'address_type' in df.columns:\n",
    "        unique_values = df['address_type'].nunique()\n",
    "        if unique_values <= 1:\n",
    "            df.drop(columns=['address_type'], inplace=True)\n",
    "            print(f\"   üóëÔ∏è Dropped 'address_type' (constant value)\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Kept 'address_type' ({unique_values} unique values)\")\n",
    "    \n",
    "    # Remove province_code as it's duplicate of province_name\n",
    "    if 'province_code' in df.columns and 'province_name' in df.columns:\n",
    "        df.drop(columns=['province_code'], inplace=True)\n",
    "        print(f\"   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\")\n",
    "    \n",
    "    # =============================\n",
    "    # 5. CLEAN NUMERIC COLUMNS\n",
    "    # =============================\n",
    "    print(\"\\n5Ô∏è‚É£ Cleaning numeric columns...\")\n",
    "    \n",
    "    # Convert age column - handle 'NA' strings\n",
    "    if 'age' in df.columns:\n",
    "        age_before = df['age'].dtype\n",
    "        df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "        age_nulls = df['age'].isnull().sum()\n",
    "        print(f\"   ‚úÖ Cleaned 'age': {age_before} ‚Üí numeric ({age_nulls:,} nulls)\")\n",
    "    \n",
    "    # Convert seniority column - handle 'NA' strings and negative values\n",
    "    if 'seniority' in df.columns:\n",
    "        seniority_before = df['seniority'].dtype\n",
    "        df['seniority'] = pd.to_numeric(df['seniority'], errors='coerce')\n",
    "        \n",
    "        # Handle special negative values (often -999999 means missing)\n",
    "        negative_values = (df['seniority'] < 0).sum()\n",
    "        if negative_values > 0:\n",
    "            df['seniority'] = df['seniority'].where(df['seniority'] >= 0, np.nan)\n",
    "            print(f\"   ‚ö†Ô∏è Converted {negative_values:,} negative seniority values to NaN\")\n",
    "        \n",
    "        seniority_nulls = df['seniority'].isnull().sum()\n",
    "        print(f\"   ‚úÖ Cleaned 'seniority': {seniority_before} ‚Üí numeric ({seniority_nulls:,} nulls)\")\n",
    "    \n",
    "    # =============================\n",
    "    # 6. SUMMARY STATISTICS\n",
    "    # =============================\n",
    "    print(\"\\nüìä CLEANUP SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"   Final shape: {df.shape}\")\n",
    "    print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    print(f\"   Null values: {df.isnull().sum().sum():,}\")\n",
    "    \n",
    "    # Show data types\n",
    "    print(f\"\\nüìã DATA TYPES AFTER CLEANUP:\")\n",
    "    for dtype in df.dtypes.value_counts().index:\n",
    "        count = df.dtypes.value_counts()[dtype]\n",
    "        print(f\"   {dtype}: {count} columns\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "839948fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ CLEANING DATASET...\n",
      "========================================\n",
      "1Ô∏è‚É£ Handling missing values...\n",
      "   ‚úÖ Filled payroll_final_label: 0 missing ‚Üí 0\n",
      "   ‚úÖ Filled pensions_2_final_label: 0 missing ‚Üí 0\n",
      "\n",
      "2Ô∏è‚É£ Creating customer tenure feature...\n",
      "   ‚úÖ Created 'customer_tenure_days' feature\n",
      "   üìä Range: -3.0 to 7498.0 days\n",
      "   üóëÔ∏è Dropped 'registration_date' column\n",
      "\n",
      "3Ô∏è‚É£ Converting last_primary_date to binary indicator...\n",
      "   ‚úÖ Created 'was_primary_customer' binary feature\n",
      "   üìä 4,600,165 nulls ‚Üí 0, 6,146 dates ‚Üí 1\n",
      "   üóëÔ∏è Dropped 'last_primary_date' column\n",
      "\n",
      "4Ô∏è‚É£ Removing constant and duplicate columns...\n",
      "   üóëÔ∏è Dropped 'address_type' (constant value)\n",
      "   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\n",
      "\n",
      "5Ô∏è‚É£ Cleaning numeric columns...\n",
      "   ‚úÖ Cleaned 'age': object ‚Üí numeric (27,734 nulls)\n",
      "   ‚ö†Ô∏è Converted 14 negative seniority values to NaN\n",
      "   ‚úÖ Cleaned 'seniority': object ‚Üí numeric (27,748 nulls)\n",
      "\n",
      "üìä CLEANUP SUMMARY:\n",
      "----------------------------------------\n",
      "   Final shape: (4606311, 46)\n",
      "   Memory usage: 2935.7 MB\n",
      "   Null values: 5,979,487\n",
      "\n",
      "üìã DATA TYPES AFTER CLEANUP:\n",
      "   int8: 24 columns\n",
      "   object: 12 columns\n",
      "   float64: 3 columns\n",
      "   float16: 3 columns\n",
      "   datetime64[ns]: 1 columns\n",
      "   int32: 1 columns\n",
      "   float32: 1 columns\n",
      "   int64: 1 columns\n"
     ]
    }
   ],
   "source": [
    "df = clean_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b2c03a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4606311 entries, 0 to 4606310\n",
      "Data columns (total 46 columns):\n",
      " #   Column                                Dtype         \n",
      "---  ------                                -----         \n",
      " 0   date                                  datetime64[ns]\n",
      " 1   customer_id                           int32         \n",
      " 2   employee_index                        object        \n",
      " 3   country_of_residence                  object        \n",
      " 4   gender                                object        \n",
      " 5   age                                   float64       \n",
      " 6   customer_tenure_days                  float64       \n",
      " 7   new_customer                          float16       \n",
      " 8   seniority                             float64       \n",
      " 9   primary_customer                      float16       \n",
      " 10  customer_type                         object        \n",
      " 11  relation_type                         object        \n",
      " 12  residence_index                       object        \n",
      " 13  foreigner_index                       object        \n",
      " 14  spouse_index                          object        \n",
      " 15  channel                               object        \n",
      " 16  deceased_index                        object        \n",
      " 17  province_name                         object        \n",
      " 18  activity_index                        float16       \n",
      " 19  income                                float32       \n",
      " 20  segment                               object        \n",
      " 21  savings_account_final_label           int8          \n",
      " 22  guarantees_final_label                int8          \n",
      " 23  current_accounts_final_label          int8          \n",
      " 24  deriv_investments_final_label         int8          \n",
      " 25  payroll_accounts_final_label          int8          \n",
      " 26  junior_accounts_final_label           int8          \n",
      " 27  more_particular_accounts_final_label  int8          \n",
      " 28  particular_accounts_final_label       int8          \n",
      " 29  particular_plus_accounts_final_label  int8          \n",
      " 30  short_term_deposits_final_label       int8          \n",
      " 31  medium_term_deposits_final_label      int8          \n",
      " 32  long_term_deposits_final_label        int8          \n",
      " 33  e_account_final_label                 int8          \n",
      " 34  funds_final_label                     int8          \n",
      " 35  mortgage_final_label                  int8          \n",
      " 36  pensions_final_label                  int8          \n",
      " 37  loans_final_label                     int8          \n",
      " 38  taxes_final_label                     int8          \n",
      " 39  credit_card_final_label               int8          \n",
      " 40  securities_final_label                int8          \n",
      " 41  home_account_final_label              int8          \n",
      " 42  payroll_final_label                   int8          \n",
      " 43  pensions_2_final_label                int8          \n",
      " 44  direct_debit_final_label              int8          \n",
      " 45  was_primary_customer                  int64         \n",
      "dtypes: datetime64[ns](1), float16(3), float32(1), float64(3), int32(1), int64(1), int8(24), object(12)\n",
      "memory usage: 764.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ada4d2",
   "metadata": {},
   "source": [
    "# Filter data for TypeI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7c47765",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_account_labels = [\n",
    "    'current_accounts_final_label',\n",
    "    'payroll_accounts_final_label',\n",
    "    'junior_accounts_final_label',\n",
    "    'more_particular_accounts_final_label',\n",
    "    'particular_accounts_final_label',\n",
    "    'particular_plus_accounts_final_label',\n",
    "    'home_account_final_label',\n",
    "    'payroll_final_label',\n",
    "    'e_account_final_label'\n",
    "]\n",
    "\n",
    "customer_features = [\n",
    "    'date', 'customer_id', 'employee_index', 'country_of_residence', 'gender',\n",
    "    'age', 'customer_tenure_days', 'seniority', 'residence_index',  # Added customer_tenure_days\n",
    "    'foreigner_index', 'spouse_index', 'channel', 'deceased_index', \n",
    "    'province_name', 'segment', 'was_primary_customer'  # Added was_primary_customer\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ee029c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df) :\n",
    "    # L·∫•y kh√°ch h√†ng kh√¥ng c√≥ t√†i kho·∫£n thanh to√°n n√†o (kh√¥ng c√≥ gi√° tr·ªã -1)\n",
    "    mask = ~(df[payment_account_labels] == -1).any(axis=1)\n",
    "    \n",
    "    # C√°c c·ªôt c·∫ßn gi·ªØ l·∫°i\n",
    "    columns_to_keep = customer_features + payment_account_labels\n",
    "    \n",
    "    # L·ªçc d·ªØ li·ªáu\n",
    "    df = df.loc[mask, columns_to_keep]\n",
    "    \n",
    "    print(f\"T·ªïng s·ªë kh√°ch h√†ng: {len(df):,}\")\n",
    "    print(f\"Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: {len(df):,}\")\n",
    "    \n",
    "    # Ki·ªÉm tra distribution\n",
    "    print(\"\\nüìä Distribution check:\")\n",
    "    for col in payment_account_labels:\n",
    "        unique_vals = df[col].unique()\n",
    "        print(f\"  {col}: {unique_vals}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0597248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë kh√°ch h√†ng: 442,736\n",
      "Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: 442,736\n",
      "\n",
      "üìä Distribution check:\n",
      "  current_accounts_final_label: [0 1]\n",
      "  payroll_accounts_final_label: [0 1]\n",
      "  junior_accounts_final_label: [0 1]\n",
      "  more_particular_accounts_final_label: [0 1]\n",
      "  particular_accounts_final_label: [0 1]\n",
      "  particular_plus_accounts_final_label: [0 1]\n",
      "  home_account_final_label: [0]\n",
      "  payroll_final_label: [0 1]\n",
      "  e_account_final_label: [0 1]\n"
     ]
    }
   ],
   "source": [
    "df = filter_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8e03aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 442736 entries, 3 to 4606305\n",
      "Data columns (total 25 columns):\n",
      " #   Column                                Non-Null Count   Dtype         \n",
      "---  ------                                --------------   -----         \n",
      " 0   date                                  442736 non-null  datetime64[ns]\n",
      " 1   customer_id                           442736 non-null  int32         \n",
      " 2   employee_index                        415002 non-null  object        \n",
      " 3   country_of_residence                  415002 non-null  object        \n",
      " 4   gender                                414987 non-null  object        \n",
      " 5   age                                   415002 non-null  float64       \n",
      " 6   customer_tenure_days                  415002 non-null  float64       \n",
      " 7   seniority                             414995 non-null  float64       \n",
      " 8   residence_index                       415002 non-null  object        \n",
      " 9   foreigner_index                       415002 non-null  object        \n",
      " 10  spouse_index                          72 non-null      object        \n",
      " 11  channel                               407282 non-null  object        \n",
      " 12  deceased_index                        415002 non-null  object        \n",
      " 13  province_name                         411009 non-null  object        \n",
      " 14  segment                               406937 non-null  object        \n",
      " 15  was_primary_customer                  442736 non-null  int64         \n",
      " 16  current_accounts_final_label          442736 non-null  int8          \n",
      " 17  payroll_accounts_final_label          442736 non-null  int8          \n",
      " 18  junior_accounts_final_label           442736 non-null  int8          \n",
      " 19  more_particular_accounts_final_label  442736 non-null  int8          \n",
      " 20  particular_accounts_final_label       442736 non-null  int8          \n",
      " 21  particular_plus_accounts_final_label  442736 non-null  int8          \n",
      " 22  home_account_final_label              442736 non-null  int8          \n",
      " 23  payroll_final_label                   442736 non-null  int8          \n",
      " 24  e_account_final_label                 442736 non-null  int8          \n",
      "dtypes: datetime64[ns](1), float64(3), int32(1), int64(1), int8(9), object(10)\n",
      "memory usage: 59.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1c3f30",
   "metadata": {},
   "source": [
    "# Clean memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dd279b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ MEMORY CLEANUP...\n",
      "========================================\n",
      "   üóëÔ∏è Garbage collected: 2127 objects\n",
      "   üíæ Current memory usage: 1390.0 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gc\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"\n",
    "    Clean up unused memory and optimize DataFrame\n",
    "    \"\"\"\n",
    "    print(\"üßπ MEMORY CLEANUP...\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Force garbage collection\n",
    "    collected = gc.collect()\n",
    "    print(f\"   üóëÔ∏è Garbage collected: {collected} objects\")\n",
    "    \n",
    "    # Get memory info\n",
    "    import psutil\n",
    "    import os\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    memory_mb = memory_info.rss / 1024 / 1024\n",
    "    \n",
    "    print(f\"   üíæ Current memory usage: {memory_mb:.1f} MB\")\n",
    "    \n",
    "    return memory_mb\n",
    "\n",
    "memory_before = cleanup_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a050f5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 442736 entries, 3 to 4606305\n",
      "Data columns (total 25 columns):\n",
      " #   Column                                Non-Null Count   Dtype         \n",
      "---  ------                                --------------   -----         \n",
      " 0   date                                  442736 non-null  datetime64[ns]\n",
      " 1   customer_id                           442736 non-null  int32         \n",
      " 2   employee_index                        415002 non-null  object        \n",
      " 3   country_of_residence                  415002 non-null  object        \n",
      " 4   gender                                414987 non-null  object        \n",
      " 5   age                                   415002 non-null  float64       \n",
      " 6   customer_tenure_days                  415002 non-null  float64       \n",
      " 7   seniority                             414995 non-null  float64       \n",
      " 8   residence_index                       415002 non-null  object        \n",
      " 9   foreigner_index                       415002 non-null  object        \n",
      " 10  spouse_index                          72 non-null      object        \n",
      " 11  channel                               407282 non-null  object        \n",
      " 12  deceased_index                        415002 non-null  object        \n",
      " 13  province_name                         411009 non-null  object        \n",
      " 14  segment                               406937 non-null  object        \n",
      " 15  was_primary_customer                  442736 non-null  int64         \n",
      " 16  current_accounts_final_label          442736 non-null  int8          \n",
      " 17  payroll_accounts_final_label          442736 non-null  int8          \n",
      " 18  junior_accounts_final_label           442736 non-null  int8          \n",
      " 19  more_particular_accounts_final_label  442736 non-null  int8          \n",
      " 20  particular_accounts_final_label       442736 non-null  int8          \n",
      " 21  particular_plus_accounts_final_label  442736 non-null  int8          \n",
      " 22  home_account_final_label              442736 non-null  int8          \n",
      " 23  payroll_final_label                   442736 non-null  int8          \n",
      " 24  e_account_final_label                 442736 non-null  int8          \n",
      "dtypes: datetime64[ns](1), float64(3), int32(1), int64(1), int8(9), object(10)\n",
      "memory usage: 59.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af1a9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·ªôt target (label) - t√†i kho·∫£n thanh to√°n\n",
    "tar_cols = [\n",
    "    'current_accounts_final_label',\n",
    "    'payroll_accounts_final_label',\n",
    "    'junior_accounts_final_label',\n",
    "    'more_particular_accounts_final_label',\n",
    "    'particular_accounts_final_label',\n",
    "    'particular_plus_accounts_final_label',\n",
    "    'home_account_final_label',\n",
    "    'payroll_final_label',\n",
    "    'e_account_final_label'\n",
    "]\n",
    "\n",
    "# C·ªôt s·ªë (numeric)\n",
    "num_cols = [\n",
    "    'age',\n",
    "    'seniority',\n",
    "    'new_customer',\n",
    "    'address_type',\n",
    "    'province_code',\n",
    "    'activity_index',\n",
    "    'income'\n",
    "]\n",
    "\n",
    "# C·ªôt ph√¢n lo·∫°i (categorical / object / category)\n",
    "cat_cols = [\n",
    "    'employee_index',\n",
    "    'country_of_residence',\n",
    "    'gender',\n",
    "    'primary_customer',\n",
    "    'customer_type',\n",
    "    'relation_type',\n",
    "    'residence_index',\n",
    "    'foreigner_index',\n",
    "    'spouse_index',\n",
    "    'channel',\n",
    "    'deceased_index',\n",
    "    'province_name',\n",
    "    'segment'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5529793",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2893fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking available columns before feature engineering:\n",
      "Available columns: ['date', 'customer_id', 'employee_index', 'country_of_residence', 'gender', 'age', 'customer_tenure_days', 'seniority', 'residence_index', 'foreigner_index', 'spouse_index', 'channel', 'deceased_index', 'province_name', 'segment', 'was_primary_customer', 'current_accounts_final_label', 'payroll_accounts_final_label', 'junior_accounts_final_label', 'more_particular_accounts_final_label', 'particular_accounts_final_label', 'particular_plus_accounts_final_label', 'home_account_final_label', 'payroll_final_label', 'e_account_final_label']\n",
      "üîß CREATING ENHANCED FEATURES...\n",
      "==================================================\n",
      "üìã Available columns: 25\n",
      "üìÖ Creating time-based features...\n",
      "üë• Creating demographic features...\n",
      "üè¶ Creating banking relationship features...\n",
      "üìä Creating product portfolio features...\n",
      "üîó Creating interaction features...\n",
      "üéØ Creating behavioral features...\n",
      "‚öñÔ∏è Creating risk and stability features...\n",
      "‚úÖ Feature engineering completed!\n",
      "Original features: 25\n",
      "Enhanced features: 66\n",
      "New features added: 41\n",
      "\n",
      "üîç CHECKING NEW FEATURES DATA TYPES...\n",
      "New features created: 41\n",
      "   year: int32\n",
      "   month: int32\n",
      "   quarter: int32\n",
      "   day_of_week: int32\n",
      "   is_weekend: int32\n",
      "   is_month_end: int32\n",
      "   is_quarter_end: int32\n",
      "   years_since_registration: float64\n",
      "   tenure_category: category\n",
      "   age_group: category\n",
      "   is_young_adult: int32\n",
      "   is_middle_aged: int32\n",
      "   is_senior: int32\n",
      "   age_squared: float64\n",
      "   seniority_years: float64\n",
      "\n",
      "üìä SAMPLE OF ENHANCED DATA:\n",
      "     year  month  quarter  day_of_week  is_weekend\n",
      "3    2015      1        1            2           0\n",
      "43   2015      1        1            2           0\n",
      "46   2015      1        1            2           0\n",
      "261  2015      1        1            2           0\n",
      "389  2015      1        1            2           0\n"
     ]
    }
   ],
   "source": [
    "def create_enhanced_features(df):\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    print(\"üîß CREATING ENHANCED FEATURES...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check available columns first\n",
    "    available_cols = df_enhanced.columns.tolist()\n",
    "    print(f\"üìã Available columns: {len(available_cols)}\")\n",
    "    \n",
    "    # =============================\n",
    "    # 1. TIME-BASED FEATURES\n",
    "    # =============================\n",
    "    print(\"üìÖ Creating time-based features...\")\n",
    "    \n",
    "    # Extract t·ª´ date column\n",
    "    if 'date' in df_enhanced.columns:\n",
    "        df_enhanced['year'] = df_enhanced['date'].dt.year\n",
    "        df_enhanced['month'] = df_enhanced['date'].dt.month\n",
    "        df_enhanced['quarter'] = df_enhanced['date'].dt.quarter\n",
    "        df_enhanced['day_of_week'] = df_enhanced['date'].dt.dayofweek\n",
    "        df_enhanced['is_weekend'] = (df_enhanced['day_of_week'] >= 5).astype(int)\n",
    "        df_enhanced['is_month_end'] = (df_enhanced['date'].dt.day >= 25).astype(int)\n",
    "        df_enhanced['is_quarter_end'] = df_enhanced['date'].dt.month.isin([3, 6, 9, 12]).astype(int)\n",
    "    \n",
    "    # Customer tenure features (using existing customer_tenure_days)\n",
    "    if 'customer_tenure_days' in df_enhanced.columns:\n",
    "        df_enhanced['years_since_registration'] = df_enhanced['customer_tenure_days'] / 365.25\n",
    "        \n",
    "        # Customer tenure categories\n",
    "        df_enhanced['tenure_category'] = pd.cut(\n",
    "            df_enhanced['customer_tenure_days'],\n",
    "            bins=[-1, 90, 365, 1095, 2190, np.inf],\n",
    "            labels=['Very_New', 'New', 'Medium', 'Long', 'Very_Long']\n",
    "        )\n",
    "    \n",
    "    # =============================\n",
    "    # 2. DEMOGRAPHIC FEATURES\n",
    "    # =============================\n",
    "    print(\"üë• Creating demographic features...\")\n",
    "    \n",
    "    # Age-based features\n",
    "    if 'age' in df_enhanced.columns:\n",
    "        df_enhanced['age_group'] = pd.cut(\n",
    "            df_enhanced['age'],\n",
    "            bins=[0, 25, 35, 45, 55, 65, 100],\n",
    "            labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+']\n",
    "        )\n",
    "        \n",
    "        age_young = (df_enhanced['age'] >= 18) & (df_enhanced['age'] <= 30)\n",
    "        age_middle = (df_enhanced['age'] >= 31) & (df_enhanced['age'] <= 50)\n",
    "        age_senior = (df_enhanced['age'] >= 60)\n",
    "        \n",
    "        df_enhanced['is_young_adult'] = age_young.fillna(False).astype(int)\n",
    "        df_enhanced['is_middle_aged'] = age_middle.fillna(False).astype(int)\n",
    "        df_enhanced['is_senior'] = age_senior.fillna(False).astype(int)\n",
    "        df_enhanced['age_squared'] = df_enhanced['age'] ** 2\n",
    "    \n",
    "    # =============================\n",
    "    # 3. BANKING RELATIONSHIP FEATURES  \n",
    "    # =============================\n",
    "    print(\"üè¶ Creating banking relationship features...\")\n",
    "    \n",
    "    # Seniority-based features\n",
    "    if 'seniority' in df_enhanced.columns:\n",
    "        df_enhanced['seniority_years'] = df_enhanced['seniority'] / 12\n",
    "        df_enhanced['seniority_category'] = pd.cut(\n",
    "            df_enhanced['seniority'],\n",
    "            bins=[-1, 0, 6, 12, 24, 60, np.inf],\n",
    "            labels=['New', 'Very_Short', 'Short', 'Medium', 'Long', 'Very_Long']\n",
    "        )\n",
    "        \n",
    "        seniority_new = df_enhanced['seniority'] <= 6\n",
    "        seniority_established = df_enhanced['seniority'] >= 24\n",
    "        \n",
    "        df_enhanced['is_new_relationship'] = seniority_new.fillna(False).astype(int)\n",
    "        df_enhanced['is_established_relationship'] = seniority_established.fillna(False).astype(int)\n",
    "    \n",
    "    # Customer status features (check if columns exist)\n",
    "    df_enhanced['is_employee'] = 0\n",
    "    if 'employee_index' in df_enhanced.columns:\n",
    "        df_enhanced['is_employee'] = (df_enhanced['employee_index'] == 1).astype(int)\n",
    "    \n",
    "    df_enhanced['is_primary_customer_flag'] = 0  \n",
    "    if 'was_primary_customer' in df_enhanced.columns:\n",
    "        df_enhanced['is_primary_customer_flag'] = df_enhanced['was_primary_customer']\n",
    "    \n",
    "    # Geographic features\n",
    "    if 'country_of_residence' in df_enhanced.columns:\n",
    "        domestic = df_enhanced['country_of_residence'] == 'ES'\n",
    "        df_enhanced['is_domestic'] = domestic.fillna(False).astype(int)\n",
    "    \n",
    "    if 'foreigner_index' in df_enhanced.columns:\n",
    "        foreigner = df_enhanced['foreigner_index'] == 1\n",
    "        df_enhanced['is_foreigner'] = foreigner.fillna(False).astype(int)\n",
    "    \n",
    "    # =============================\n",
    "    # 4. PRODUCT PORTFOLIO FEATURES\n",
    "    # =============================\n",
    "    print(\"üìä Creating product portfolio features...\")\n",
    "    \n",
    "    # Define tar_cols as payment_account_labels\n",
    "    tar_cols = payment_account_labels\n",
    "    \n",
    "    # Total products owned\n",
    "    df_enhanced['total_products'] = df_enhanced[tar_cols].sum(axis=1)\n",
    "    df_enhanced['has_any_product'] = (df_enhanced['total_products'] > 0).astype(int)\n",
    "    df_enhanced['is_single_product'] = (df_enhanced['total_products'] == 1).astype(int)\n",
    "    df_enhanced['is_multi_product'] = (df_enhanced['total_products'] > 1).astype(int)\n",
    "    \n",
    "    # Product diversity\n",
    "    df_enhanced['product_diversity_ratio'] = df_enhanced['total_products'] / len(tar_cols)\n",
    "    \n",
    "    # Product category features\n",
    "    current_products = ['current_accounts_final_label']\n",
    "    savings_products = ['payroll_accounts_final_label', 'junior_accounts_final_label']\n",
    "    premium_products = ['particular_plus_accounts_final_label', 'more_particular_accounts_final_label']\n",
    "    \n",
    "    df_enhanced['has_current_account'] = df_enhanced[current_products].sum(axis=1)\n",
    "    df_enhanced['has_savings_account'] = df_enhanced[savings_products].sum(axis=1)\n",
    "    df_enhanced['has_premium_account'] = df_enhanced[premium_products].sum(axis=1)\n",
    "    \n",
    "    # =============================\n",
    "    # 5. INTERACTION FEATURES\n",
    "    # =============================\n",
    "    print(\"üîó Creating interaction features...\")\n",
    "    \n",
    "    # Age-Seniority interactions\n",
    "    if 'age' in df_enhanced.columns and 'seniority' in df_enhanced.columns:\n",
    "        age_filled = df_enhanced['age'].fillna(0)\n",
    "        seniority_filled = df_enhanced['seniority'].fillna(0)\n",
    "        \n",
    "        df_enhanced['age_seniority_interaction'] = (age_filled * seniority_filled) / 100\n",
    "        df_enhanced['seniority_per_age'] = seniority_filled / (age_filled + 1)\n",
    "    \n",
    "    # Age-Tenure interactions\n",
    "    if 'age' in df_enhanced.columns and 'customer_tenure_days' in df_enhanced.columns:\n",
    "        age_filled = df_enhanced['age'].fillna(0)\n",
    "        tenure_filled = df_enhanced['customer_tenure_days'].fillna(0)\n",
    "        \n",
    "        df_enhanced['age_tenure_ratio'] = age_filled / (tenure_filled/365 + 1)\n",
    "    \n",
    "    # =============================\n",
    "    # 6. BEHAVIORAL FEATURES\n",
    "    # =============================\n",
    "    print(\"üéØ Creating behavioral features...\")\n",
    "    \n",
    "    # Channel preference\n",
    "    if 'channel' in df_enhanced.columns:\n",
    "        channel_mapping = {\n",
    "            'KAT': 'Traditional',\n",
    "            'KFC': 'Phone', \n",
    "            'KHE': 'Digital',\n",
    "            'KHM': 'Mobile',\n",
    "            'KHN': 'Online'\n",
    "        }\n",
    "        df_enhanced['channel_type'] = df_enhanced['channel'].map(channel_mapping).fillna('Other')\n",
    "        df_enhanced['is_digital_channel'] = df_enhanced['channel_type'].isin(['Digital', 'Mobile', 'Online']).astype(int)\n",
    "    \n",
    "    # Customer segment enhancement\n",
    "    if 'segment' in df_enhanced.columns:\n",
    "        df_enhanced['is_vip_segment'] = df_enhanced['segment'].str.contains('VIP', na=False).astype(int)\n",
    "        df_enhanced['is_university_segment'] = df_enhanced['segment'].str.contains('UNIVERSITY', na=False).astype(int)\n",
    "    \n",
    "    # =============================\n",
    "    # 7. RISK & STABILITY FEATURES\n",
    "    # =============================\n",
    "    print(\"‚öñÔ∏è Creating risk and stability features...\")\n",
    "    \n",
    "    # Customer stability score\n",
    "    stability_score = 0\n",
    "    \n",
    "    if 'seniority' in df_enhanced.columns:\n",
    "        seniority_stable = df_enhanced['seniority'] >= 12\n",
    "        stability_score += seniority_stable.fillna(False).astype(int)\n",
    "    \n",
    "    if 'age' in df_enhanced.columns:\n",
    "        age_stable = df_enhanced['age'] >= 30\n",
    "        stability_score += age_stable.fillna(False).astype(int)\n",
    "    \n",
    "    stability_score += df_enhanced['is_primary_customer_flag']\n",
    "    stability_score += df_enhanced['is_employee']\n",
    "    \n",
    "    df_enhanced['customer_stability_score'] = stability_score\n",
    "    df_enhanced['is_stable_customer'] = (stability_score >= 2).astype(int)\n",
    "    \n",
    "    # Potential value score\n",
    "    potential_score = 0\n",
    "    \n",
    "    if 'age' in df_enhanced.columns:\n",
    "        age_prime = (df_enhanced['age'] >= 25) & (df_enhanced['age'] <= 55)\n",
    "        potential_score += age_prime.fillna(False).astype(int)\n",
    "    \n",
    "    potential_score += df_enhanced['is_digital_channel'] if 'is_digital_channel' in df_enhanced.columns else 0\n",
    "    potential_score += df_enhanced['is_domestic'] if 'is_domestic' in df_enhanced.columns else 0\n",
    "    \n",
    "    df_enhanced['customer_potential_score'] = potential_score\n",
    "    df_enhanced['is_high_potential'] = (potential_score >= 2).astype(int)\n",
    "    \n",
    "    print(f\"‚úÖ Feature engineering completed!\")\n",
    "    print(f\"Original features: {df.shape[1]}\")\n",
    "    print(f\"Enhanced features: {df_enhanced.shape[1]}\")\n",
    "    print(f\"New features added: {df_enhanced.shape[1] - df.shape[1]}\")\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# =============================\n",
    "# APPLY FEATURE ENGINEERING\n",
    "# =============================\n",
    "print(\"üîç Checking available columns before feature engineering:\")\n",
    "print(f\"Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "df_enhanced = create_enhanced_features(df)\n",
    "\n",
    "# =============================\n",
    "# VERIFY DATA TYPES\n",
    "# =============================\n",
    "print(\"\\nüîç CHECKING NEW FEATURES DATA TYPES...\")\n",
    "new_features = [col for col in df_enhanced.columns if col not in df.columns]\n",
    "print(f\"New features created: {len(new_features)}\")\n",
    "\n",
    "for feature in new_features[:15]:  # Show first 15 new features\n",
    "    print(f\"   {feature}: {df_enhanced[feature].dtype}\")\n",
    "\n",
    "print(f\"\\nüìä SAMPLE OF ENHANCED DATA:\")\n",
    "if len(new_features) > 0:\n",
    "    sample_features = new_features[:5] if len(new_features) >= 5 else new_features\n",
    "    print(df_enhanced[sample_features].head())\n",
    "else:\n",
    "    print(\"No new features were created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01b09d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b31ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 442736 entries, 3 to 4606305\n",
      "Data columns (total 66 columns):\n",
      " #   Column                                Non-Null Count   Dtype         \n",
      "---  ------                                --------------   -----         \n",
      " 0   date                                  442736 non-null  datetime64[ns]\n",
      " 1   customer_id                           442736 non-null  int32         \n",
      " 2   employee_index                        415002 non-null  object        \n",
      " 3   country_of_residence                  415002 non-null  object        \n",
      " 4   gender                                414987 non-null  object        \n",
      " 5   age                                   415002 non-null  float64       \n",
      " 6   customer_tenure_days                  415002 non-null  float64       \n",
      " 7   seniority                             414995 non-null  float64       \n",
      " 8   residence_index                       415002 non-null  object        \n",
      " 9   foreigner_index                       415002 non-null  object        \n",
      " 10  spouse_index                          72 non-null      object        \n",
      " 11  channel                               407282 non-null  object        \n",
      " 12  deceased_index                        415002 non-null  object        \n",
      " 13  province_name                         411009 non-null  object        \n",
      " 14  segment                               406937 non-null  object        \n",
      " 15  was_primary_customer                  442736 non-null  int64         \n",
      " 16  current_accounts_final_label          442736 non-null  int8          \n",
      " 17  payroll_accounts_final_label          442736 non-null  int8          \n",
      " 18  junior_accounts_final_label           442736 non-null  int8          \n",
      " 19  more_particular_accounts_final_label  442736 non-null  int8          \n",
      " 20  particular_accounts_final_label       442736 non-null  int8          \n",
      " 21  particular_plus_accounts_final_label  442736 non-null  int8          \n",
      " 22  home_account_final_label              442736 non-null  int8          \n",
      " 23  payroll_final_label                   442736 non-null  int8          \n",
      " 24  e_account_final_label                 442736 non-null  int8          \n",
      " 25  year                                  442736 non-null  int32         \n",
      " 26  month                                 442736 non-null  int32         \n",
      " 27  quarter                               442736 non-null  int32         \n",
      " 28  day_of_week                           442736 non-null  int32         \n",
      " 29  is_weekend                            442736 non-null  int32         \n",
      " 30  is_month_end                          442736 non-null  int32         \n",
      " 31  is_quarter_end                        442736 non-null  int32         \n",
      " 32  years_since_registration              415002 non-null  float64       \n",
      " 33  tenure_category                       412165 non-null  category      \n",
      " 34  age_group                             414328 non-null  category      \n",
      " 35  is_young_adult                        442736 non-null  int32         \n",
      " 36  is_middle_aged                        442736 non-null  int32         \n",
      " 37  is_senior                             442736 non-null  int32         \n",
      " 38  age_squared                           415002 non-null  float64       \n",
      " 39  seniority_years                       414995 non-null  float64       \n",
      " 40  seniority_category                    414995 non-null  category      \n",
      " 41  is_new_relationship                   442736 non-null  int32         \n",
      " 42  is_established_relationship           442736 non-null  int32         \n",
      " 43  is_employee                           442736 non-null  int32         \n",
      " 44  is_primary_customer_flag              442736 non-null  int64         \n",
      " 45  is_domestic                           442736 non-null  int32         \n",
      " 46  is_foreigner                          442736 non-null  int32         \n",
      " 47  total_products                        442736 non-null  int64         \n",
      " 48  has_any_product                       442736 non-null  int32         \n",
      " 49  is_single_product                     442736 non-null  int32         \n",
      " 50  is_multi_product                      442736 non-null  int32         \n",
      " 51  product_diversity_ratio               442736 non-null  float64       \n",
      " 52  has_current_account                   442736 non-null  int64         \n",
      " 53  has_savings_account                   442736 non-null  int64         \n",
      " 54  has_premium_account                   442736 non-null  int64         \n",
      " 55  age_seniority_interaction             442736 non-null  float64       \n",
      " 56  seniority_per_age                     442736 non-null  float64       \n",
      " 57  age_tenure_ratio                      442736 non-null  float64       \n",
      " 58  channel_type                          442736 non-null  object        \n",
      " 59  is_digital_channel                    442736 non-null  int32         \n",
      " 60  is_vip_segment                        442736 non-null  int32         \n",
      " 61  is_university_segment                 442736 non-null  int32         \n",
      " 62  customer_stability_score              442736 non-null  int64         \n",
      " 63  is_stable_customer                    442736 non-null  int32         \n",
      " 64  customer_potential_score              442736 non-null  int32         \n",
      " 65  is_high_potential                     442736 non-null  int32         \n",
      "dtypes: category(3), datetime64[ns](1), float64(10), int32(25), int64(7), int8(9), object(11)\n",
      "memory usage: 148.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_enhanced.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0336ea94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FEATURE LISTS DEFINED:\n",
      "   Numeric features: 48\n",
      "   Categorical features: 5\n",
      "   Target columns: 9\n",
      "   Total features: 53\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# FEATURE LISTS FOR MODELING\n",
    "# =============================\n",
    "\n",
    "# Numeric features (engineered features)\n",
    "numeric_features = [\n",
    "    'year', 'month', 'quarter', 'day_of_week', 'is_weekend', 'is_month_end', 'is_quarter_end',\n",
    "    'days_since_registration', 'registration_year', 'registration_month', 'years_since_registration',\n",
    "    'is_young_adult', 'is_middle_aged', 'is_senior', 'age_squared',\n",
    "    'income_vs_median', 'is_high_income', 'is_low_income', 'log_income',\n",
    "    'seniority_years', 'is_new_relationship', 'is_established_relationship',\n",
    "    'is_primary_customer', 'is_new_customer', 'is_active', 'is_domestic', 'is_foreigner',\n",
    "    'total_products', 'has_any_product', 'is_single_product', 'is_multi_product',\n",
    "    'product_diversity_ratio', 'has_current_account', 'has_savings_account', 'has_premium_account',\n",
    "    'age_income_interaction', 'income_per_age', 'seniority_income_interaction', 'income_growth_proxy',\n",
    "    'young_high_income', 'senior_established', 'is_digital_channel',\n",
    "    'is_vip_segment', 'is_university_segment', 'customer_stability_score',\n",
    "    'is_stable_customer', 'customer_potential_score', 'is_high_potential'\n",
    "]\n",
    "\n",
    "# Categorical features\n",
    "categorical_features = [\n",
    "    'tenure_category', 'age_group', 'income_quartile', 'seniority_category', 'channel_type'\n",
    "]\n",
    "\n",
    "# Target columns\n",
    "target_cols = [\n",
    "    'current_accounts_final_label',\n",
    "    'payroll_accounts_final_label',\n",
    "    'junior_accounts_final_label',\n",
    "    'more_particular_accounts_final_label',\n",
    "    'particular_accounts_final_label',\n",
    "    'particular_plus_accounts_final_label',\n",
    "    'home_account_final_label',\n",
    "    'payroll_final_label',\n",
    "    'e_account_final_label'\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ FEATURE LISTS DEFINED:\")\n",
    "print(f\"   Numeric features: {len(numeric_features)}\")\n",
    "print(f\"   Categorical features: {len(categorical_features)}\")\n",
    "print(f\"   Target columns: {len(target_cols)}\")\n",
    "print(f\"   Total features: {len(numeric_features) + len(categorical_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5948fe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ FINAL FEATURE SELECTION:\n",
      "Available numeric features: 32\n",
      "Available categorical features: 4\n",
      "Target labels: 9\n",
      "\n",
      "üìã DATA SHAPE:\n",
      "X shape: (442736, 36)\n",
      "y shape: (442736, 9)\n",
      "\n",
      "üîç DATA QUALITY CHECK:\n",
      "Missing values in X: 169929\n",
      "Missing values in y: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# PREPARE DATA FOR MODELING\n",
    "# =============================\n",
    "\n",
    "# Features to exclude from modeling\n",
    "exclude_features = [\n",
    "    'date', 'customer_id', 'registration_date', 'last_primary_date'\n",
    "]\n",
    "\n",
    "# Filter features that exist in dataframe\n",
    "available_num_cols = [col for col in numeric_features if col in df_enhanced.columns and col not in exclude_features]\n",
    "available_cat_cols = [col for col in categorical_features if col in df_enhanced.columns and col not in exclude_features]\n",
    "\n",
    "print(f\"üéØ FINAL FEATURE SELECTION:\")\n",
    "print(f\"Available numeric features: {len(available_num_cols)}\")\n",
    "print(f\"Available categorical features: {len(available_cat_cols)}\")\n",
    "print(f\"Target labels: {len(target_cols)}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = df_enhanced[available_num_cols + available_cat_cols]\n",
    "y = df_enhanced[target_cols]\n",
    "\n",
    "print(f\"\\nüìã DATA SHAPE:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"\\nüîç DATA QUALITY CHECK:\")\n",
    "print(f\"Missing values in X: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y: {y.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed920174",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59f30ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEGION\\AppData\\Local\\Temp\\ipykernel_9544\\3226311478.py:1: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_10_2015 = pd.read_csv('data/processed/test_10_2015.csv')\n",
      "C:\\Users\\LEGION\\AppData\\Local\\Temp\\ipykernel_9544\\3226311478.py:2: DtypeWarning: Columns (5,8,11,12,15,16,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_11_2015 = pd.read_csv('data/processed/test_11_2015.csv')\n",
      "C:\\Users\\LEGION\\AppData\\Local\\Temp\\ipykernel_9544\\3226311478.py:3: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_12_2015 = pd.read_csv('data/processed/test_12_2015.csv')\n"
     ]
    }
   ],
   "source": [
    "test_10_2015 = pd.read_csv('data/processed/test_10_2015.csv')\n",
    "test_11_2015 = pd.read_csv('data/processed/test_11_2015.csv')\n",
    "test_12_2015 = pd.read_csv('data/processed/test_12_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb6924a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 903.96 Mb (53.6% reduction)\n",
      "Mem. usage decreased to 950.77 Mb (53.6% reduction)\n",
      "Mem. usage decreased to 998.31 Mb (53.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "test_10_2015 = reduce_memory_usage(test_10_2015)\n",
    "test_11_2015 = reduce_memory_usage(test_11_2015)\n",
    "test_12_2015 = reduce_memory_usage(test_12_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19d895f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ CLEANING DATASET...\n",
      "========================================\n",
      "1Ô∏è‚É£ Handling missing values...\n",
      "   ‚úÖ Filled payroll_final_label: 0 missing ‚Üí 0\n",
      "   ‚úÖ Filled pensions_2_final_label: 0 missing ‚Üí 0\n",
      "\n",
      "2Ô∏è‚É£ Creating customer tenure feature...\n",
      "   ‚úÖ Created 'customer_tenure_days' feature\n",
      "   üìä Range: -3.0 to 7590.0 days\n",
      "   üóëÔ∏è Dropped 'registration_date' column\n",
      "\n",
      "3Ô∏è‚É£ Converting last_primary_date to binary indicator...\n",
      "   ‚úÖ Created 'was_primary_customer' binary feature\n",
      "   üìä 5,316,273 nulls ‚Üí 0, 8,870 dates ‚Üí 1\n",
      "   üóëÔ∏è Dropped 'last_primary_date' column\n",
      "\n",
      "4Ô∏è‚É£ Removing constant and duplicate columns...\n",
      "   üóëÔ∏è Dropped 'address_type' (constant value)\n",
      "   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\n",
      "\n",
      "5Ô∏è‚É£ Cleaning numeric columns...\n",
      "   ‚úÖ Cleaned 'age': object ‚Üí numeric (9,750 nulls)\n",
      "   ‚ö†Ô∏è Converted 14 negative seniority values to NaN\n",
      "   ‚úÖ Cleaned 'seniority': object ‚Üí numeric (9,764 nulls)\n",
      "\n",
      "üìä CLEANUP SUMMARY:\n",
      "----------------------------------------\n",
      "   Final shape: (5325143, 46)\n",
      "   Memory usage: 4408.8 MB\n",
      "   Null values: 6,810,911\n",
      "\n",
      "üìã DATA TYPES AFTER CLEANUP:\n",
      "   int64: 26 columns\n",
      "   object: 12 columns\n",
      "   float64: 7 columns\n",
      "   datetime64[ns]: 1 columns\n",
      "üßπ CLEANING DATASET...\n",
      "========================================\n",
      "1Ô∏è‚É£ Handling missing values...\n",
      "   ‚úÖ Filled payroll_final_label: 0 missing ‚Üí 0\n",
      "   ‚úÖ Filled pensions_2_final_label: 0 missing ‚Üí 0\n",
      "\n",
      "2Ô∏è‚É£ Creating customer tenure feature...\n",
      "   ‚úÖ Created 'customer_tenure_days' feature\n",
      "   üìä Range: -3.0 to 7621.0 days\n",
      "   üóëÔ∏è Dropped 'registration_date' column\n",
      "\n",
      "3Ô∏è‚É£ Converting last_primary_date to binary indicator...\n",
      "   ‚úÖ Created 'was_primary_customer' binary feature\n",
      "   üìä 5,591,002 nulls ‚Üí 0, 9,883 dates ‚Üí 1\n",
      "   üóëÔ∏è Dropped 'last_primary_date' column\n",
      "\n",
      "4Ô∏è‚É£ Removing constant and duplicate columns...\n",
      "   üóëÔ∏è Dropped 'address_type' (constant value)\n",
      "   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\n",
      "\n",
      "5Ô∏è‚É£ Cleaning numeric columns...\n",
      "   ‚úÖ Cleaned 'age': object ‚Üí numeric (5,458 nulls)\n",
      "   ‚ö†Ô∏è Converted 14 negative seniority values to NaN\n",
      "   ‚úÖ Cleaned 'seniority': object ‚Üí numeric (5,472 nulls)\n",
      "\n",
      "üìä CLEANUP SUMMARY:\n",
      "----------------------------------------\n",
      "   Final shape: (5600885, 46)\n",
      "   Memory usage: 4647.5 MB\n",
      "   Null values: 7,165,962\n",
      "\n",
      "üìã DATA TYPES AFTER CLEANUP:\n",
      "   int64: 26 columns\n",
      "   object: 12 columns\n",
      "   float64: 7 columns\n",
      "   datetime64[ns]: 1 columns\n",
      "üßπ CLEANING DATASET...\n",
      "========================================\n",
      "1Ô∏è‚É£ Handling missing values...\n",
      "   ‚úÖ Filled payroll_final_label: 0 missing ‚Üí 0\n",
      "   ‚úÖ Filled pensions_2_final_label: 0 missing ‚Üí 0\n",
      "\n",
      "2Ô∏è‚É£ Creating customer tenure feature...\n",
      "   ‚úÖ Created 'customer_tenure_days' feature\n",
      "   üìä Range: -3.0 to 7651.0 days\n",
      "   üóëÔ∏è Dropped 'registration_date' column\n",
      "\n",
      "3Ô∏è‚É£ Converting last_primary_date to binary indicator...\n",
      "   ‚úÖ Created 'was_primary_customer' binary feature\n",
      "   üìä 5,868,778 nulls ‚Üí 0, 12,171 dates ‚Üí 1\n",
      "   üóëÔ∏è Dropped 'last_primary_date' column\n",
      "\n",
      "4Ô∏è‚É£ Removing constant and duplicate columns...\n",
      "   üóëÔ∏è Dropped 'address_type' (constant value)\n",
      "   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\n",
      "\n",
      "5Ô∏è‚É£ Cleaning numeric columns...\n",
      "   ‚úÖ Cleaned 'age': object ‚Üí numeric (1,861 nulls)\n",
      "   ‚ö†Ô∏è Converted 14 negative seniority values to NaN\n",
      "   ‚úÖ Cleaned 'seniority': object ‚Üí numeric (1,875 nulls)\n",
      "\n",
      "üìä CLEANUP SUMMARY:\n",
      "----------------------------------------\n",
      "   Final shape: (5880949, 46)\n",
      "   Memory usage: 4887.3 MB\n",
      "   Null values: 7,513,301\n",
      "\n",
      "üìã DATA TYPES AFTER CLEANUP:\n",
      "   int64: 26 columns\n",
      "   object: 12 columns\n",
      "   float64: 7 columns\n",
      "   datetime64[ns]: 1 columns\n"
     ]
    }
   ],
   "source": [
    "test_10_2015 = clean_dataset(test_10_2015)\n",
    "test_11_2015 = clean_dataset(test_11_2015)\n",
    "test_12_2015 = clean_dataset(test_12_2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efa4562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë kh√°ch h√†ng: 1,017,191\n",
      "Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: 1,017,191\n",
      "\n",
      "üìä Distribution check:\n",
      "  current_accounts_final_label: [0 1]\n",
      "  payroll_accounts_final_label: [0 1]\n",
      "  junior_accounts_final_label: [0 1]\n",
      "  more_particular_accounts_final_label: [0 1]\n",
      "  particular_accounts_final_label: [0 1]\n",
      "  particular_plus_accounts_final_label: [0 1]\n",
      "  home_account_final_label: [0]\n",
      "  payroll_final_label: [0 1]\n",
      "  e_account_final_label: [0 1]\n",
      "T·ªïng s·ªë kh√°ch h√†ng: 1,223,624\n",
      "Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: 1,223,624\n",
      "\n",
      "üìä Distribution check:\n",
      "  current_accounts_final_label: [0 1]\n",
      "  payroll_accounts_final_label: [0 1]\n",
      "  junior_accounts_final_label: [0 1]\n",
      "  more_particular_accounts_final_label: [0 1]\n",
      "  particular_accounts_final_label: [0 1]\n",
      "  particular_plus_accounts_final_label: [0 1]\n",
      "  home_account_final_label: [0]\n",
      "  payroll_final_label: [0 1]\n",
      "  e_account_final_label: [0 1]\n",
      "T·ªïng s·ªë kh√°ch h√†ng: 1,425,848\n",
      "Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: 1,425,848\n",
      "\n",
      "üìä Distribution check:\n",
      "  current_accounts_final_label: [0 1]\n",
      "  payroll_accounts_final_label: [0 1]\n",
      "  junior_accounts_final_label: [0 1]\n",
      "  more_particular_accounts_final_label: [0 1]\n",
      "  particular_accounts_final_label: [0 1]\n",
      "  particular_plus_accounts_final_label: [0 1]\n",
      "  home_account_final_label: [0 1]\n",
      "  payroll_final_label: [0 1]\n",
      "  e_account_final_label: [0 1]\n"
     ]
    }
   ],
   "source": [
    "test_10_2015 = filter_data(test_10_2015)\n",
    "test_11_2015 = filter_data(test_11_2015)\n",
    "test_12_2015 = filter_data(test_12_2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f3c63b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CREATING ENHANCED FEATURES...\n",
      "==================================================\n",
      "üìã Available columns: 25\n",
      "üìÖ Creating time-based features...\n",
      "üë• Creating demographic features...\n",
      "üè¶ Creating banking relationship features...\n",
      "üìä Creating product portfolio features...\n",
      "üîó Creating interaction features...\n",
      "üéØ Creating behavioral features...\n",
      "‚öñÔ∏è Creating risk and stability features...\n",
      "‚úÖ Feature engineering completed!\n",
      "Original features: 25\n",
      "Enhanced features: 66\n",
      "New features added: 41\n",
      "üîß CREATING ENHANCED FEATURES...\n",
      "==================================================\n",
      "üìã Available columns: 25\n",
      "üìÖ Creating time-based features...\n",
      "üë• Creating demographic features...\n",
      "üè¶ Creating banking relationship features...\n",
      "üìä Creating product portfolio features...\n",
      "üîó Creating interaction features...\n",
      "üéØ Creating behavioral features...\n",
      "‚öñÔ∏è Creating risk and stability features...\n",
      "‚úÖ Feature engineering completed!\n",
      "Original features: 25\n",
      "Enhanced features: 66\n",
      "New features added: 41\n",
      "üîß CREATING ENHANCED FEATURES...\n",
      "==================================================\n",
      "üìã Available columns: 25\n",
      "üìÖ Creating time-based features...\n",
      "üë• Creating demographic features...\n",
      "üè¶ Creating banking relationship features...\n",
      "üìä Creating product portfolio features...\n",
      "üîó Creating interaction features...\n",
      "üéØ Creating behavioral features...\n",
      "‚öñÔ∏è Creating risk and stability features...\n",
      "‚úÖ Feature engineering completed!\n",
      "Original features: 25\n",
      "Enhanced features: 66\n",
      "New features added: 41\n"
     ]
    }
   ],
   "source": [
    "test_10_2015 = create_enhanced_features(test_10_2015)\n",
    "test_11_2015 = create_enhanced_features(test_11_2015)\n",
    "test_12_2015 = create_enhanced_features(test_12_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20fa9d",
   "metadata": {},
   "source": [
    "# Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9917f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=7):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    \n",
    "    Parameters:\n",
    "    actual : list - A list of actual relevant items\n",
    "    predicted : list - A list of predicted items ordered by rank\n",
    "    k : int - The maximum number of predicted elements\n",
    "    \n",
    "    Returns:\n",
    "    score : double - The average precision at k\n",
    "    \"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=7):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    \n",
    "    Parameters:\n",
    "    actual : list of lists - A list of lists of actual relevant items\n",
    "    predicted : list of lists - A list of lists of predicted items ordered by rank\n",
    "    k : int - The maximum number of predicted elements\n",
    "    \n",
    "    Returns:\n",
    "    score : double - The mean average precision at k\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "def evaluate_recommendations(actual_products, predicted_products, k=7):\n",
    "    \"\"\"\n",
    "    Evaluate recommendation system using MAP@k\n",
    "    \n",
    "    Parameters:\n",
    "    actual_products : dict - {user_id: [list of actual products]}\n",
    "    predicted_products : dict - {user_id: [list of predicted products]}\n",
    "    k : int - Number of recommendations to consider\n",
    "    \n",
    "    Returns:\n",
    "    map_score : float - MAP@k score\n",
    "    \"\"\"\n",
    "    actual_list = []\n",
    "    predicted_list = []\n",
    "    \n",
    "    for user_id in actual_products.keys():\n",
    "        if user_id in predicted_products:\n",
    "            actual_list.append(actual_products[user_id])\n",
    "            predicted_list.append(predicted_products[user_id])\n",
    "        else:\n",
    "            actual_list.append(actual_products[user_id])\n",
    "            predicted_list.append([])  # No predictions for this user\n",
    "    \n",
    "    map_score = mapk(actual_list, predicted_list, k)\n",
    "    \n",
    "    print(f\" MAP@{k}: {map_score:.4f}\")\n",
    "    print(f\" Users evaluated: {len(actual_list):,}\")\n",
    "    print(f\" Coverage: {len([p for p in predicted_list if p]) / len(predicted_list):.2%}\")\n",
    "    \n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3d16c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precision_at_k(actual, predicted, k=7):\n",
    "    \"\"\"\n",
    "    Computes the precision at k.\n",
    "\n",
    "    Parameters:\n",
    "    actual : list - A list of actual relevant items\n",
    "    predicted : list - A list of predicted items ordered by rank\n",
    "    k : int - The maximum number of predicted elements\n",
    "\n",
    "    Returns:\n",
    "    precision : double - The precision at k\n",
    "    \"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    \n",
    "    if not predicted:\n",
    "        return 0.0\n",
    "    \n",
    "    # ƒê·∫øm s·ªë d·ª± ƒëo√°n ƒë√∫ng trong top-k\n",
    "    correct = len(set(predicted) & set(actual))\n",
    "    return correct / k\n",
    "\n",
    "\n",
    "def evaluate_recommendations_with_precision(actual_products, predicted_products, k=7):\n",
    "    \"\"\"\n",
    "    Evaluate recommendation system using MAP@k and Precision@k.\n",
    "\n",
    "    Parameters:\n",
    "    actual_products : dict - {user_id: [list of actual products]}\n",
    "    predicted_products : dict - {user_id: [list of predicted products]}\n",
    "    k : int - Number of recommendations to consider\n",
    "\n",
    "    Returns:\n",
    "    metrics : dict - {'MAP@k': ..., 'Precision@k': ...}\n",
    "    \"\"\"\n",
    "    actual_list = []\n",
    "    predicted_list = []\n",
    "\n",
    "    for user_id in actual_products.keys():\n",
    "        if user_id in predicted_products:\n",
    "            actual_list.append(actual_products[user_id])\n",
    "            predicted_list.append(predicted_products[user_id])\n",
    "        else:\n",
    "            actual_list.append(actual_products[user_id])\n",
    "            predicted_list.append([])  # No predictions for this user\n",
    "\n",
    "    # MAP@k\n",
    "    map_score = mapk(actual_list, predicted_list, k)\n",
    "\n",
    "    # Precision@k trung b√¨nh\n",
    "    precision_scores = [\n",
    "        precision_at_k(a, p, k) for a, p in zip(actual_list, predicted_list)\n",
    "    ]\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "\n",
    "    print(f\" MAP@{k}: {map_score:.4f}\")\n",
    "    print(f\" Precision@{k}: {avg_precision:.4f}\")\n",
    "    print(f\" Users evaluated: {len(actual_list):,}\")\n",
    "    print(f\" Coverage: {len([p for p in predicted_list if p]) / len(predicted_list):.2%}\")\n",
    "\n",
    "    return {f\"MAP@{k}\": map_score, f\"Precision@{k}\": avg_precision}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9f5d0",
   "metadata": {},
   "source": [
    "# Data Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7dbb7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessor updated with:\n",
      "   - 32 numeric features\n",
      "   - 4 categorical features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Updated preprocessors\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),   # Fill missing values with mean\n",
    "    ('scaler', StandardScaler())                   # Scale numerical data\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing with most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))     # One-hot encode categorical data\n",
    "])\n",
    "\n",
    "# Updated ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, available_num_cols),\n",
    "        ('cat', categorical_transformer, available_cat_cols)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any remaining columns\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Preprocessor updated with:\")\n",
    "print(f\"   - {len(available_num_cols)} numeric features\")\n",
    "print(f\"   - {len(available_cat_cols)} categorical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da672c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XGBoost added to models (with fixed parameters)\n",
      "‚úÖ LightGBM added to models\n",
      "\n",
      "ü§ñ MODELS PREPARED: ['Random Forest', 'XGBoost', 'LightGBM']\n",
      "üìä Total models to train: 3\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# MULTI-LABEL CLASSIFICATION MODELS - FIXED\n",
    "# =============================\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, hamming_loss, jaccard_score\n",
    "import time\n",
    "\n",
    "# Define all models for multi-label classification\n",
    "models = {\n",
    "    'Random Forest': MultiOutputClassifier(\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    "}\n",
    "\n",
    "# Add XGBoost if available - FIXED with proper parameters\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    models['XGBoost'] = MultiOutputClassifier(\n",
    "        XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbosity=0,\n",
    "            base_score=0.5,  # Fix for the logistic loss error\n",
    "            objective='binary:logistic',  # Explicitly set objective\n",
    "            eval_metric='logloss'  # Set evaluation metric\n",
    "        )\n",
    "    )\n",
    "    print(\"‚úÖ XGBoost added to models (with fixed parameters)\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost not available - install with: pip install xgboost\")\n",
    "\n",
    "# Add LightGBM if available - with proper parameters\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    models['LightGBM'] = MultiOutputClassifier(\n",
    "        LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1,\n",
    "            objective='binary',  # Explicitly set objective\n",
    "            boosting_type='gbdt'\n",
    "        )\n",
    "    )\n",
    "    print(\"‚úÖ LightGBM added to models\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LightGBM not available - install with: pip install lightgbm\")\n",
    "\n",
    "# Create complete pipelines for all models\n",
    "pipelines = {}\n",
    "for name, model in models.items():\n",
    "    pipelines[name] = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "print(f\"\\nü§ñ MODELS PREPARED: {list(pipelines.keys())}\")\n",
    "print(f\"üìä Total models to train: {len(pipelines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f841ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# COMPREHENSIVE MODEL EVALUATION FUNCTION - UPDATED WITH MAP@2,3,4,5\n",
    "# =============================\n",
    "\n",
    "def evaluate_model_comprehensive(model_pipeline, model_name, processed_test_data, tar_cols):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation function for recommendation system\n",
    "    \n",
    "    Parameters:\n",
    "    model_pipeline : sklearn.pipeline.Pipeline - Trained model pipeline\n",
    "    model_name : str - Name of the model\n",
    "    processed_test_data : dict - Dictionary of processed test datasets\n",
    "    tar_cols : list - List of target column names\n",
    "    \n",
    "    Returns:\n",
    "    results_df : pd.DataFrame - Comprehensive results DataFrame\n",
    "    detailed_results : dict - Detailed results for further analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüß™ EVALUATING {model_name.upper()}...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_results = []\n",
    "    detailed_results = {\n",
    "        'model_name': model_name,\n",
    "        'monthly_results': {},\n",
    "        'overall_metrics': {},\n",
    "        'all_actual_products': {},\n",
    "        'all_predicted_products': {}\n",
    "    }\n",
    "    \n",
    "    customer_offset = 0\n",
    "    \n",
    "    for month_name, data in processed_test_data.items():\n",
    "        print(f\"\\nüìä Evaluating on {month_name}...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        X_test_month = data['X']\n",
    "        y_test_month = data['y']\n",
    "        \n",
    "        if y_test_month is not None:\n",
    "            # Make predictions\n",
    "            start_time = time.time()\n",
    "            y_pred_month = model_pipeline.predict(X_test_month)\n",
    "            prediction_time = time.time() - start_time\n",
    "            \n",
    "            # Get prediction probabilities for ranking\n",
    "            y_pred_proba = model_pipeline.predict_proba(X_test_month)\n",
    "            \n",
    "            # Calculate standard classification metrics\n",
    "            test_accuracy = accuracy_score(y_test_month, y_pred_month)\n",
    "            hamming_loss_score = hamming_loss(y_test_month, y_pred_month)\n",
    "            jaccard_score_macro = jaccard_score(y_test_month, y_pred_month, average='macro')\n",
    "            \n",
    "            print(f\"   üìà Standard Metrics:\")\n",
    "            print(f\"      Accuracy: {test_accuracy:.4f}\")\n",
    "            print(f\"      Hamming Loss: {hamming_loss_score:.4f}\")\n",
    "            print(f\"      Jaccard (Macro): {jaccard_score_macro:.4f}\")\n",
    "            \n",
    "            # =============================\n",
    "            # PREPARE RECOMMENDATION DATA\n",
    "            # =============================\n",
    "            actual_products = {}\n",
    "            predicted_products = {}\n",
    "            \n",
    "            for i in range(len(y_test_month)):\n",
    "                customer_id = i\n",
    "                \n",
    "                # Get actual products\n",
    "                actual = []\n",
    "                for j, product in enumerate(tar_cols):\n",
    "                    if y_test_month.iloc[i, j] == 1:\n",
    "                        actual.append(product)\n",
    "                actual_products[customer_id] = actual\n",
    "                \n",
    "                # Get predicted products (ranked by probability)\n",
    "                product_scores = []\n",
    "                for j, product in enumerate(tar_cols):\n",
    "                    # Get probability of class 1\n",
    "                    prob = y_pred_proba[j][i][1] if len(y_pred_proba[j][i]) > 1 else y_pred_proba[j][i][0]\n",
    "                    product_scores.append((product, prob))\n",
    "                \n",
    "                # Sort by probability (descending)\n",
    "                product_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                predicted = [product for product, score in product_scores]\n",
    "                predicted_products[customer_id] = predicted\n",
    "                \n",
    "                # Add to overall data with offset\n",
    "                detailed_results['all_actual_products'][customer_id + customer_offset] = actual\n",
    "                detailed_results['all_predicted_products'][customer_id + customer_offset] = predicted\n",
    "            \n",
    "            customer_offset += len(actual_products)\n",
    "            \n",
    "            # =============================\n",
    "            # CALCULATE RECOMMENDATION METRICS - UPDATED K VALUES\n",
    "            # =============================\n",
    "            print(f\"\\n   üéØ Recommendation Metrics:\")\n",
    "            \n",
    "            month_metrics = {}\n",
    "            for k in [2, 3, 4, 5]:  # CHANGED: Test v·ªõi k=2,3,4,5\n",
    "                # Calculate MAP@k and Precision@k\n",
    "                actual_list = list(actual_products.values())\n",
    "                predicted_list = list(predicted_products.values())\n",
    "                \n",
    "                # MAP@k\n",
    "                map_score = mapk(actual_list, predicted_list, k)\n",
    "                \n",
    "                # Precision@k\n",
    "                precision_scores = [precision_at_k(a, p, k) for a, p in zip(actual_list, predicted_list)]\n",
    "                avg_precision = np.mean(precision_scores)\n",
    "                \n",
    "                month_metrics[f'MAP@{k}'] = map_score\n",
    "                month_metrics[f'Precision@{k}'] = avg_precision\n",
    "                \n",
    "                print(f\"      k={k}: MAP={map_score:.4f}, Precision={avg_precision:.4f}\")\n",
    "            \n",
    "            # =============================\n",
    "            # MONTHLY STATISTICS\n",
    "            # =============================\n",
    "            total_customers = len(actual_products)\n",
    "            customers_with_products = len([p for p in actual_products.values() if p])\n",
    "            avg_products_per_customer = np.mean([len(p) for p in actual_products.values()])\n",
    "            \n",
    "            # Product distribution\n",
    "            product_counts = {}\n",
    "            for products in actual_products.values():\n",
    "                for product in products:\n",
    "                    product_counts[product] = product_counts.get(product, 0) + 1\n",
    "            \n",
    "            # Store monthly result\n",
    "            month_result = {\n",
    "                'Model': model_name,\n",
    "                'Month': month_name,\n",
    "                'Total_Customers': total_customers,\n",
    "                'Customers_With_Products': customers_with_products,\n",
    "                'Coverage_Rate': customers_with_products / total_customers,\n",
    "                'Avg_Products_Per_Customer': avg_products_per_customer,\n",
    "                'Test_Accuracy': test_accuracy,\n",
    "                'Hamming_Loss': hamming_loss_score,\n",
    "                'Jaccard_Macro': jaccard_score_macro,\n",
    "                'Prediction_Time': prediction_time,\n",
    "                **month_metrics  # Add all MAP@k and Precision@k metrics\n",
    "            }\n",
    "            \n",
    "            all_results.append(month_result)\n",
    "            \n",
    "            # Store detailed results\n",
    "            detailed_results['monthly_results'][month_name] = {\n",
    "                'actual_products': actual_products,\n",
    "                'predicted_products': predicted_products,\n",
    "                'metrics': month_metrics,\n",
    "                'stats': month_result,\n",
    "                'product_distribution': product_counts\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚è±Ô∏è Time: {prediction_time:.2f}s\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No ground truth available for {month_name}\")\n",
    "    \n",
    "    # =============================\n",
    "    # CALCULATE OVERALL METRICS - UPDATED K VALUES\n",
    "    # =============================\n",
    "    if detailed_results['all_actual_products'] and detailed_results['all_predicted_products']:\n",
    "        print(f\"\\nüéØ Overall Performance:\")\n",
    "        \n",
    "        overall_metrics = {}\n",
    "        for k in [2, 3, 4, 5]:  # CHANGED: Overall metrics v·ªõi k=2,3,4,5\n",
    "            actual_list = list(detailed_results['all_actual_products'].values())\n",
    "            predicted_list = list(detailed_results['all_predicted_products'].values())\n",
    "            \n",
    "            # Overall MAP@k and Precision@k\n",
    "            map_score = mapk(actual_list, predicted_list, k)\n",
    "            precision_scores = [precision_at_k(a, p, k) for a, p in zip(actual_list, predicted_list)]\n",
    "            avg_precision = np.mean(precision_scores)\n",
    "            \n",
    "            overall_metrics[f'Overall_MAP@{k}'] = map_score\n",
    "            overall_metrics[f'Overall_Precision@{k}'] = avg_precision\n",
    "            \n",
    "            print(f\"   Overall k={k}: MAP={map_score:.4f}, Precision={avg_precision:.4f}\")\n",
    "        \n",
    "        # Add overall row\n",
    "        overall_result = {\n",
    "            'Model': model_name,\n",
    "            'Month': 'OVERALL',\n",
    "            'Total_Customers': len(detailed_results['all_actual_products']),\n",
    "            'Customers_With_Products': len([p for p in detailed_results['all_actual_products'].values() if p]),\n",
    "            'Coverage_Rate': len([p for p in detailed_results['all_actual_products'].values() if p]) / len(detailed_results['all_actual_products']),\n",
    "            'Avg_Products_Per_Customer': np.mean([len(p) for p in detailed_results['all_actual_products'].values()]),\n",
    "            'Test_Accuracy': np.mean([r['Test_Accuracy'] for r in all_results]),\n",
    "            'Hamming_Loss': np.mean([r['Hamming_Loss'] for r in all_results]),\n",
    "            'Jaccard_Macro': np.mean([r['Jaccard_Macro'] for r in all_results]),\n",
    "            'Prediction_Time': sum([r['Prediction_Time'] for r in all_results]),\n",
    "        }\n",
    "        \n",
    "        # Add overall metrics - UPDATED K VALUES\n",
    "        for k in [2, 3, 4, 5]:  # CHANGED: Add k=2,3,4,5 to overall results\n",
    "            overall_result[f'MAP@{k}'] = overall_metrics[f'Overall_MAP@{k}']\n",
    "            overall_result[f'Precision@{k}'] = overall_metrics[f'Overall_Precision@{k}']\n",
    "        \n",
    "        all_results.append(overall_result)\n",
    "        detailed_results['overall_metrics'] = overall_metrics\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    return results_df, detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5606ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Formatting test data: 1,017,191 samples\n",
      "   ‚úÖ Ground truth available: (1017191, 9)\n",
      "üìã Formatting test data: 1,223,624 samples\n",
      "   ‚úÖ Ground truth available: (1223624, 9)\n",
      "üìã Formatting test data: 1,425,848 samples\n",
      "   ‚úÖ Ground truth available: (1425848, 9)\n",
      "\n",
      "‚úÖ TEST DATA FORMATTED FOR EVALUATION!\n",
      "   October 2015: X=(1017191, 36), y=(1017191, 9)\n",
      "   November 2015: X=(1223624, 36), y=(1223624, 9)\n",
      "   December 2015: X=(1425848, 36), y=(1425848, 9)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# PREPARE TEST DATA FORMAT FOR EVALUATION\n",
    "# =============================\n",
    "\n",
    "def format_test_data_for_evaluation(test_df, available_num_cols, available_cat_cols, tar_cols):\n",
    "    \"\"\"\n",
    "    Format processed test data ƒë·ªÉ c√≥ structure ph√π h·ª£p v·ªõi evaluation function\n",
    "    \"\"\"\n",
    "    print(f\"üìã Formatting test data: {len(test_df):,} samples\")\n",
    "    \n",
    "    # Th√™m missing features n·∫øu c·∫ßn\n",
    "    test_formatted = test_df.copy()\n",
    "    \n",
    "    # Add missing numeric features\n",
    "    for feature in available_num_cols:\n",
    "        if feature not in test_formatted.columns:\n",
    "            test_formatted[feature] = 0\n",
    "            print(f\"   ‚ö†Ô∏è Added missing numeric: {feature}\")\n",
    "    \n",
    "    # Add missing categorical features  \n",
    "    for feature in available_cat_cols:\n",
    "        if feature not in test_formatted.columns:\n",
    "            test_formatted[feature] = 'Unknown'\n",
    "            print(f\"   ‚ö†Ô∏è Added missing categorical: {feature}\")\n",
    "    \n",
    "    # Prepare X v√† y\n",
    "    X_test = test_formatted[available_num_cols + available_cat_cols]\n",
    "    \n",
    "    # Check target columns\n",
    "    if all(col in test_formatted.columns for col in tar_cols):\n",
    "        y_test = test_formatted[tar_cols]\n",
    "        print(f\"   ‚úÖ Ground truth available: {y_test.shape}\")\n",
    "    else:\n",
    "        y_test = None\n",
    "        print(f\"   ‚ö†Ô∏è No ground truth available\")\n",
    "    \n",
    "    return {'X': X_test, 'y': y_test}\n",
    "\n",
    "# Format test datasets\n",
    "processed_test_data = {\n",
    "    'October 2015': format_test_data_for_evaluation(\n",
    "        test_10_2015, available_num_cols, available_cat_cols, tar_cols\n",
    "    ),\n",
    "    'November 2015': format_test_data_for_evaluation(\n",
    "        test_11_2015, available_num_cols, available_cat_cols, tar_cols\n",
    "    ),\n",
    "    'December 2015': format_test_data_for_evaluation(\n",
    "        test_12_2015, available_num_cols, available_cat_cols, tar_cols\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ TEST DATA FORMATTED FOR EVALUATION!\")\n",
    "for month, data in processed_test_data.items():\n",
    "    x_shape = data['X'].shape\n",
    "    y_shape = data['y'].shape if data['y'] is not None else 'None'\n",
    "    print(f\"   {month}: X={x_shape}, y={y_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "671e9243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ TRAINING AND EVALUATING ALL MODELS...\n",
      "======================================================================\n",
      "\n",
      "ü§ñ TRAINING RANDOM FOREST...\n",
      "--------------------------------------------------\n",
      "üìä Calculating training metrics...\n",
      "   ‚úÖ Training completed in 51.28s\n",
      "   üìà Train Accuracy: 0.9982\n",
      "\n",
      "üß™ EVALUATING RANDOM FOREST...\n",
      "============================================================\n",
      "\n",
      "üìä Evaluating on October 2015...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9964\n",
      "      Hamming Loss: 0.0005\n",
      "      Jaccard (Macro): 0.5829\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0140, Precision=0.0149\n",
      "      k=3: MAP=0.0150, Precision=0.0111\n",
      "      k=4: MAP=0.0153, Precision=0.0086\n",
      "      k=5: MAP=0.0154, Precision=0.0070\n",
      "   ‚è±Ô∏è Time: 12.56s\n",
      "\n",
      "üìä Evaluating on November 2015...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9971\n",
      "      Hamming Loss: 0.0004\n",
      "      Jaccard (Macro): 0.5918\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0130, Precision=0.0137\n",
      "      k=3: MAP=0.0138, Precision=0.0101\n",
      "      k=4: MAP=0.0141, Precision=0.0078\n",
      "      k=5: MAP=0.0141, Precision=0.0063\n",
      "   ‚è±Ô∏è Time: 15.11s\n",
      "\n",
      "üìä Evaluating on December 2015...\n",
      "----------------------------------------\n",
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9975\n",
      "      Hamming Loss: 0.0003\n",
      "      Jaccard (Macro): 0.5630\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0103, Precision=0.0109\n",
      "      k=3: MAP=0.0110, Precision=0.0080\n",
      "      k=4: MAP=0.0112, Precision=0.0062\n",
      "      k=5: MAP=0.0112, Precision=0.0050\n",
      "   ‚è±Ô∏è Time: 17.60s\n",
      "\n",
      "üéØ Overall Performance:\n",
      "   Overall k=2: MAP=0.0122, Precision=0.0130\n",
      "   Overall k=3: MAP=0.0130, Precision=0.0095\n",
      "   Overall k=4: MAP=0.0133, Precision=0.0074\n",
      "   Overall k=5: MAP=0.0133, Precision=0.0060\n",
      "\n",
      "‚úÖ Random Forest evaluation completed!\n",
      "üîß Pipeline saved for future use\n",
      "\n",
      "ü§ñ TRAINING XGBOOST...\n",
      "--------------------------------------------------\n",
      "üìä Calculating training metrics...\n",
      "   ‚úÖ Training completed in 7.91s\n",
      "   üìà Train Accuracy: 0.9998\n",
      "\n",
      "üß™ EVALUATING XGBOOST...\n",
      "============================================================\n",
      "\n",
      "üìä Evaluating on October 2015...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9984\n",
      "      Hamming Loss: 0.0003\n",
      "      Jaccard (Macro): 0.6893\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0301, Precision=0.0168\n",
      "      k=3: MAP=0.0302, Precision=0.0116\n",
      "      k=4: MAP=0.0303, Precision=0.0088\n",
      "      k=5: MAP=0.0303, Precision=0.0070\n",
      "   ‚è±Ô∏è Time: 2.32s\n",
      "\n",
      "üìä Evaluating on November 2015...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9987\n",
      "      Hamming Loss: 0.0002\n",
      "      Jaccard (Macro): 0.6788\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0277, Precision=0.0153\n",
      "      k=3: MAP=0.0278, Precision=0.0105\n",
      "      k=4: MAP=0.0279, Precision=0.0080\n",
      "      k=5: MAP=0.0279, Precision=0.0064\n",
      "   ‚è±Ô∏è Time: 2.92s\n",
      "\n",
      "üìä Evaluating on December 2015...\n",
      "----------------------------------------\n",
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9989\n",
      "      Hamming Loss: 0.0002\n",
      "      Jaccard (Macro): 0.6557\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0219, Precision=0.0120\n",
      "      k=3: MAP=0.0221, Precision=0.0083\n",
      "      k=4: MAP=0.0221, Precision=0.0062\n",
      "      k=5: MAP=0.0221, Precision=0.0050\n",
      "   ‚è±Ô∏è Time: 3.29s\n",
      "\n",
      "üéØ Overall Performance:\n",
      "   Overall k=2: MAP=0.0261, Precision=0.0144\n",
      "   Overall k=3: MAP=0.0262, Precision=0.0099\n",
      "   Overall k=4: MAP=0.0263, Precision=0.0075\n",
      "   Overall k=5: MAP=0.0263, Precision=0.0060\n",
      "\n",
      "‚úÖ XGBoost evaluation completed!\n",
      "üîß Pipeline saved for future use\n",
      "\n",
      "ü§ñ TRAINING LIGHTGBM...\n",
      "--------------------------------------------------\n",
      "üìä Calculating training metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Training completed in 7.57s\n",
      "   üìà Train Accuracy: 0.9923\n",
      "\n",
      "üß™ EVALUATING LIGHTGBM...\n",
      "============================================================\n",
      "\n",
      "üìä Evaluating on October 2015...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9906\n",
      "      Hamming Loss: 0.0012\n",
      "      Jaccard (Macro): 0.3378\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0275, Precision=0.0154\n",
      "      k=3: MAP=0.0275, Precision=0.0105\n",
      "      k=4: MAP=0.0276, Precision=0.0079\n",
      "      k=5: MAP=0.0276, Precision=0.0064\n",
      "   ‚è±Ô∏è Time: 3.71s\n",
      "\n",
      "üìä Evaluating on November 2015...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in labels with no true or predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9903\n",
      "      Hamming Loss: 0.0012\n",
      "      Jaccard (Macro): 0.3299\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0254, Precision=0.0141\n",
      "      k=3: MAP=0.0254, Precision=0.0096\n",
      "      k=4: MAP=0.0254, Precision=0.0072\n",
      "      k=5: MAP=0.0255, Precision=0.0058\n",
      "   ‚è±Ô∏è Time: 4.65s\n",
      "\n",
      "üìä Evaluating on December 2015...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9913\n",
      "      Hamming Loss: 0.0011\n",
      "      Jaccard (Macro): 0.3186\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0201, Precision=0.0110\n",
      "      k=3: MAP=0.0200, Precision=0.0074\n",
      "      k=4: MAP=0.0201, Precision=0.0056\n",
      "      k=5: MAP=0.0201, Precision=0.0045\n",
      "   ‚è±Ô∏è Time: 5.57s\n",
      "\n",
      "üéØ Overall Performance:\n",
      "   Overall k=2: MAP=0.0239, Precision=0.0132\n",
      "   Overall k=3: MAP=0.0239, Precision=0.0090\n",
      "   Overall k=4: MAP=0.0239, Precision=0.0068\n",
      "   Overall k=5: MAP=0.0240, Precision=0.0055\n",
      "\n",
      "‚úÖ LightGBM evaluation completed!\n",
      "üîß Pipeline saved for future use\n",
      "\n",
      "üéâ ALL MODELS TRAINED AND EVALUATED!\n",
      "üìä SUMMARY:\n",
      "   Models evaluated: 3\n",
      "   Total result rows: 12\n",
      "   Test datasets: 3\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# TRAIN ALL MODELS AND EVALUATE COMPREHENSIVELY\n",
    "# =============================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train = df_enhanced[available_num_cols + available_cat_cols]\n",
    "y_train = df_enhanced[tar_cols]\n",
    "\n",
    "# Storage for all results\n",
    "all_model_results = []\n",
    "all_detailed_results = {}\n",
    "training_results = {}\n",
    "\n",
    "print(\"üöÄ TRAINING AND EVALUATING ALL MODELS...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"\\nü§ñ TRAINING {model_name.upper()}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # =============================\n",
    "    # TRAIN MODEL\n",
    "    # =============================\n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Training metrics\n",
    "    print(\"üìä Calculating training metrics...\")\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    print(f\"   ‚úÖ Training completed in {training_time:.2f}s\")\n",
    "    print(f\"   üìà Train Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Store training results\n",
    "    training_results[model_name] = {\n",
    "        'pipeline': pipeline,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    # =============================\n",
    "    # EVALUATE ON TEST SETS\n",
    "    # =============================\n",
    "    model_results_df, detailed_results = evaluate_model_comprehensive(\n",
    "        pipeline, model_name, processed_test_data, tar_cols\n",
    "    )\n",
    "    \n",
    "    # Add training info to results\n",
    "    model_results_df['Training_Time'] = training_time\n",
    "    model_results_df['Train_Accuracy'] = train_accuracy\n",
    "    \n",
    "    # Store results\n",
    "    all_model_results.append(model_results_df)\n",
    "    all_detailed_results[model_name] = detailed_results\n",
    "    \n",
    "    print(f\"\\n‚úÖ {model_name} evaluation completed!\")\n",
    "    print(f\"üîß Pipeline saved for future use\")\n",
    "\n",
    "# =============================\n",
    "# COMBINE ALL RESULTS\n",
    "# =============================\n",
    "print(f\"\\nüéâ ALL MODELS TRAINED AND EVALUATED!\")\n",
    "final_results_df = pd.concat(all_model_results, ignore_index=True)\n",
    "\n",
    "print(f\"üìä SUMMARY:\")\n",
    "print(f\"   Models evaluated: {len(pipelines)}\")\n",
    "print(f\"   Total result rows: {len(final_results_df)}\")\n",
    "print(f\"   Test datasets: {len(processed_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01d489a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DETAILED RESULTS TABLE:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Month</th>\n",
       "      <th>Total_Customers</th>\n",
       "      <th>Customers_With_Products</th>\n",
       "      <th>Coverage_Rate</th>\n",
       "      <th>Avg_Products_Per_Customer</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Hamming_Loss</th>\n",
       "      <th>Jaccard_Macro</th>\n",
       "      <th>Prediction_Time</th>\n",
       "      <th>MAP@2</th>\n",
       "      <th>Precision@2</th>\n",
       "      <th>MAP@3</th>\n",
       "      <th>Precision@3</th>\n",
       "      <th>MAP@4</th>\n",
       "      <th>Precision@4</th>\n",
       "      <th>MAP@5</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Training_Time</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>1017191</td>\n",
       "      <td>31170</td>\n",
       "      <td>0.030643</td>\n",
       "      <td>0.035039</td>\n",
       "      <td>0.996449</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.582871</td>\n",
       "      <td>12.560119</td>\n",
       "      <td>0.014018</td>\n",
       "      <td>0.014931</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>0.015285</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>0.015368</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>51.279366</td>\n",
       "      <td>0.998161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>November 2015</td>\n",
       "      <td>1223624</td>\n",
       "      <td>34541</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>0.031845</td>\n",
       "      <td>0.997064</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.591792</td>\n",
       "      <td>15.111525</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>0.013746</td>\n",
       "      <td>0.013819</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.014063</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>51.279366</td>\n",
       "      <td>0.998161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>December 2015</td>\n",
       "      <td>1425848</td>\n",
       "      <td>31935</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.024978</td>\n",
       "      <td>0.997516</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.563023</td>\n",
       "      <td>17.599548</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.010875</td>\n",
       "      <td>0.010964</td>\n",
       "      <td>0.007951</td>\n",
       "      <td>0.011163</td>\n",
       "      <td>0.006166</td>\n",
       "      <td>0.011202</td>\n",
       "      <td>0.004972</td>\n",
       "      <td>51.279366</td>\n",
       "      <td>0.998161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>OVERALL</td>\n",
       "      <td>3666663</td>\n",
       "      <td>97646</td>\n",
       "      <td>0.026631</td>\n",
       "      <td>0.030061</td>\n",
       "      <td>0.997010</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.579229</td>\n",
       "      <td>45.271192</td>\n",
       "      <td>0.012243</td>\n",
       "      <td>0.012958</td>\n",
       "      <td>0.013036</td>\n",
       "      <td>0.009530</td>\n",
       "      <td>0.013274</td>\n",
       "      <td>0.007393</td>\n",
       "      <td>0.013335</td>\n",
       "      <td>0.005976</td>\n",
       "      <td>51.279366</td>\n",
       "      <td>0.998161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>1017191</td>\n",
       "      <td>31170</td>\n",
       "      <td>0.030643</td>\n",
       "      <td>0.035039</td>\n",
       "      <td>0.998448</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.689261</td>\n",
       "      <td>2.321311</td>\n",
       "      <td>0.030052</td>\n",
       "      <td>0.016814</td>\n",
       "      <td>0.030203</td>\n",
       "      <td>0.011567</td>\n",
       "      <td>0.030267</td>\n",
       "      <td>0.008754</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.007007</td>\n",
       "      <td>7.906032</td>\n",
       "      <td>0.999808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>November 2015</td>\n",
       "      <td>1223624</td>\n",
       "      <td>34541</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>0.031845</td>\n",
       "      <td>0.998652</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.678838</td>\n",
       "      <td>2.920624</td>\n",
       "      <td>0.027675</td>\n",
       "      <td>0.015315</td>\n",
       "      <td>0.027828</td>\n",
       "      <td>0.010539</td>\n",
       "      <td>0.027871</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>0.027874</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>7.906032</td>\n",
       "      <td>0.999808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>December 2015</td>\n",
       "      <td>1425848</td>\n",
       "      <td>31935</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.024978</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.655683</td>\n",
       "      <td>3.294385</td>\n",
       "      <td>0.021937</td>\n",
       "      <td>0.012020</td>\n",
       "      <td>0.022065</td>\n",
       "      <td>0.008270</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.006242</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.004994</td>\n",
       "      <td>7.906032</td>\n",
       "      <td>0.999808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>OVERALL</td>\n",
       "      <td>3666663</td>\n",
       "      <td>97646</td>\n",
       "      <td>0.026631</td>\n",
       "      <td>0.030061</td>\n",
       "      <td>0.998666</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.674594</td>\n",
       "      <td>8.536320</td>\n",
       "      <td>0.026103</td>\n",
       "      <td>0.014450</td>\n",
       "      <td>0.026246</td>\n",
       "      <td>0.009942</td>\n",
       "      <td>0.026291</td>\n",
       "      <td>0.007511</td>\n",
       "      <td>0.026293</td>\n",
       "      <td>0.006011</td>\n",
       "      <td>7.906032</td>\n",
       "      <td>0.999808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>1017191</td>\n",
       "      <td>31170</td>\n",
       "      <td>0.030643</td>\n",
       "      <td>0.035039</td>\n",
       "      <td>0.990609</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.337837</td>\n",
       "      <td>3.706887</td>\n",
       "      <td>0.027493</td>\n",
       "      <td>0.015406</td>\n",
       "      <td>0.027544</td>\n",
       "      <td>0.010513</td>\n",
       "      <td>0.027564</td>\n",
       "      <td>0.007917</td>\n",
       "      <td>0.027631</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>7.566023</td>\n",
       "      <td>0.992305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>November 2015</td>\n",
       "      <td>1223624</td>\n",
       "      <td>34541</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>0.031845</td>\n",
       "      <td>0.990329</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.329925</td>\n",
       "      <td>4.652761</td>\n",
       "      <td>0.025368</td>\n",
       "      <td>0.014070</td>\n",
       "      <td>0.025370</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.025387</td>\n",
       "      <td>0.007190</td>\n",
       "      <td>0.025455</td>\n",
       "      <td>0.005820</td>\n",
       "      <td>7.566023</td>\n",
       "      <td>0.992305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>December 2015</td>\n",
       "      <td>1425848</td>\n",
       "      <td>31935</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.024978</td>\n",
       "      <td>0.991287</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.318576</td>\n",
       "      <td>5.569417</td>\n",
       "      <td>0.020054</td>\n",
       "      <td>0.010985</td>\n",
       "      <td>0.020034</td>\n",
       "      <td>0.007427</td>\n",
       "      <td>0.020055</td>\n",
       "      <td>0.005595</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.004539</td>\n",
       "      <td>7.566023</td>\n",
       "      <td>0.992305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>OVERALL</td>\n",
       "      <td>3666663</td>\n",
       "      <td>97646</td>\n",
       "      <td>0.026631</td>\n",
       "      <td>0.030061</td>\n",
       "      <td>0.990742</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.328779</td>\n",
       "      <td>13.929065</td>\n",
       "      <td>0.023891</td>\n",
       "      <td>0.013241</td>\n",
       "      <td>0.023898</td>\n",
       "      <td>0.008992</td>\n",
       "      <td>0.023918</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0.023982</td>\n",
       "      <td>0.005484</td>\n",
       "      <td>7.566023</td>\n",
       "      <td>0.992305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model          Month  Total_Customers  Customers_With_Products  \\\n",
       "0   Random Forest   October 2015          1017191                    31170   \n",
       "1   Random Forest  November 2015          1223624                    34541   \n",
       "2   Random Forest  December 2015          1425848                    31935   \n",
       "3   Random Forest        OVERALL          3666663                    97646   \n",
       "4         XGBoost   October 2015          1017191                    31170   \n",
       "5         XGBoost  November 2015          1223624                    34541   \n",
       "6         XGBoost  December 2015          1425848                    31935   \n",
       "7         XGBoost        OVERALL          3666663                    97646   \n",
       "8        LightGBM   October 2015          1017191                    31170   \n",
       "9        LightGBM  November 2015          1223624                    34541   \n",
       "10       LightGBM  December 2015          1425848                    31935   \n",
       "11       LightGBM        OVERALL          3666663                    97646   \n",
       "\n",
       "    Coverage_Rate  Avg_Products_Per_Customer  Test_Accuracy  Hamming_Loss  \\\n",
       "0        0.030643                   0.035039       0.996449      0.000457   \n",
       "1        0.028228                   0.031845       0.997064      0.000370   \n",
       "2        0.022397                   0.024978       0.997516      0.000309   \n",
       "3        0.026631                   0.030061       0.997010      0.000379   \n",
       "4        0.030643                   0.035039       0.998448      0.000275   \n",
       "5        0.028228                   0.031845       0.998652      0.000234   \n",
       "6        0.022397                   0.024978       0.998900      0.000196   \n",
       "7        0.026631                   0.030061       0.998666      0.000235   \n",
       "8        0.030643                   0.035039       0.990609      0.001168   \n",
       "9        0.028228                   0.031845       0.990329      0.001189   \n",
       "10       0.022397                   0.024978       0.991287      0.001065   \n",
       "11       0.026631                   0.030061       0.990742      0.001141   \n",
       "\n",
       "    Jaccard_Macro  Prediction_Time     MAP@2  Precision@2     MAP@3  \\\n",
       "0        0.582871        12.560119  0.014018     0.014931  0.015000   \n",
       "1        0.591792        15.111525  0.012989     0.013746  0.013819   \n",
       "2        0.563023        17.599548  0.010338     0.010875  0.010964   \n",
       "3        0.579229        45.271192  0.012243     0.012958  0.013036   \n",
       "4        0.689261         2.321311  0.030052     0.016814  0.030203   \n",
       "5        0.678838         2.920624  0.027675     0.015315  0.027828   \n",
       "6        0.655683         3.294385  0.021937     0.012020  0.022065   \n",
       "7        0.674594         8.536320  0.026103     0.014450  0.026246   \n",
       "8        0.337837         3.706887  0.027493     0.015406  0.027544   \n",
       "9        0.329925         4.652761  0.025368     0.014070  0.025370   \n",
       "10       0.318576         5.569417  0.020054     0.010985  0.020034   \n",
       "11       0.328779        13.929065  0.023891     0.013241  0.023898   \n",
       "\n",
       "    Precision@3     MAP@4  Precision@4     MAP@5  Precision@5  Training_Time  \\\n",
       "0      0.011062  0.015285     0.008593  0.015368     0.006958      51.279366   \n",
       "1      0.010097  0.014063     0.007825  0.014131     0.006329      51.279366   \n",
       "2      0.007951  0.011163     0.006166  0.011202     0.004972      51.279366   \n",
       "3      0.009530  0.013274     0.007393  0.013335     0.005976      51.279366   \n",
       "4      0.011567  0.030267     0.008754  0.030270     0.007007       7.906032   \n",
       "5      0.010539  0.027871     0.007957  0.027874     0.006369       7.906032   \n",
       "6      0.008270  0.022100     0.006242  0.022100     0.004994       7.906032   \n",
       "7      0.009942  0.026291     0.007511  0.026293     0.006011       7.906032   \n",
       "8      0.010513  0.027564     0.007917  0.027631     0.006404       7.566023   \n",
       "9      0.009550  0.025387     0.007190  0.025455     0.005820       7.566023   \n",
       "10     0.007427  0.020055     0.005595  0.020116     0.004539       7.566023   \n",
       "11     0.008992  0.023918     0.006771  0.023982     0.005484       7.566023   \n",
       "\n",
       "    Train_Accuracy  \n",
       "0         0.998161  \n",
       "1         0.998161  \n",
       "2         0.998161  \n",
       "3         0.998161  \n",
       "4         0.999808  \n",
       "5         0.999808  \n",
       "6         0.999808  \n",
       "7         0.999808  \n",
       "8         0.992305  \n",
       "9         0.992305  \n",
       "10        0.992305  \n",
       "11        0.992305  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ MODEL PERFORMANCE SUMMARY:\n",
      "================================================================================\n",
      "\n",
      "üéØ RANKING BY MAP@5:\n",
      "--------------------------------------------------\n",
      "1. XGBoost: MAP@5 = 0.0263\n",
      "2. LightGBM: MAP@5 = 0.0240\n",
      "3. Random Forest: MAP@5 = 0.0133\n",
      "\n",
      "üìã KEY METRICS COMPARISON:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAP@2</th>\n",
       "      <th>MAP@3</th>\n",
       "      <th>MAP@4</th>\n",
       "      <th>MAP@5</th>\n",
       "      <th>Precision@2</th>\n",
       "      <th>Precision@3</th>\n",
       "      <th>Precision@4</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Training_Time</th>\n",
       "      <th>Prediction_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>51.2794</td>\n",
       "      <td>45.2712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>7.9060</td>\n",
       "      <td>8.5363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>7.5660</td>\n",
       "      <td>13.9291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model   MAP@2   MAP@3   MAP@4   MAP@5  Precision@2  Precision@3  \\\n",
       "3   Random Forest  0.0122  0.0130  0.0133  0.0133       0.0130       0.0095   \n",
       "7         XGBoost  0.0261  0.0262  0.0263  0.0263       0.0144       0.0099   \n",
       "11       LightGBM  0.0239  0.0239  0.0239  0.0240       0.0132       0.0090   \n",
       "\n",
       "    Precision@4  Precision@5  Test_Accuracy  Training_Time  Prediction_Time  \n",
       "3        0.0074       0.0060         0.9970        51.2794          45.2712  \n",
       "7        0.0075       0.0060         0.9987         7.9060           8.5363  \n",
       "11       0.0068       0.0055         0.9907         7.5660          13.9291  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç DETAILED PERFORMANCE ANALYSIS:\n",
      "================================================================================\n",
      "\n",
      "ü§ñ RANDOM FOREST PERFORMANCE:\n",
      "--------------------------------------------------\n",
      "   üìà MAP Scores:\n",
      "      MAP@2: 0.0122\n",
      "      MAP@3: 0.0130\n",
      "      MAP@4: 0.0133\n",
      "      MAP@5: 0.0133\n",
      "   üéØ Precision Scores:\n",
      "      Precision@2: 0.0130\n",
      "      Precision@3: 0.0095\n",
      "      Precision@4: 0.0074\n",
      "      Precision@5: 0.0060\n",
      "   ‚ö° Performance Metrics:\n",
      "      Test Accuracy: 0.9970\n",
      "      Training Time: 51.28s\n",
      "      Prediction Time: 45.27s\n",
      "      Total Customers: 3,666,663\n",
      "\n",
      "ü§ñ XGBOOST PERFORMANCE:\n",
      "--------------------------------------------------\n",
      "   üìà MAP Scores:\n",
      "      MAP@2: 0.0261\n",
      "      MAP@3: 0.0262\n",
      "      MAP@4: 0.0263\n",
      "      MAP@5: 0.0263\n",
      "   üéØ Precision Scores:\n",
      "      Precision@2: 0.0144\n",
      "      Precision@3: 0.0099\n",
      "      Precision@4: 0.0075\n",
      "      Precision@5: 0.0060\n",
      "   ‚ö° Performance Metrics:\n",
      "      Test Accuracy: 0.9987\n",
      "      Training Time: 7.91s\n",
      "      Prediction Time: 8.54s\n",
      "      Total Customers: 3,666,663\n",
      "\n",
      "ü§ñ LIGHTGBM PERFORMANCE:\n",
      "--------------------------------------------------\n",
      "   üìà MAP Scores:\n",
      "      MAP@2: 0.0239\n",
      "      MAP@3: 0.0239\n",
      "      MAP@4: 0.0239\n",
      "      MAP@5: 0.0240\n",
      "   üéØ Precision Scores:\n",
      "      Precision@2: 0.0132\n",
      "      Precision@3: 0.0090\n",
      "      Precision@4: 0.0068\n",
      "      Precision@5: 0.0055\n",
      "   ‚ö° Performance Metrics:\n",
      "      Test Accuracy: 0.9907\n",
      "      Training Time: 7.57s\n",
      "      Prediction Time: 13.93s\n",
      "      Total Customers: 3,666,663\n",
      "\n",
      "üìÖ MONTHLY PERFORMANCE BREAKDOWN:\n",
      "================================================================================\n",
      "\n",
      "üìä October 2015:\n",
      "   XGBoost: MAP@5=0.0303, Precision@5=0.0070, Accuracy=0.9984\n",
      "   LightGBM: MAP@5=0.0276, Precision@5=0.0064, Accuracy=0.9906\n",
      "   Random Forest: MAP@5=0.0154, Precision@5=0.0070, Accuracy=0.9964\n",
      "\n",
      "üìä November 2015:\n",
      "   XGBoost: MAP@5=0.0279, Precision@5=0.0064, Accuracy=0.9987\n",
      "   LightGBM: MAP@5=0.0255, Precision@5=0.0058, Accuracy=0.9903\n",
      "   Random Forest: MAP@5=0.0141, Precision@5=0.0063, Accuracy=0.9971\n",
      "\n",
      "üìä December 2015:\n",
      "   XGBoost: MAP@5=0.0221, Precision@5=0.0050, Accuracy=0.9989\n",
      "   LightGBM: MAP@5=0.0201, Precision@5=0.0045, Accuracy=0.9913\n",
      "   Random Forest: MAP@5=0.0112, Precision@5=0.0050, Accuracy=0.9975\n",
      "\n",
      "üèÖ BEST MODEL IDENTIFICATION:\n",
      "================================================================================\n",
      "üèÜ Best MAP@2: XGBoost (0.0261)\n",
      "üèÜ Best MAP@3: XGBoost (0.0262)\n",
      "üèÜ Best MAP@4: XGBoost (0.0263)\n",
      "üèÜ Best MAP@5: XGBoost (0.0263)\n",
      "üèÜ Best Precision@2: XGBoost (0.0144)\n",
      "üèÜ Best Precision@3: XGBoost (0.0099)\n",
      "üèÜ Best Precision@4: XGBoost (0.0075)\n",
      "üèÜ Best Precision@5: XGBoost (0.0060)\n",
      "üèÜ Best Test_Accuracy: XGBoost (0.9987)\n",
      "\n",
      "üéñÔ∏è OVERALL WINNER: XGBoost (9 best scores)\n",
      "\n",
      "üí° PERFORMANCE INSIGHTS:\n",
      "================================================================================\n",
      "üìà MAP@k Trends:\n",
      "   Random Forest: üìà Improving (MAP@2=0.012 ‚Üí MAP@5=0.013)\n",
      "   XGBoost: üìà Improving (MAP@2=0.026 ‚Üí MAP@5=0.026)\n",
      "   LightGBM: üìà Improving (MAP@2=0.024 ‚Üí MAP@5=0.024)\n",
      "\n",
      "‚ö° Speed vs Accuracy Analysis:\n",
      "   Random Forest: Efficiency Ratio = 0.26 (MAP@5/training_time)\n",
      "   XGBoost: Efficiency Ratio = 3.33 (MAP@5/training_time)\n",
      "   LightGBM: Efficiency Ratio = 3.17 (MAP@5/training_time)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# COMPREHENSIVE MODEL COMPARISON AND SUMMARY\n",
    "# =============================\n",
    "\n",
    "# Display final results\n",
    "print(\"üìä DETAILED RESULTS TABLE:\")\n",
    "print(\"=\"*80)\n",
    "display(final_results_df)\n",
    "\n",
    "# =============================\n",
    "# MODEL PERFORMANCE SUMMARY\n",
    "# =============================\n",
    "\n",
    "print(\"\\nüèÜ MODEL PERFORMANCE SUMMARY:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall performance comparison (filter OVERALL rows only)\n",
    "overall_results = final_results_df[final_results_df['Month'] == 'OVERALL'].copy()\n",
    "\n",
    "if not overall_results.empty:\n",
    "    # Sort by MAP@5 (primary metric)\n",
    "    overall_results_sorted = overall_results.sort_values('MAP@5', ascending=False)\n",
    "    \n",
    "    print(\"\\nüéØ RANKING BY MAP@5:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (_, row) in enumerate(overall_results_sorted.iterrows(), 1):\n",
    "        print(f\"{i}. {row['Model']}: MAP@5 = {row['MAP@5']:.4f}\")\n",
    "    \n",
    "    # Key metrics comparison table\n",
    "    print(f\"\\nüìã KEY METRICS COMPARISON:\")\n",
    "    comparison_metrics = ['Model', 'MAP@2', 'MAP@3', 'MAP@4', 'MAP@5', \n",
    "                         'Precision@2', 'Precision@3', 'Precision@4', 'Precision@5',\n",
    "                         'Test_Accuracy', 'Training_Time', 'Prediction_Time']\n",
    "    \n",
    "    summary_table = overall_results[comparison_metrics].round(4)\n",
    "    display(summary_table)\n",
    "\n",
    "# =============================\n",
    "# DETAILED PERFORMANCE ANALYSIS\n",
    "# =============================\n",
    "\n",
    "print(f\"\\nüîç DETAILED PERFORMANCE ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in overall_results['Model'].values:\n",
    "    print(f\"\\nü§ñ {model_name.upper()} PERFORMANCE:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    model_data = overall_results[overall_results['Model'] == model_name].iloc[0]\n",
    "    \n",
    "    # MAP@k scores\n",
    "    print(f\"   üìà MAP Scores:\")\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        map_score = model_data[f'MAP@{k}']\n",
    "        print(f\"      MAP@{k}: {map_score:.4f}\")\n",
    "    \n",
    "    # Precision@k scores  \n",
    "    print(f\"   üéØ Precision Scores:\")\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        precision_score = model_data[f'Precision@{k}']\n",
    "        print(f\"      Precision@{k}: {precision_score:.4f}\")\n",
    "    \n",
    "    # Other metrics\n",
    "    print(f\"   ‚ö° Performance Metrics:\")\n",
    "    print(f\"      Test Accuracy: {model_data['Test_Accuracy']:.4f}\")\n",
    "    print(f\"      Training Time: {model_data['Training_Time']:.2f}s\")\n",
    "    print(f\"      Prediction Time: {model_data['Prediction_Time']:.2f}s\")\n",
    "    print(f\"      Total Customers: {model_data['Total_Customers']:,}\")\n",
    "\n",
    "# =============================\n",
    "# MONTHLY PERFORMANCE BREAKDOWN\n",
    "# =============================\n",
    "\n",
    "print(f\"\\nüìÖ MONTHLY PERFORMANCE BREAKDOWN:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "monthly_results = final_results_df[final_results_df['Month'] != 'OVERALL'].copy()\n",
    "\n",
    "if not monthly_results.empty:\n",
    "    # Group by month and show MAP@5 for each model\n",
    "    for month in monthly_results['Month'].unique():\n",
    "        print(f\"\\nüìä {month}:\")\n",
    "        month_data = monthly_results[monthly_results['Month'] == month]\n",
    "        month_sorted = month_data.sort_values('MAP@5', ascending=False)\n",
    "        \n",
    "        for _, row in month_sorted.iterrows():\n",
    "            print(f\"   {row['Model']}: MAP@5={row['MAP@5']:.4f}, \"\n",
    "                  f\"Precision@5={row['Precision@5']:.4f}, \"\n",
    "                  f\"Accuracy={row['Test_Accuracy']:.4f}\")\n",
    "\n",
    "# =============================\n",
    "# BEST MODEL IDENTIFICATION\n",
    "# =============================\n",
    "\n",
    "print(f\"\\nüèÖ BEST MODEL IDENTIFICATION:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not overall_results.empty:\n",
    "    # Best model by different metrics\n",
    "    best_models = {}\n",
    "    \n",
    "    metrics_to_check = ['MAP@2', 'MAP@3', 'MAP@4', 'MAP@5', \n",
    "                       'Precision@2', 'Precision@3', 'Precision@4', 'Precision@5',\n",
    "                       'Test_Accuracy']\n",
    "    \n",
    "    for metric in metrics_to_check:\n",
    "        best_idx = overall_results[metric].idxmax()\n",
    "        best_model = overall_results.loc[best_idx, 'Model']\n",
    "        best_score = overall_results.loc[best_idx, metric]\n",
    "        best_models[metric] = (best_model, best_score)\n",
    "        print(f\"üèÜ Best {metric}: {best_model} ({best_score:.4f})\")\n",
    "    \n",
    "    # Overall winner (most wins)\n",
    "    model_wins = {}\n",
    "    for metric, (model, score) in best_models.items():\n",
    "        model_wins[model] = model_wins.get(model, 0) + 1\n",
    "    \n",
    "    overall_winner = max(model_wins.items(), key=lambda x: x[1])\n",
    "    print(f\"\\nüéñÔ∏è OVERALL WINNER: {overall_winner[0]} ({overall_winner[1]} best scores)\")\n",
    "\n",
    "# =============================\n",
    "# PERFORMANCE INSIGHTS\n",
    "# =============================\n",
    "\n",
    "print(f\"\\nüí° PERFORMANCE INSIGHTS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not overall_results.empty:\n",
    "    # MAP@k trend analysis\n",
    "    print(f\"üìà MAP@k Trends:\")\n",
    "    for _, row in overall_results.iterrows():\n",
    "        model = row['Model']\n",
    "        map_scores = [row[f'MAP@{k}'] for k in [2, 3, 4, 5]]\n",
    "        trend = \"üìà Improving\" if all(map_scores[i] <= map_scores[i+1] for i in range(len(map_scores)-1)) \\\n",
    "                else \"üìâ Declining\" if all(map_scores[i] >= map_scores[i+1] for i in range(len(map_scores)-1)) \\\n",
    "                else \"üìä Mixed\"\n",
    "        print(f\"   {model}: {trend} (MAP@2={map_scores[0]:.3f} ‚Üí MAP@5={map_scores[3]:.3f})\")\n",
    "    \n",
    "    # Speed vs Accuracy analysis\n",
    "    print(f\"\\n‚ö° Speed vs Accuracy Analysis:\")\n",
    "    for _, row in overall_results.iterrows():\n",
    "        model = row['Model']\n",
    "        speed_score = 1 / (row['Training_Time'] + row['Prediction_Time'])  # Inverse of total time\n",
    "        accuracy_score = row['MAP@5']\n",
    "        efficiency_ratio = accuracy_score / row['Training_Time'] * 1000  # MAP@5 per second * 1000\n",
    "        print(f\"   {model}: Efficiency Ratio = {efficiency_ratio:.2f} (MAP@5/training_time)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50605daa",
   "metadata": {},
   "source": [
    "# Recommendation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59dd37",
   "metadata": {},
   "source": [
    "## Collaborative filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25d0795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CollaborativeFilteringRecommender:\n",
    "    \"\"\"\n",
    "    User-based collaborative filtering for product recommendations.\n",
    "    \n",
    "    Uses cosine similarity to find similar users and recommend products.\n",
    "    Supports both dense and sparse matrices for scalability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_sparse=True):\n",
    "        \"\"\"Initialize with option to use sparse matrices for large datasets.\"\"\"\n",
    "        self.user_item_matrix = None\n",
    "        self.similarity_matrix = None\n",
    "        self.user_means = None\n",
    "        self.user_index_map = {}\n",
    "        self.item_index_map = {}\n",
    "        self.use_sparse = use_sparse\n",
    "        \n",
    "    def create_user_item_matrix(self, df, product_cols, sample_size=None):\n",
    "        \"\"\"Create matrix of users and their product interactions.\"\"\"\n",
    "        if sample_size and len(df) > sample_size:\n",
    "            df = df.sample(sample_size, random_state=42)\n",
    "        latest_data = df.groupby('customer_id').last().reset_index()\n",
    "        user_item_data = latest_data[['customer_id'] + product_cols].set_index('customer_id')\n",
    "        self.user_index_map = {user_id: idx for idx, user_id in enumerate(user_item_data.index)}\n",
    "        self.item_index_map = {item: idx for idx, item in enumerate(product_cols)}\n",
    "        user_item_data = user_item_data.fillna(0)\n",
    "        if self.use_sparse:\n",
    "            self.user_item_matrix = sparse.csr_matrix(user_item_data.values)\n",
    "            self.matrix_columns = product_cols\n",
    "            self.matrix_index = user_item_data.index\n",
    "        else:\n",
    "            self.user_item_matrix = user_item_data\n",
    "        return self.user_item_matrix\n",
    "            \n",
    "    def compute_similarity_matrix(self, chunk_size=1000):\n",
    "        \"\"\"Calculate similarity between users using cosine similarity.\"\"\"\n",
    "        if self.use_sparse:\n",
    "            n_users = self.user_item_matrix.shape[0]\n",
    "            similarity_matrix = np.zeros((n_users, n_users))\n",
    "            for i in tqdm(range(0, n_users, chunk_size), desc=\"Computing similarity\"):\n",
    "                end = min(i + chunk_size, n_users)\n",
    "                chunk = self.user_item_matrix[i:end]\n",
    "                chunk_similarities = cosine_similarity(chunk, self.user_item_matrix)\n",
    "                similarity_matrix[i:end] = chunk_similarities\n",
    "        else:\n",
    "            similarity_matrix = cosine_similarity(self.user_item_matrix)\n",
    "        np.fill_diagonal(similarity_matrix, 0)\n",
    "        self.similarity_matrix = similarity_matrix\n",
    "        return similarity_matrix\n",
    "            \n",
    "    def fit(self, df, product_cols, sample_size=None):\n",
    "        \"\"\"Prepare recommender by creating matrices and computing similarities.\"\"\"\n",
    "        self.create_user_item_matrix(df, product_cols, sample_size)\n",
    "        self.compute_similarity_matrix()\n",
    "        if self.use_sparse:\n",
    "            self.user_means = np.array([\n",
    "                self.user_item_matrix[i].sum() / max(1, self.user_item_matrix[i].count_nonzero()) \n",
    "                for i in range(self.user_item_matrix.shape[0])\n",
    "            ])\n",
    "        else:\n",
    "            self.user_means = self.user_item_matrix.mean(axis=1)\n",
    "                \n",
    "    def predict_for_user(self, user_idx, top_k=7, n_neighbors=50):\n",
    "        \"\"\"Recommend top products for a specific user based on similar users.\"\"\"\n",
    "        if self.use_sparse:\n",
    "            user_items = self.user_item_matrix[user_idx].toarray().flatten()\n",
    "        else:\n",
    "            user_items = self.user_item_matrix.iloc[user_idx]\n",
    "        user_similarities = self.similarity_matrix[user_idx]\n",
    "        similar_users_idx = np.argsort(user_similarities)[::-1][:n_neighbors]\n",
    "        predictions = {}\n",
    "        for item_idx in range(len(self.matrix_columns if self.use_sparse else self.user_item_matrix.columns)):\n",
    "            item_name = self.matrix_columns[item_idx] if self.use_sparse else self.user_item_matrix.columns[item_idx]\n",
    "            if user_items[item_idx] == 0:\n",
    "                numerator = denominator = 0\n",
    "                for similar_user_idx in similar_users_idx:\n",
    "                    if similar_user_idx != user_idx:\n",
    "                        similarity = user_similarities[similar_user_idx]\n",
    "                        if similarity > 0:\n",
    "                            if self.use_sparse:\n",
    "                                rating = self.user_item_matrix[similar_user_idx, item_idx]\n",
    "                            else:\n",
    "                                rating = self.user_item_matrix.iloc[similar_user_idx, item_idx]\n",
    "                            numerator += similarity * rating\n",
    "                            denominator += similarity\n",
    "                if denominator > 0:\n",
    "                    predicted_rating = numerator / denominator\n",
    "                    predictions[item_name] = predicted_rating\n",
    "        sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [item for item, _ in sorted_predictions[:top_k]]\n",
    "    \n",
    "    def predict_for_users(self, user_ids, top_k=7, batch_size=100):\n",
    "        \"\"\"Generate recommendations for multiple users in batches.\"\"\"\n",
    "        predictions = {}\n",
    "        for i in range(0, len(user_ids), batch_size):\n",
    "            batch_users = user_ids[i:i+batch_size]\n",
    "            for user_id in tqdm(batch_users, desc=f\"User-based CF predictions (batch {i//batch_size + 1})\"):\n",
    "                if user_id in self.user_index_map:\n",
    "                    user_idx = self.user_index_map[user_id]\n",
    "                    predictions[user_id] = self.predict_for_user(user_idx, top_k)\n",
    "                else:\n",
    "                    predictions[user_id] = self._get_popular_items(top_k)\n",
    "        return predictions\n",
    "    \n",
    "    def _get_popular_items(self, top_k=7):\n",
    "        \"\"\"Return most popular products as fallback recommendations.\"\"\"\n",
    "        if self.use_sparse:\n",
    "            item_popularity = np.array(self.user_item_matrix.sum(axis=0)).flatten()\n",
    "            popular_indices = np.argsort(item_popularity)[::-1][:top_k]\n",
    "            return [self.matrix_columns[i] for i in popular_indices]\n",
    "        else:\n",
    "            item_popularity = self.user_item_matrix.sum(axis=0).sort_values(ascending=False)\n",
    "            return item_popularity.head(top_k).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25950419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEGION\\AppData\\Local\\Temp\\ipykernel_9780\\1227308709.py:1: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/processed/train.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/processed/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916d084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 781.94 Mb (53.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "df= reduce_memory_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a32cd348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ CLEANING DATASET...\n",
      "========================================\n",
      "1Ô∏è‚É£ Handling missing values...\n",
      "   ‚úÖ Filled payroll_final_label: 0 missing ‚Üí 0\n",
      "   ‚úÖ Filled pensions_2_final_label: 0 missing ‚Üí 0\n",
      "\n",
      "2Ô∏è‚É£ Creating customer tenure feature...\n",
      "   ‚úÖ Created 'customer_tenure_days' feature\n",
      "   üìä Range: -3.0 to 7498.0 days\n",
      "   üóëÔ∏è Dropped 'registration_date' column\n",
      "\n",
      "3Ô∏è‚É£ Converting last_primary_date to binary indicator...\n",
      "   ‚úÖ Created 'was_primary_customer' binary feature\n",
      "   üìä 4,600,165 nulls ‚Üí 0, 6,146 dates ‚Üí 1\n",
      "   üóëÔ∏è Dropped 'last_primary_date' column\n",
      "\n",
      "4Ô∏è‚É£ Removing constant and duplicate columns...\n",
      "   üóëÔ∏è Dropped 'address_type' (constant value)\n",
      "   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\n",
      "\n",
      "5Ô∏è‚É£ Cleaning numeric columns...\n",
      "   ‚úÖ Cleaned 'age': object ‚Üí numeric (27,734 nulls)\n",
      "   ‚ö†Ô∏è Converted 14 negative seniority values to NaN\n",
      "   ‚úÖ Cleaned 'seniority': object ‚Üí numeric (27,748 nulls)\n",
      "\n",
      "üìä CLEANUP SUMMARY:\n",
      "----------------------------------------\n",
      "   Final shape: (4606311, 46)\n",
      "   Memory usage: 2935.7 MB\n",
      "   Null values: 5,979,487\n",
      "\n",
      "üìã DATA TYPES AFTER CLEANUP:\n",
      "   int8: 24 columns\n",
      "   object: 12 columns\n",
      "   float64: 3 columns\n",
      "   float16: 3 columns\n",
      "   datetime64[ns]: 1 columns\n",
      "   int32: 1 columns\n",
      "   float32: 1 columns\n",
      "   int64: 1 columns\n"
     ]
    }
   ],
   "source": [
    "df = clean_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "675f6d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë kh√°ch h√†ng: 442,736\n",
      "Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: 442,736\n",
      "\n",
      "üìä Distribution check:\n",
      "  current_accounts_final_label: [0 1]\n",
      "  payroll_accounts_final_label: [0 1]\n",
      "  junior_accounts_final_label: [0 1]\n",
      "  more_particular_accounts_final_label: [0 1]\n",
      "  particular_accounts_final_label: [0 1]\n",
      "  particular_plus_accounts_final_label: [0 1]\n",
      "  home_account_final_label: [0]\n",
      "  payroll_final_label: [0 1]\n",
      "  e_account_final_label: [0 1]\n"
     ]
    }
   ],
   "source": [
    "df = filter_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5f99b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEGION\\AppData\\Local\\Temp\\ipykernel_9780\\895759621.py:1: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_10_2015 = pd.read_csv('data/processed/test_10_2015.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 903.96 Mb (53.6% reduction)\n",
      "üßπ CLEANING DATASET...\n",
      "========================================\n",
      "1Ô∏è‚É£ Handling missing values...\n",
      "   ‚úÖ Filled payroll_final_label: 0 missing ‚Üí 0\n",
      "   ‚úÖ Filled pensions_2_final_label: 0 missing ‚Üí 0\n",
      "\n",
      "2Ô∏è‚É£ Creating customer tenure feature...\n",
      "   ‚úÖ Created 'customer_tenure_days' feature\n",
      "   üìä Range: -3.0 to 7590.0 days\n",
      "   üóëÔ∏è Dropped 'registration_date' column\n",
      "\n",
      "3Ô∏è‚É£ Converting last_primary_date to binary indicator...\n",
      "   ‚úÖ Created 'was_primary_customer' binary feature\n",
      "   üìä 5,316,273 nulls ‚Üí 0, 8,870 dates ‚Üí 1\n",
      "   üóëÔ∏è Dropped 'last_primary_date' column\n",
      "\n",
      "4Ô∏è‚É£ Removing constant and duplicate columns...\n",
      "   üóëÔ∏è Dropped 'address_type' (constant value)\n",
      "   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\n",
      "\n",
      "5Ô∏è‚É£ Cleaning numeric columns...\n",
      "   ‚úÖ Cleaned 'age': object ‚Üí numeric (9,750 nulls)\n",
      "   ‚ö†Ô∏è Converted 14 negative seniority values to NaN\n",
      "   ‚úÖ Cleaned 'seniority': object ‚Üí numeric (9,764 nulls)\n",
      "\n",
      "üìä CLEANUP SUMMARY:\n",
      "----------------------------------------\n",
      "   Final shape: (5325143, 46)\n",
      "   Memory usage: 3423.5 MB\n",
      "   Null values: 6,810,911\n",
      "\n",
      "üìã DATA TYPES AFTER CLEANUP:\n",
      "   int8: 24 columns\n",
      "   object: 12 columns\n",
      "   float64: 3 columns\n",
      "   float16: 3 columns\n",
      "   datetime64[ns]: 1 columns\n",
      "   int32: 1 columns\n",
      "   float32: 1 columns\n",
      "   int64: 1 columns\n",
      "T·ªïng s·ªë kh√°ch h√†ng: 1,017,191\n",
      "Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: 1,017,191\n",
      "\n",
      "üìä Distribution check:\n",
      "  current_accounts_final_label: [0 1]\n",
      "  payroll_accounts_final_label: [0 1]\n",
      "  junior_accounts_final_label: [0 1]\n",
      "  more_particular_accounts_final_label: [0 1]\n",
      "  particular_accounts_final_label: [0 1]\n",
      "  particular_plus_accounts_final_label: [0 1]\n",
      "  home_account_final_label: [0]\n",
      "  payroll_final_label: [0 1]\n",
      "  e_account_final_label: [0 1]\n"
     ]
    }
   ],
   "source": [
    "test_10_2015 = pd.read_csv('data/processed/test_10_2015.csv')\n",
    "\n",
    "test_10_2015 = filter_data(clean_dataset(reduce_memory_usage(test_10_2015)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e5c99f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Collaborative Filtering models (User-Based and Item-Based) on the test set\n",
    "\n",
    "# Prepare actual products for test users\n",
    "actual_products_cf = {}\n",
    "test_latest_cf = test_10_2015.groupby('customer_id').last().reset_index()\n",
    "for _, row in test_latest_cf.iterrows():\n",
    "    user_id = row['customer_id']\n",
    "    owned_products = [col for col in payment_account_labels if row[col] == 1]\n",
    "    if owned_products:\n",
    "        actual_products_cf[user_id] = owned_products\n",
    "\n",
    "# Select users for evaluation (subset for speed, or use all for full evaluation)\n",
    "eval_users_cf = list(actual_products_cf.keys())[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2408ada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:08<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating User-Based Collaborative Filtering:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "User-based CF predictions (batch 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 3916.51it/s]\n",
      "User-based CF predictions (batch 2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 3070.61it/s]\n",
      "User-based CF predictions (batch 3): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 4330.50it/s]\n",
      "User-based CF predictions (batch 4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 3378.93it/s]\n",
      "User-based CF predictions (batch 5): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 4155.29it/s]\n",
      "User-based CF predictions (batch 6): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 2730.51it/s]\n",
      "User-based CF predictions (batch 7): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 3444.39it/s]\n",
      "User-based CF predictions (batch 8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 2482.76it/s]\n",
      "User-based CF predictions (batch 9): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 3331.72it/s]\n",
      "User-based CF predictions (batch 10): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 3766.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MAP@2: 0.5853\n",
      " Precision@2: 0.3485\n",
      " Users evaluated: 1,000\n",
      " Coverage: 80.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_cf = CollaborativeFilteringRecommender(use_sparse=True)\n",
    "user_cf.fit(df, payment_account_labels, sample_size=50000)\n",
    "\n",
    "print(\"Evaluating User-Based Collaborative Filtering:\")\n",
    "ucf_predictions = user_cf.predict_for_users(eval_users_cf, top_k=2)\n",
    "ucf_actual = {user: actual_products_cf[user] for user in eval_users_cf if user in actual_products_cf}\n",
    "ucf_score = evaluate_recommendations_with_precision(ucf_actual, ucf_predictions, k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de67afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
