{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbffbed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Viz & Regular Expression Libraries :\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# Scikit-Learn ML Libraries :\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Boosting Algorithm Libraries :\n",
    "\n",
    "# from xgboost                          import XGBClassifier\n",
    "# from catboost                         import CatBoostClassifier\n",
    "# from lightgbm                         import LGBMClassifier\n",
    "from sklearn.ensemble                 import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics                  import accuracy_score\n",
    "from sklearn.model_selection          import StratifiedKFold,KFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004f7d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/processed/train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847f5c3",
   "metadata": {},
   "source": [
    "# Reduce memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23b059ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0078533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 781.94 Mb (53.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = reduce_memory_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "463575dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4606311 entries, 0 to 4606310\n",
      "Data columns (total 48 columns):\n",
      " #   Column                                Dtype  \n",
      "---  ------                                -----  \n",
      " 0   date                                  object \n",
      " 1   customer_id                           int32  \n",
      " 2   employee_index                        object \n",
      " 3   country_of_residence                  object \n",
      " 4   gender                                object \n",
      " 5   age                                   object \n",
      " 6   registration_date                     object \n",
      " 7   new_customer                          float16\n",
      " 8   seniority                             object \n",
      " 9   primary_customer                      float16\n",
      " 10  last_primary_date                     object \n",
      " 11  customer_type                         object \n",
      " 12  relation_type                         object \n",
      " 13  residence_index                       object \n",
      " 14  foreigner_index                       object \n",
      " 15  spouse_index                          object \n",
      " 16  channel                               object \n",
      " 17  deceased_index                        object \n",
      " 18  address_type                          float16\n",
      " 19  province_code                         float16\n",
      " 20  province_name                         object \n",
      " 21  activity_index                        float16\n",
      " 22  income                                float32\n",
      " 23  segment                               object \n",
      " 24  savings_account_final_label           int8   \n",
      " 25  guarantees_final_label                int8   \n",
      " 26  current_accounts_final_label          int8   \n",
      " 27  deriv_investments_final_label         int8   \n",
      " 28  payroll_accounts_final_label          int8   \n",
      " 29  junior_accounts_final_label           int8   \n",
      " 30  more_particular_accounts_final_label  int8   \n",
      " 31  particular_accounts_final_label       int8   \n",
      " 32  particular_plus_accounts_final_label  int8   \n",
      " 33  short_term_deposits_final_label       int8   \n",
      " 34  medium_term_deposits_final_label      int8   \n",
      " 35  long_term_deposits_final_label        int8   \n",
      " 36  e_account_final_label                 int8   \n",
      " 37  funds_final_label                     int8   \n",
      " 38  mortgage_final_label                  int8   \n",
      " 39  pensions_final_label                  int8   \n",
      " 40  loans_final_label                     int8   \n",
      " 41  taxes_final_label                     int8   \n",
      " 42  credit_card_final_label               int8   \n",
      " 43  securities_final_label                int8   \n",
      " 44  home_account_final_label              int8   \n",
      " 45  payroll_final_label                   int8   \n",
      " 46  pensions_2_final_label                int8   \n",
      " 47  direct_debit_final_label              int8   \n",
      "dtypes: float16(5), float32(1), int32(1), int8(24), object(17)\n",
      "memory usage: 781.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c630c3",
   "metadata": {},
   "source": [
    "# Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e845db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the banking dataset\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame - Raw dataset\n",
    "    \n",
    "    Returns:\n",
    "    df: pandas DataFrame - Cleaned dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üßπ CLEANING DATASET...\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # =============================\n",
    "    # 1. HANDLE MISSING VALUES\n",
    "    # =============================\n",
    "    print(\"1Ô∏è‚É£ Handling missing values...\")\n",
    "    \n",
    "    # Fill missing values for payroll indicators\n",
    "    # These columns indicate if customer has payroll/pension products\n",
    "    missing_before = df[['payroll_final_label', 'pensions_2_final_label']].isnull().sum()\n",
    "    df.fillna(value={\n",
    "        'payroll_final_label': 0,\n",
    "        'pensions_2_final_label': 0\n",
    "    }, inplace=True)\n",
    "    \n",
    "    print(f\"   ‚úÖ Filled payroll_final_label: {missing_before['payroll_final_label']} missing ‚Üí 0\")\n",
    "    print(f\"   ‚úÖ Filled pensions_2_final_label: {missing_before['pensions_2_final_label']} missing ‚Üí 0\")\n",
    "    \n",
    "    # =============================\n",
    "    # 2. CREATE CUSTOMER TENURE FEATURE\n",
    "    # =============================\n",
    "    print(\"\\n2Ô∏è‚É£ Creating customer tenure feature...\")\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['registration_date'] = pd.to_datetime(df['registration_date'])\n",
    "    \n",
    "    # Calculate days since registration (customer tenure)\n",
    "    days_column = (df['date'] - df['registration_date']).dt.days\n",
    "    \n",
    "    # Insert the new column at position 6\n",
    "    df.insert(loc=6, column='customer_tenure_days', value=days_column)\n",
    "    \n",
    "    print(f\"   ‚úÖ Created 'customer_tenure_days' feature\")\n",
    "    print(f\"   üìä Range: {days_column.min()} to {days_column.max()} days\")\n",
    "    \n",
    "    # Drop the original registration_date column to save memory\n",
    "    df.drop(columns=['registration_date'], inplace=True)\n",
    "    print(f\"   üóëÔ∏è Dropped 'registration_date' column\")\n",
    "    \n",
    "    # =============================\n",
    "    # 3. CONVERT LAST_PRIMARY_DATE TO BINARY INDICATOR\n",
    "    # =============================\n",
    "    print(\"\\n3Ô∏è‚É£ Converting last_primary_date to binary indicator...\")\n",
    "    \n",
    "    # Convert last_primary_date to binary: 1 if date exists, 0 if null\n",
    "    # This indicates if customer was ever a primary customer\n",
    "    original_nulls = df['last_primary_date'].isnull().sum()\n",
    "    df['was_primary_customer'] = df['last_primary_date'].apply(\n",
    "        lambda x: 1 if pd.notnull(x) else 0\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Created 'was_primary_customer' binary feature\")\n",
    "    print(f\"   üìä {original_nulls:,} nulls ‚Üí 0, {len(df) - original_nulls:,} dates ‚Üí 1\")\n",
    "    \n",
    "    # Drop the original last_primary_date column\n",
    "    df.drop(columns=['last_primary_date'], inplace=True)\n",
    "    print(f\"   üóëÔ∏è Dropped 'last_primary_date' column\")\n",
    "    \n",
    "    # =============================\n",
    "    # 4. REMOVE CONSTANT/DUPLICATE COLUMNS\n",
    "    # =============================\n",
    "    print(\"\\n4Ô∏è‚É£ Removing constant and duplicate columns...\")\n",
    "    \n",
    "    # Remove address_type if it has the same value for all customers\n",
    "    if 'address_type' in df.columns:\n",
    "        unique_values = df['address_type'].nunique()\n",
    "        if unique_values <= 1:\n",
    "            df.drop(columns=['address_type'], inplace=True)\n",
    "            print(f\"   üóëÔ∏è Dropped 'address_type' (constant value)\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Kept 'address_type' ({unique_values} unique values)\")\n",
    "    \n",
    "    # Remove province_code as it's duplicate of province_name\n",
    "    if 'province_code' in df.columns and 'province_name' in df.columns:\n",
    "        df.drop(columns=['province_code'], inplace=True)\n",
    "        print(f\"   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\")\n",
    "    \n",
    "    # =============================\n",
    "    # 5. CLEAN NUMERIC COLUMNS\n",
    "    # =============================\n",
    "    print(\"\\n5Ô∏è‚É£ Cleaning numeric columns...\")\n",
    "    \n",
    "    # Convert age column - handle 'NA' strings\n",
    "    if 'age' in df.columns:\n",
    "        age_before = df['age'].dtype\n",
    "        df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "        age_nulls = df['age'].isnull().sum()\n",
    "        print(f\"   ‚úÖ Cleaned 'age': {age_before} ‚Üí numeric ({age_nulls:,} nulls)\")\n",
    "    \n",
    "    # Convert seniority column - handle 'NA' strings and negative values\n",
    "    if 'seniority' in df.columns:\n",
    "        seniority_before = df['seniority'].dtype\n",
    "        df['seniority'] = pd.to_numeric(df['seniority'], errors='coerce')\n",
    "        \n",
    "        # Handle special negative values (often -999999 means missing)\n",
    "        negative_values = (df['seniority'] < 0).sum()\n",
    "        if negative_values > 0:\n",
    "            df['seniority'] = df['seniority'].where(df['seniority'] >= 0, np.nan)\n",
    "            print(f\"   ‚ö†Ô∏è Converted {negative_values:,} negative seniority values to NaN\")\n",
    "        \n",
    "        seniority_nulls = df['seniority'].isnull().sum()\n",
    "        print(f\"   ‚úÖ Cleaned 'seniority': {seniority_before} ‚Üí numeric ({seniority_nulls:,} nulls)\")\n",
    "    \n",
    "    # =============================\n",
    "    # 6. SUMMARY STATISTICS\n",
    "    # =============================\n",
    "    print(\"\\nüìä CLEANUP SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"   Final shape: {df.shape}\")\n",
    "    print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    print(f\"   Null values: {df.isnull().sum().sum():,}\")\n",
    "    \n",
    "    # Show data types\n",
    "    print(f\"\\nüìã DATA TYPES AFTER CLEANUP:\")\n",
    "    for dtype in df.dtypes.value_counts().index:\n",
    "        count = df.dtypes.value_counts()[dtype]\n",
    "        print(f\"   {dtype}: {count} columns\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "839948fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ CLEANING DATASET...\n",
      "========================================\n",
      "1Ô∏è‚É£ Handling missing values...\n",
      "   ‚úÖ Filled payroll_final_label: 0 missing ‚Üí 0\n",
      "   ‚úÖ Filled pensions_2_final_label: 0 missing ‚Üí 0\n",
      "\n",
      "2Ô∏è‚É£ Creating customer tenure feature...\n",
      "   ‚úÖ Created 'customer_tenure_days' feature\n",
      "   üìä Range: -3.0 to 7498.0 days\n",
      "   üóëÔ∏è Dropped 'registration_date' column\n",
      "\n",
      "3Ô∏è‚É£ Converting last_primary_date to binary indicator...\n",
      "   ‚úÖ Created 'was_primary_customer' binary feature\n",
      "   üìä 4,600,165 nulls ‚Üí 0, 6,146 dates ‚Üí 1\n",
      "   üóëÔ∏è Dropped 'last_primary_date' column\n",
      "\n",
      "4Ô∏è‚É£ Removing constant and duplicate columns...\n",
      "   üóëÔ∏è Dropped 'address_type' (constant value)\n",
      "   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\n",
      "\n",
      "5Ô∏è‚É£ Cleaning numeric columns...\n",
      "   ‚úÖ Cleaned 'age': object ‚Üí numeric (27,734 nulls)\n",
      "   ‚ö†Ô∏è Converted 14 negative seniority values to NaN\n",
      "   ‚úÖ Cleaned 'seniority': object ‚Üí numeric (27,748 nulls)\n",
      "\n",
      "üìä CLEANUP SUMMARY:\n",
      "----------------------------------------\n",
      "   Final shape: (4606311, 46)\n",
      "   Memory usage: 2935.7 MB\n",
      "   Null values: 5,979,487\n",
      "\n",
      "üìã DATA TYPES AFTER CLEANUP:\n",
      "   int8: 24 columns\n",
      "   object: 12 columns\n",
      "   float64: 3 columns\n",
      "   float16: 3 columns\n",
      "   datetime64[ns]: 1 columns\n",
      "   int32: 1 columns\n",
      "   float32: 1 columns\n",
      "   int64: 1 columns\n"
     ]
    }
   ],
   "source": [
    "df = clean_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2c03a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4606311 entries, 0 to 4606310\n",
      "Data columns (total 46 columns):\n",
      " #   Column                                Dtype         \n",
      "---  ------                                -----         \n",
      " 0   date                                  datetime64[ns]\n",
      " 1   customer_id                           int32         \n",
      " 2   employee_index                        object        \n",
      " 3   country_of_residence                  object        \n",
      " 4   gender                                object        \n",
      " 5   age                                   float64       \n",
      " 6   customer_tenure_days                  float64       \n",
      " 7   new_customer                          float16       \n",
      " 8   seniority                             float64       \n",
      " 9   primary_customer                      float16       \n",
      " 10  customer_type                         object        \n",
      " 11  relation_type                         object        \n",
      " 12  residence_index                       object        \n",
      " 13  foreigner_index                       object        \n",
      " 14  spouse_index                          object        \n",
      " 15  channel                               object        \n",
      " 16  deceased_index                        object        \n",
      " 17  province_name                         object        \n",
      " 18  activity_index                        float16       \n",
      " 19  income                                float32       \n",
      " 20  segment                               object        \n",
      " 21  savings_account_final_label           int8          \n",
      " 22  guarantees_final_label                int8          \n",
      " 23  current_accounts_final_label          int8          \n",
      " 24  deriv_investments_final_label         int8          \n",
      " 25  payroll_accounts_final_label          int8          \n",
      " 26  junior_accounts_final_label           int8          \n",
      " 27  more_particular_accounts_final_label  int8          \n",
      " 28  particular_accounts_final_label       int8          \n",
      " 29  particular_plus_accounts_final_label  int8          \n",
      " 30  short_term_deposits_final_label       int8          \n",
      " 31  medium_term_deposits_final_label      int8          \n",
      " 32  long_term_deposits_final_label        int8          \n",
      " 33  e_account_final_label                 int8          \n",
      " 34  funds_final_label                     int8          \n",
      " 35  mortgage_final_label                  int8          \n",
      " 36  pensions_final_label                  int8          \n",
      " 37  loans_final_label                     int8          \n",
      " 38  taxes_final_label                     int8          \n",
      " 39  credit_card_final_label               int8          \n",
      " 40  securities_final_label                int8          \n",
      " 41  home_account_final_label              int8          \n",
      " 42  payroll_final_label                   int8          \n",
      " 43  pensions_2_final_label                int8          \n",
      " 44  direct_debit_final_label              int8          \n",
      " 45  was_primary_customer                  int64         \n",
      "dtypes: datetime64[ns](1), float16(3), float32(1), float64(3), int32(1), int64(1), int8(24), object(12)\n",
      "memory usage: 764.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ada4d2",
   "metadata": {},
   "source": [
    "# Filter data for TypeI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c47765",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_account_labels = [\n",
    "    'current_accounts_final_label',\n",
    "    'payroll_accounts_final_label',\n",
    "    'junior_accounts_final_label',\n",
    "    'more_particular_accounts_final_label',\n",
    "    'particular_accounts_final_label',\n",
    "    'particular_plus_accounts_final_label',\n",
    "    'home_account_final_label',\n",
    "    'payroll_final_label',\n",
    "    'e_account_final_label'\n",
    "]\n",
    "\n",
    "customer_features = [\n",
    "    'date', 'customer_id', 'employee_index', 'country_of_residence', 'gender',\n",
    "    'age', 'customer_tenure_days', 'seniority', 'residence_index',  # Added customer_tenure_days\n",
    "    'foreigner_index', 'spouse_index', 'channel', 'deceased_index', \n",
    "    'province_name', 'segment', 'was_primary_customer'  # Added was_primary_customer\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee029c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df) :\n",
    "    # L·∫•y kh√°ch h√†ng kh√¥ng c√≥ t√†i kho·∫£n thanh to√°n n√†o (kh√¥ng c√≥ gi√° tr·ªã -1)\n",
    "    mask = ~(df[payment_account_labels] == -1).any(axis=1)\n",
    "    \n",
    "    # C√°c c·ªôt c·∫ßn gi·ªØ l·∫°i\n",
    "    columns_to_keep = customer_features + payment_account_labels\n",
    "    \n",
    "    # L·ªçc d·ªØ li·ªáu\n",
    "    df = df.loc[mask, columns_to_keep]\n",
    "    \n",
    "    print(f\"T·ªïng s·ªë kh√°ch h√†ng: {len(df):,}\")\n",
    "    print(f\"Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: {len(df):,}\")\n",
    "    \n",
    "    # Ki·ªÉm tra distribution\n",
    "    print(\"\\nüìä Distribution check:\")\n",
    "    for col in payment_account_labels:\n",
    "        unique_vals = df[col].unique()\n",
    "        print(f\"  {col}: {unique_vals}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0597248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë kh√°ch h√†ng: 442,736\n",
      "Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: 442,736\n",
      "\n",
      "üìä Distribution check:\n",
      "  current_accounts_final_label: [0 1]\n",
      "  payroll_accounts_final_label: [0 1]\n",
      "  junior_accounts_final_label: [0 1]\n",
      "  more_particular_accounts_final_label: [0 1]\n",
      "  particular_accounts_final_label: [0 1]\n",
      "  particular_plus_accounts_final_label: [0 1]\n",
      "  home_account_final_label: [0]\n",
      "  payroll_final_label: [0 1]\n",
      "  e_account_final_label: [0 1]\n"
     ]
    }
   ],
   "source": [
    "df = filter_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e03aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 442736 entries, 3 to 4606305\n",
      "Data columns (total 25 columns):\n",
      " #   Column                                Non-Null Count   Dtype         \n",
      "---  ------                                --------------   -----         \n",
      " 0   date                                  442736 non-null  datetime64[ns]\n",
      " 1   customer_id                           442736 non-null  int32         \n",
      " 2   employee_index                        415002 non-null  object        \n",
      " 3   country_of_residence                  415002 non-null  object        \n",
      " 4   gender                                414987 non-null  object        \n",
      " 5   age                                   415002 non-null  float64       \n",
      " 6   customer_tenure_days                  415002 non-null  float64       \n",
      " 7   seniority                             414995 non-null  float64       \n",
      " 8   residence_index                       415002 non-null  object        \n",
      " 9   foreigner_index                       415002 non-null  object        \n",
      " 10  spouse_index                          72 non-null      object        \n",
      " 11  channel                               407282 non-null  object        \n",
      " 12  deceased_index                        415002 non-null  object        \n",
      " 13  province_name                         411009 non-null  object        \n",
      " 14  segment                               406937 non-null  object        \n",
      " 15  was_primary_customer                  442736 non-null  int64         \n",
      " 16  current_accounts_final_label          442736 non-null  int8          \n",
      " 17  payroll_accounts_final_label          442736 non-null  int8          \n",
      " 18  junior_accounts_final_label           442736 non-null  int8          \n",
      " 19  more_particular_accounts_final_label  442736 non-null  int8          \n",
      " 20  particular_accounts_final_label       442736 non-null  int8          \n",
      " 21  particular_plus_accounts_final_label  442736 non-null  int8          \n",
      " 22  home_account_final_label              442736 non-null  int8          \n",
      " 23  payroll_final_label                   442736 non-null  int8          \n",
      " 24  e_account_final_label                 442736 non-null  int8          \n",
      "dtypes: datetime64[ns](1), float64(3), int32(1), int64(1), int8(9), object(10)\n",
      "memory usage: 59.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1c3f30",
   "metadata": {},
   "source": [
    "# Clean memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd279b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ MEMORY CLEANUP...\n",
      "========================================\n",
      "   üóëÔ∏è Garbage collected: 0 objects\n",
      "   üíæ Current memory usage: 417.2 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gc\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"\n",
    "    Clean up unused memory and optimize DataFrame\n",
    "    \"\"\"\n",
    "    print(\"üßπ MEMORY CLEANUP...\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Force garbage collection\n",
    "    collected = gc.collect()\n",
    "    print(f\"   üóëÔ∏è Garbage collected: {collected} objects\")\n",
    "    \n",
    "    # Get memory info\n",
    "    import psutil\n",
    "    import os\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    memory_mb = memory_info.rss / 1024 / 1024\n",
    "    \n",
    "    print(f\"   üíæ Current memory usage: {memory_mb:.1f} MB\")\n",
    "    \n",
    "    return memory_mb\n",
    "\n",
    "memory_before = cleanup_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a050f5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 442736 entries, 3 to 4606305\n",
      "Data columns (total 25 columns):\n",
      " #   Column                                Non-Null Count   Dtype         \n",
      "---  ------                                --------------   -----         \n",
      " 0   date                                  442736 non-null  datetime64[ns]\n",
      " 1   customer_id                           442736 non-null  int32         \n",
      " 2   employee_index                        415002 non-null  object        \n",
      " 3   country_of_residence                  415002 non-null  object        \n",
      " 4   gender                                414987 non-null  object        \n",
      " 5   age                                   415002 non-null  float64       \n",
      " 6   customer_tenure_days                  415002 non-null  float64       \n",
      " 7   seniority                             414995 non-null  float64       \n",
      " 8   residence_index                       415002 non-null  object        \n",
      " 9   foreigner_index                       415002 non-null  object        \n",
      " 10  spouse_index                          72 non-null      object        \n",
      " 11  channel                               407282 non-null  object        \n",
      " 12  deceased_index                        415002 non-null  object        \n",
      " 13  province_name                         411009 non-null  object        \n",
      " 14  segment                               406937 non-null  object        \n",
      " 15  was_primary_customer                  442736 non-null  int64         \n",
      " 16  current_accounts_final_label          442736 non-null  int8          \n",
      " 17  payroll_accounts_final_label          442736 non-null  int8          \n",
      " 18  junior_accounts_final_label           442736 non-null  int8          \n",
      " 19  more_particular_accounts_final_label  442736 non-null  int8          \n",
      " 20  particular_accounts_final_label       442736 non-null  int8          \n",
      " 21  particular_plus_accounts_final_label  442736 non-null  int8          \n",
      " 22  home_account_final_label              442736 non-null  int8          \n",
      " 23  payroll_final_label                   442736 non-null  int8          \n",
      " 24  e_account_final_label                 442736 non-null  int8          \n",
      "dtypes: datetime64[ns](1), float64(3), int32(1), int64(1), int8(9), object(10)\n",
      "memory usage: 59.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af1a9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·ªôt target (label) - t√†i kho·∫£n thanh to√°n\n",
    "tar_cols = [\n",
    "    'current_accounts_final_label',\n",
    "    'payroll_accounts_final_label',\n",
    "    'junior_accounts_final_label',\n",
    "    'more_particular_accounts_final_label',\n",
    "    'particular_accounts_final_label',\n",
    "    'particular_plus_accounts_final_label',\n",
    "    'home_account_final_label',\n",
    "    'payroll_final_label',\n",
    "    'e_account_final_label'\n",
    "]\n",
    "\n",
    "# C·ªôt s·ªë (numeric)\n",
    "num_cols = [\n",
    "    'age',\n",
    "    'seniority',\n",
    "    'new_customer',\n",
    "    'address_type',\n",
    "    'province_code',\n",
    "    'activity_index',\n",
    "    'income'\n",
    "]\n",
    "\n",
    "# C·ªôt ph√¢n lo·∫°i (categorical / object / category)\n",
    "cat_cols = [\n",
    "    'employee_index',\n",
    "    'country_of_residence',\n",
    "    'gender',\n",
    "    'primary_customer',\n",
    "    'customer_type',\n",
    "    'relation_type',\n",
    "    'residence_index',\n",
    "    'foreigner_index',\n",
    "    'spouse_index',\n",
    "    'channel',\n",
    "    'deceased_index',\n",
    "    'province_name',\n",
    "    'segment'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5529793",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2893fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking available columns before feature engineering:\n",
      "Available columns: ['date', 'customer_id', 'employee_index', 'country_of_residence', 'gender', 'age', 'customer_tenure_days', 'seniority', 'residence_index', 'foreigner_index', 'spouse_index', 'channel', 'deceased_index', 'province_name', 'segment', 'was_primary_customer', 'current_accounts_final_label', 'payroll_accounts_final_label', 'junior_accounts_final_label', 'more_particular_accounts_final_label', 'particular_accounts_final_label', 'particular_plus_accounts_final_label', 'home_account_final_label', 'payroll_final_label', 'e_account_final_label']\n",
      "üîß CREATING ENHANCED FEATURES...\n",
      "==================================================\n",
      "üìã Available columns: 25\n",
      "üìÖ Creating time-based features...\n",
      "üë• Creating demographic features...\n",
      "üè¶ Creating banking relationship features...\n",
      "üîó Creating interaction features...\n",
      "üéØ Creating behavioral features...\n",
      "‚öñÔ∏è Creating risk and stability features...\n",
      "‚úÖ Feature engineering completed!\n",
      "Original features: 25\n",
      "Enhanced features: 58\n",
      "New features added: 33\n",
      "\n",
      "üîç CHECKING NEW FEATURES DATA TYPES...\n",
      "New features created: 33\n",
      "   year: int32\n",
      "   month: int32\n",
      "   quarter: int32\n",
      "   day_of_week: int32\n",
      "   is_weekend: int32\n",
      "   is_month_end: int32\n",
      "   is_quarter_end: int32\n",
      "   years_since_registration: float64\n",
      "   tenure_category: category\n",
      "   age_group: category\n",
      "   is_young_adult: int32\n",
      "   is_middle_aged: int32\n",
      "   is_senior: int32\n",
      "   age_squared: float64\n",
      "   seniority_years: float64\n",
      "\n",
      "üìä SAMPLE OF ENHANCED DATA:\n",
      "     year  month  quarter  day_of_week  is_weekend\n",
      "3    2015      1        1            2           0\n",
      "43   2015      1        1            2           0\n",
      "46   2015      1        1            2           0\n",
      "261  2015      1        1            2           0\n",
      "389  2015      1        1            2           0\n"
     ]
    }
   ],
   "source": [
    "def create_enhanced_features(df):\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    print(\"üîß CREATING ENHANCED FEATURES...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check available columns first\n",
    "    available_cols = df_enhanced.columns.tolist()\n",
    "    print(f\"üìã Available columns: {len(available_cols)}\")\n",
    "    \n",
    "    # =============================\n",
    "    # 1. TIME-BASED FEATURES\n",
    "    # =============================\n",
    "    print(\"üìÖ Creating time-based features...\")\n",
    "    \n",
    "    # Extract t·ª´ date column\n",
    "    if 'date' in df_enhanced.columns:\n",
    "        df_enhanced['year'] = df_enhanced['date'].dt.year\n",
    "        df_enhanced['month'] = df_enhanced['date'].dt.month\n",
    "        df_enhanced['quarter'] = df_enhanced['date'].dt.quarter\n",
    "        df_enhanced['day_of_week'] = df_enhanced['date'].dt.dayofweek\n",
    "        df_enhanced['is_weekend'] = (df_enhanced['day_of_week'] >= 5).astype(int)\n",
    "        df_enhanced['is_month_end'] = (df_enhanced['date'].dt.day >= 25).astype(int)\n",
    "        df_enhanced['is_quarter_end'] = df_enhanced['date'].dt.month.isin([3, 6, 9, 12]).astype(int)\n",
    "    \n",
    "    # Customer tenure features (using existing customer_tenure_days)\n",
    "    if 'customer_tenure_days' in df_enhanced.columns:\n",
    "        df_enhanced['years_since_registration'] = df_enhanced['customer_tenure_days'] / 365.25\n",
    "        \n",
    "        # Customer tenure categories\n",
    "        df_enhanced['tenure_category'] = pd.cut(\n",
    "            df_enhanced['customer_tenure_days'],\n",
    "            bins=[-1, 90, 365, 1095, 2190, np.inf],\n",
    "            labels=['Very_New', 'New', 'Medium', 'Long', 'Very_Long']\n",
    "        )\n",
    "    \n",
    "    # =============================\n",
    "    # 2. DEMOGRAPHIC FEATURES\n",
    "    # =============================\n",
    "    print(\"üë• Creating demographic features...\")\n",
    "    \n",
    "    # Age-based features\n",
    "    if 'age' in df_enhanced.columns:\n",
    "        df_enhanced['age_group'] = pd.cut(\n",
    "            df_enhanced['age'],\n",
    "            bins=[0, 25, 35, 45, 55, 65, 100],\n",
    "            labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+']\n",
    "        )\n",
    "        \n",
    "        age_young = (df_enhanced['age'] >= 18) & (df_enhanced['age'] <= 30)\n",
    "        age_middle = (df_enhanced['age'] >= 31) & (df_enhanced['age'] <= 50)\n",
    "        age_senior = (df_enhanced['age'] >= 60)\n",
    "        \n",
    "        df_enhanced['is_young_adult'] = age_young.fillna(False).astype(int)\n",
    "        df_enhanced['is_middle_aged'] = age_middle.fillna(False).astype(int)\n",
    "        df_enhanced['is_senior'] = age_senior.fillna(False).astype(int)\n",
    "        df_enhanced['age_squared'] = df_enhanced['age'] ** 2\n",
    "    \n",
    "    # =============================\n",
    "    # 3. BANKING RELATIONSHIP FEATURES  \n",
    "    # =============================\n",
    "    print(\"üè¶ Creating banking relationship features...\")\n",
    "    \n",
    "    # Seniority-based features\n",
    "    if 'seniority' in df_enhanced.columns:\n",
    "        df_enhanced['seniority_years'] = df_enhanced['seniority'] / 12\n",
    "        df_enhanced['seniority_category'] = pd.cut(\n",
    "            df_enhanced['seniority'],\n",
    "            bins=[-1, 0, 6, 12, 24, 60, np.inf],\n",
    "            labels=['New', 'Very_Short', 'Short', 'Medium', 'Long', 'Very_Long']\n",
    "        )\n",
    "        \n",
    "        seniority_new = df_enhanced['seniority'] <= 6\n",
    "        seniority_established = df_enhanced['seniority'] >= 24\n",
    "        \n",
    "        df_enhanced['is_new_relationship'] = seniority_new.fillna(False).astype(int)\n",
    "        df_enhanced['is_established_relationship'] = seniority_established.fillna(False).astype(int)\n",
    "    \n",
    "    # Customer status features (check if columns exist)\n",
    "    df_enhanced['is_employee'] = 0\n",
    "    if 'employee_index' in df_enhanced.columns:\n",
    "        df_enhanced['is_employee'] = (df_enhanced['employee_index'] == 1).astype(int)\n",
    "    \n",
    "    df_enhanced['is_primary_customer_flag'] = 0  \n",
    "    if 'was_primary_customer' in df_enhanced.columns:\n",
    "        df_enhanced['is_primary_customer_flag'] = df_enhanced['was_primary_customer']\n",
    "    \n",
    "    # Geographic features\n",
    "    if 'country_of_residence' in df_enhanced.columns:\n",
    "        domestic = df_enhanced['country_of_residence'] == 'ES'\n",
    "        df_enhanced['is_domestic'] = domestic.fillna(False).astype(int)\n",
    "    \n",
    "    if 'foreigner_index' in df_enhanced.columns:\n",
    "        foreigner = df_enhanced['foreigner_index'] == 1\n",
    "        df_enhanced['is_foreigner'] = foreigner.fillna(False).astype(int)\n",
    "    \n",
    "\n",
    "    # =============================\n",
    "    # 5. INTERACTION FEATURES\n",
    "    # =============================\n",
    "    print(\"üîó Creating interaction features...\")\n",
    "    \n",
    "    # Age-Seniority interactions\n",
    "    if 'age' in df_enhanced.columns and 'seniority' in df_enhanced.columns:\n",
    "        age_filled = df_enhanced['age'].fillna(0)\n",
    "        seniority_filled = df_enhanced['seniority'].fillna(0)\n",
    "        \n",
    "        df_enhanced['age_seniority_interaction'] = (age_filled * seniority_filled) / 100\n",
    "        df_enhanced['seniority_per_age'] = seniority_filled / (age_filled + 1)\n",
    "    \n",
    "    # Age-Tenure interactions\n",
    "    if 'age' in df_enhanced.columns and 'customer_tenure_days' in df_enhanced.columns:\n",
    "        age_filled = df_enhanced['age'].fillna(0)\n",
    "        tenure_filled = df_enhanced['customer_tenure_days'].fillna(0)\n",
    "        \n",
    "        df_enhanced['age_tenure_ratio'] = age_filled / (tenure_filled/365 + 1)\n",
    "    \n",
    "    # =============================\n",
    "    # 6. BEHAVIORAL FEATURES\n",
    "    # =============================\n",
    "    print(\"üéØ Creating behavioral features...\")\n",
    "    \n",
    "    # Channel preference\n",
    "    if 'channel' in df_enhanced.columns:\n",
    "        channel_mapping = {\n",
    "            'KAT': 'Traditional',\n",
    "            'KFC': 'Phone', \n",
    "            'KHE': 'Digital',\n",
    "            'KHM': 'Mobile',\n",
    "            'KHN': 'Online'\n",
    "        }\n",
    "        df_enhanced['channel_type'] = df_enhanced['channel'].map(channel_mapping).fillna('Other')\n",
    "        df_enhanced['is_digital_channel'] = df_enhanced['channel_type'].isin(['Digital', 'Mobile', 'Online']).astype(int)\n",
    "    \n",
    "    # Customer segment enhancement\n",
    "    if 'segment' in df_enhanced.columns:\n",
    "        df_enhanced['is_vip_segment'] = df_enhanced['segment'].str.contains('VIP', na=False).astype(int)\n",
    "        df_enhanced['is_university_segment'] = df_enhanced['segment'].str.contains('UNIVERSITY', na=False).astype(int)\n",
    "    \n",
    "    # =============================\n",
    "    # 7. RISK & STABILITY FEATURES\n",
    "    # =============================\n",
    "    print(\"‚öñÔ∏è Creating risk and stability features...\")\n",
    "    \n",
    "    # Customer stability score\n",
    "    stability_score = 0\n",
    "    \n",
    "    if 'seniority' in df_enhanced.columns:\n",
    "        seniority_stable = df_enhanced['seniority'] >= 12\n",
    "        stability_score += seniority_stable.fillna(False).astype(int)\n",
    "    \n",
    "    if 'age' in df_enhanced.columns:\n",
    "        age_stable = df_enhanced['age'] >= 30\n",
    "        stability_score += age_stable.fillna(False).astype(int)\n",
    "    \n",
    "    stability_score += df_enhanced['is_primary_customer_flag']\n",
    "    stability_score += df_enhanced['is_employee']\n",
    "    \n",
    "    df_enhanced['customer_stability_score'] = stability_score\n",
    "    df_enhanced['is_stable_customer'] = (stability_score >= 2).astype(int)\n",
    "    \n",
    "    # Potential value score\n",
    "    potential_score = 0\n",
    "    \n",
    "    if 'age' in df_enhanced.columns:\n",
    "        age_prime = (df_enhanced['age'] >= 25) & (df_enhanced['age'] <= 55)\n",
    "        potential_score += age_prime.fillna(False).astype(int)\n",
    "    \n",
    "    potential_score += df_enhanced['is_digital_channel'] if 'is_digital_channel' in df_enhanced.columns else 0\n",
    "    potential_score += df_enhanced['is_domestic'] if 'is_domestic' in df_enhanced.columns else 0\n",
    "    \n",
    "    df_enhanced['customer_potential_score'] = potential_score\n",
    "    df_enhanced['is_high_potential'] = (potential_score >= 2).astype(int)\n",
    "    \n",
    "    print(f\"‚úÖ Feature engineering completed!\")\n",
    "    print(f\"Original features: {df.shape[1]}\")\n",
    "    print(f\"Enhanced features: {df_enhanced.shape[1]}\")\n",
    "    print(f\"New features added: {df_enhanced.shape[1] - df.shape[1]}\")\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# =============================\n",
    "# APPLY FEATURE ENGINEERING\n",
    "# =============================\n",
    "print(\"üîç Checking available columns before feature engineering:\")\n",
    "print(f\"Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "df_enhanced = create_enhanced_features(df)\n",
    "\n",
    "# =============================\n",
    "# VERIFY DATA TYPES\n",
    "# =============================\n",
    "print(\"\\nüîç CHECKING NEW FEATURES DATA TYPES...\")\n",
    "new_features = [col for col in df_enhanced.columns if col not in df.columns]\n",
    "print(f\"New features created: {len(new_features)}\")\n",
    "\n",
    "for feature in new_features[:15]:  # Show first 15 new features\n",
    "    print(f\"   {feature}: {df_enhanced[feature].dtype}\")\n",
    "\n",
    "print(f\"\\nüìä SAMPLE OF ENHANCED DATA:\")\n",
    "if len(new_features) > 0:\n",
    "    sample_features = new_features[:5] if len(new_features) >= 5 else new_features\n",
    "    print(df_enhanced[sample_features].head())\n",
    "else:\n",
    "    print(\"No new features were created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01b09d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b31ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 442736 entries, 3 to 4606305\n",
      "Data columns (total 58 columns):\n",
      " #   Column                                Non-Null Count   Dtype         \n",
      "---  ------                                --------------   -----         \n",
      " 0   date                                  442736 non-null  datetime64[ns]\n",
      " 1   customer_id                           442736 non-null  int32         \n",
      " 2   employee_index                        415002 non-null  object        \n",
      " 3   country_of_residence                  415002 non-null  object        \n",
      " 4   gender                                414987 non-null  object        \n",
      " 5   age                                   415002 non-null  float64       \n",
      " 6   customer_tenure_days                  415002 non-null  float64       \n",
      " 7   seniority                             414995 non-null  float64       \n",
      " 8   residence_index                       415002 non-null  object        \n",
      " 9   foreigner_index                       415002 non-null  object        \n",
      " 10  spouse_index                          72 non-null      object        \n",
      " 11  channel                               407282 non-null  object        \n",
      " 12  deceased_index                        415002 non-null  object        \n",
      " 13  province_name                         411009 non-null  object        \n",
      " 14  segment                               406937 non-null  object        \n",
      " 15  was_primary_customer                  442736 non-null  int64         \n",
      " 16  current_accounts_final_label          442736 non-null  int8          \n",
      " 17  payroll_accounts_final_label          442736 non-null  int8          \n",
      " 18  junior_accounts_final_label           442736 non-null  int8          \n",
      " 19  more_particular_accounts_final_label  442736 non-null  int8          \n",
      " 20  particular_accounts_final_label       442736 non-null  int8          \n",
      " 21  particular_plus_accounts_final_label  442736 non-null  int8          \n",
      " 22  home_account_final_label              442736 non-null  int8          \n",
      " 23  payroll_final_label                   442736 non-null  int8          \n",
      " 24  e_account_final_label                 442736 non-null  int8          \n",
      " 25  year                                  442736 non-null  int32         \n",
      " 26  month                                 442736 non-null  int32         \n",
      " 27  quarter                               442736 non-null  int32         \n",
      " 28  day_of_week                           442736 non-null  int32         \n",
      " 29  is_weekend                            442736 non-null  int32         \n",
      " 30  is_month_end                          442736 non-null  int32         \n",
      " 31  is_quarter_end                        442736 non-null  int32         \n",
      " 32  years_since_registration              415002 non-null  float64       \n",
      " 33  tenure_category                       412165 non-null  category      \n",
      " 34  age_group                             414328 non-null  category      \n",
      " 35  is_young_adult                        442736 non-null  int32         \n",
      " 36  is_middle_aged                        442736 non-null  int32         \n",
      " 37  is_senior                             442736 non-null  int32         \n",
      " 38  age_squared                           415002 non-null  float64       \n",
      " 39  seniority_years                       414995 non-null  float64       \n",
      " 40  seniority_category                    414995 non-null  category      \n",
      " 41  is_new_relationship                   442736 non-null  int32         \n",
      " 42  is_established_relationship           442736 non-null  int32         \n",
      " 43  is_employee                           442736 non-null  int32         \n",
      " 44  is_primary_customer_flag              442736 non-null  int64         \n",
      " 45  is_domestic                           442736 non-null  int32         \n",
      " 46  is_foreigner                          442736 non-null  int32         \n",
      " 47  age_seniority_interaction             442736 non-null  float64       \n",
      " 48  seniority_per_age                     442736 non-null  float64       \n",
      " 49  age_tenure_ratio                      442736 non-null  float64       \n",
      " 50  channel_type                          442736 non-null  object        \n",
      " 51  is_digital_channel                    442736 non-null  int32         \n",
      " 52  is_vip_segment                        442736 non-null  int32         \n",
      " 53  is_university_segment                 442736 non-null  int32         \n",
      " 54  customer_stability_score              442736 non-null  int64         \n",
      " 55  is_stable_customer                    442736 non-null  int32         \n",
      " 56  customer_potential_score              442736 non-null  int32         \n",
      " 57  is_high_potential                     442736 non-null  int32         \n",
      "dtypes: category(3), datetime64[ns](1), float64(9), int32(22), int64(3), int8(9), object(11)\n",
      "memory usage: 126.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_enhanced.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0336ea94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FEATURE LISTS DEFINED:\n",
      "   Numeric features: 48\n",
      "   Categorical features: 5\n",
      "   Target columns: 9\n",
      "   Total features: 53\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# FEATURE LISTS FOR MODELING\n",
    "# =============================\n",
    "\n",
    "# Numeric features (engineered features)\n",
    "numeric_features = [\n",
    "    'year', 'month', 'quarter', 'day_of_week', 'is_weekend', 'is_month_end', 'is_quarter_end',\n",
    "    'days_since_registration', 'registration_year', 'registration_month', 'years_since_registration',\n",
    "    'is_young_adult', 'is_middle_aged', 'is_senior', 'age_squared',\n",
    "    'income_vs_median', 'is_high_income', 'is_low_income', 'log_income',\n",
    "    'seniority_years', 'is_new_relationship', 'is_established_relationship',\n",
    "    'is_primary_customer', 'is_new_customer', 'is_active', 'is_domestic', 'is_foreigner',\n",
    "    'total_products', 'has_any_product', 'is_single_product', 'is_multi_product',\n",
    "    'product_diversity_ratio', 'has_current_account', 'has_savings_account', 'has_premium_account',\n",
    "    'age_income_interaction', 'income_per_age', 'seniority_income_interaction', 'income_growth_proxy',\n",
    "    'young_high_income', 'senior_established', 'is_digital_channel',\n",
    "    'is_vip_segment', 'is_university_segment', 'customer_stability_score',\n",
    "    'is_stable_customer', 'customer_potential_score', 'is_high_potential'\n",
    "]\n",
    "\n",
    "# Categorical features\n",
    "categorical_features = [\n",
    "    'tenure_category', 'age_group', 'income_quartile', 'seniority_category', 'channel_type'\n",
    "]\n",
    "\n",
    "# Target columns\n",
    "target_cols = [\n",
    "    'current_accounts_final_label',\n",
    "    'payroll_accounts_final_label',\n",
    "    'junior_accounts_final_label',\n",
    "    'more_particular_accounts_final_label',\n",
    "    'particular_accounts_final_label',\n",
    "    'particular_plus_accounts_final_label',\n",
    "    'home_account_final_label',\n",
    "    'payroll_final_label',\n",
    "    'e_account_final_label'\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ FEATURE LISTS DEFINED:\")\n",
    "print(f\"   Numeric features: {len(numeric_features)}\")\n",
    "print(f\"   Categorical features: {len(categorical_features)}\")\n",
    "print(f\"   Target columns: {len(target_cols)}\")\n",
    "print(f\"   Total features: {len(numeric_features) + len(categorical_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5948fe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ FINAL FEATURE SELECTION:\n",
      "Available numeric features: 24\n",
      "Available categorical features: 4\n",
      "Target labels: 9\n",
      "\n",
      "üìã DATA SHAPE:\n",
      "X shape: (442736, 28)\n",
      "y shape: (442736, 9)\n",
      "\n",
      "üîç DATA QUALITY CHECK:\n",
      "Missing values in X: 169929\n",
      "Missing values in y: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# PREPARE DATA FOR MODELING\n",
    "# =============================\n",
    "\n",
    "# Features to exclude from modeling\n",
    "exclude_features = [\n",
    "    'date', 'customer_id', 'registration_date', 'last_primary_date'\n",
    "]\n",
    "\n",
    "# Filter features that exist in dataframe\n",
    "available_num_cols = [col for col in numeric_features if col in df_enhanced.columns and col not in exclude_features]\n",
    "available_cat_cols = [col for col in categorical_features if col in df_enhanced.columns and col not in exclude_features]\n",
    "\n",
    "print(f\"üéØ FINAL FEATURE SELECTION:\")\n",
    "print(f\"Available numeric features: {len(available_num_cols)}\")\n",
    "print(f\"Available categorical features: {len(available_cat_cols)}\")\n",
    "print(f\"Target labels: {len(target_cols)}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = df_enhanced[available_num_cols + available_cat_cols]\n",
    "y = df_enhanced[target_cols]\n",
    "\n",
    "print(f\"\\nüìã DATA SHAPE:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"\\nüîç DATA QUALITY CHECK:\")\n",
    "print(f\"Missing values in X: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y: {y.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed920174",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59f30ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_10_2015 = pd.read_csv('data/processed/test_10_2015.csv')\n",
    "test_11_2015 = pd.read_csv('data/processed/test_11_2015.csv')\n",
    "test_12_2015 = pd.read_csv('data/processed/test_12_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb6924a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 903.96 Mb (53.6% reduction)\n",
      "Mem. usage decreased to 950.77 Mb (53.6% reduction)\n",
      "Mem. usage decreased to 998.31 Mb (53.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "test_10_2015 = reduce_memory_usage(test_10_2015)\n",
    "test_11_2015 = reduce_memory_usage(test_11_2015)\n",
    "test_12_2015 = reduce_memory_usage(test_12_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19d895f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ CLEANING DATASET...\n",
      "========================================\n",
      "1Ô∏è‚É£ Handling missing values...\n",
      "   ‚úÖ Filled payroll_final_label: 0 missing ‚Üí 0\n",
      "   ‚úÖ Filled pensions_2_final_label: 0 missing ‚Üí 0\n",
      "\n",
      "2Ô∏è‚É£ Creating customer tenure feature...\n",
      "   ‚úÖ Created 'customer_tenure_days' feature\n",
      "   üìä Range: -3.0 to 7590.0 days\n",
      "   üóëÔ∏è Dropped 'registration_date' column\n",
      "\n",
      "3Ô∏è‚É£ Converting last_primary_date to binary indicator...\n",
      "   ‚úÖ Created 'was_primary_customer' binary feature\n",
      "   üìä 5,316,273 nulls ‚Üí 0, 8,870 dates ‚Üí 1\n",
      "   üóëÔ∏è Dropped 'last_primary_date' column\n",
      "\n",
      "4Ô∏è‚É£ Removing constant and duplicate columns...\n",
      "   üóëÔ∏è Dropped 'address_type' (constant value)\n",
      "   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\n",
      "\n",
      "5Ô∏è‚É£ Cleaning numeric columns...\n",
      "   ‚úÖ Cleaned 'age': object ‚Üí numeric (9,750 nulls)\n",
      "   ‚ö†Ô∏è Converted 14 negative seniority values to NaN\n",
      "   ‚úÖ Cleaned 'seniority': object ‚Üí numeric (9,764 nulls)\n",
      "\n",
      "üìä CLEANUP SUMMARY:\n",
      "----------------------------------------\n",
      "   Final shape: (5325143, 46)\n",
      "   Memory usage: 3423.5 MB\n",
      "   Null values: 6,810,911\n",
      "\n",
      "üìã DATA TYPES AFTER CLEANUP:\n",
      "   int8: 24 columns\n",
      "   object: 12 columns\n",
      "   float64: 3 columns\n",
      "   float16: 3 columns\n",
      "   datetime64[ns]: 1 columns\n",
      "   int32: 1 columns\n",
      "   float32: 1 columns\n",
      "   int64: 1 columns\n",
      "üßπ CLEANING DATASET...\n",
      "========================================\n",
      "1Ô∏è‚É£ Handling missing values...\n",
      "   ‚úÖ Filled payroll_final_label: 0 missing ‚Üí 0\n",
      "   ‚úÖ Filled pensions_2_final_label: 0 missing ‚Üí 0\n",
      "\n",
      "2Ô∏è‚É£ Creating customer tenure feature...\n",
      "   ‚úÖ Created 'customer_tenure_days' feature\n",
      "   üìä Range: -3.0 to 7621.0 days\n",
      "   üóëÔ∏è Dropped 'registration_date' column\n",
      "\n",
      "3Ô∏è‚É£ Converting last_primary_date to binary indicator...\n",
      "   ‚úÖ Created 'was_primary_customer' binary feature\n",
      "   üìä 5,591,002 nulls ‚Üí 0, 9,883 dates ‚Üí 1\n",
      "   üóëÔ∏è Dropped 'last_primary_date' column\n",
      "\n",
      "4Ô∏è‚É£ Removing constant and duplicate columns...\n",
      "   üóëÔ∏è Dropped 'address_type' (constant value)\n",
      "   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\n",
      "\n",
      "5Ô∏è‚É£ Cleaning numeric columns...\n",
      "   ‚úÖ Cleaned 'age': object ‚Üí numeric (5,458 nulls)\n",
      "   ‚ö†Ô∏è Converted 14 negative seniority values to NaN\n",
      "   ‚úÖ Cleaned 'seniority': object ‚Üí numeric (5,472 nulls)\n",
      "\n",
      "üìä CLEANUP SUMMARY:\n",
      "----------------------------------------\n",
      "   Final shape: (5600885, 46)\n",
      "   Memory usage: 3611.3 MB\n",
      "   Null values: 7,165,962\n",
      "\n",
      "üìã DATA TYPES AFTER CLEANUP:\n",
      "   int8: 24 columns\n",
      "   object: 12 columns\n",
      "   float64: 3 columns\n",
      "   float16: 3 columns\n",
      "   datetime64[ns]: 1 columns\n",
      "   int32: 1 columns\n",
      "   float32: 1 columns\n",
      "   int64: 1 columns\n",
      "üßπ CLEANING DATASET...\n",
      "========================================\n",
      "1Ô∏è‚É£ Handling missing values...\n",
      "   ‚úÖ Filled payroll_final_label: 0 missing ‚Üí 0\n",
      "   ‚úÖ Filled pensions_2_final_label: 0 missing ‚Üí 0\n",
      "\n",
      "2Ô∏è‚É£ Creating customer tenure feature...\n",
      "   ‚úÖ Created 'customer_tenure_days' feature\n",
      "   üìä Range: -3.0 to 7651.0 days\n",
      "   üóëÔ∏è Dropped 'registration_date' column\n",
      "\n",
      "3Ô∏è‚É£ Converting last_primary_date to binary indicator...\n",
      "   ‚úÖ Created 'was_primary_customer' binary feature\n",
      "   üìä 5,868,778 nulls ‚Üí 0, 12,171 dates ‚Üí 1\n",
      "   üóëÔ∏è Dropped 'last_primary_date' column\n",
      "\n",
      "4Ô∏è‚É£ Removing constant and duplicate columns...\n",
      "   üóëÔ∏è Dropped 'address_type' (constant value)\n",
      "   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\n",
      "\n",
      "5Ô∏è‚É£ Cleaning numeric columns...\n",
      "   ‚úÖ Cleaned 'age': object ‚Üí numeric (1,861 nulls)\n",
      "   ‚ö†Ô∏è Converted 14 negative seniority values to NaN\n",
      "   ‚úÖ Cleaned 'seniority': object ‚Üí numeric (1,875 nulls)\n",
      "\n",
      "üìä CLEANUP SUMMARY:\n",
      "----------------------------------------\n",
      "   Final shape: (5880949, 46)\n",
      "   Memory usage: 3799.3 MB\n",
      "   Null values: 7,513,301\n",
      "\n",
      "üìã DATA TYPES AFTER CLEANUP:\n",
      "   int8: 24 columns\n",
      "   object: 12 columns\n",
      "   float64: 3 columns\n",
      "   float16: 3 columns\n",
      "   datetime64[ns]: 1 columns\n",
      "   int32: 1 columns\n",
      "   float32: 1 columns\n",
      "   int64: 1 columns\n"
     ]
    }
   ],
   "source": [
    "test_10_2015 = clean_dataset(test_10_2015)\n",
    "test_11_2015 = clean_dataset(test_11_2015)\n",
    "test_12_2015 = clean_dataset(test_12_2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efa4562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng s·ªë kh√°ch h√†ng: 1,017,191\n",
      "Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: 1,017,191\n",
      "\n",
      "üìä Distribution check:\n",
      "  current_accounts_final_label: [0 1]\n",
      "  payroll_accounts_final_label: [0 1]\n",
      "  junior_accounts_final_label: [0 1]\n",
      "  more_particular_accounts_final_label: [0 1]\n",
      "  particular_accounts_final_label: [0 1]\n",
      "  particular_plus_accounts_final_label: [0 1]\n",
      "  home_account_final_label: [0]\n",
      "  payroll_final_label: [0 1]\n",
      "  e_account_final_label: [0 1]\n",
      "T·ªïng s·ªë kh√°ch h√†ng: 1,223,624\n",
      "Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: 1,223,624\n",
      "\n",
      "üìä Distribution check:\n",
      "  current_accounts_final_label: [0 1]\n",
      "  payroll_accounts_final_label: [0 1]\n",
      "  junior_accounts_final_label: [0 1]\n",
      "  more_particular_accounts_final_label: [0 1]\n",
      "  particular_accounts_final_label: [0 1]\n",
      "  particular_plus_accounts_final_label: [0 1]\n",
      "  home_account_final_label: [0]\n",
      "  payroll_final_label: [0 1]\n",
      "  e_account_final_label: [0 1]\n",
      "T·ªïng s·ªë kh√°ch h√†ng: 1,425,848\n",
      "Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: 1,425,848\n",
      "\n",
      "üìä Distribution check:\n",
      "  current_accounts_final_label: [0 1]\n",
      "  payroll_accounts_final_label: [0 1]\n",
      "  junior_accounts_final_label: [0 1]\n",
      "  more_particular_accounts_final_label: [0 1]\n",
      "  particular_accounts_final_label: [0 1]\n",
      "  particular_plus_accounts_final_label: [0 1]\n",
      "  home_account_final_label: [0 1]\n",
      "  payroll_final_label: [0 1]\n",
      "  e_account_final_label: [0 1]\n"
     ]
    }
   ],
   "source": [
    "test_10_2015 = filter_data(test_10_2015)\n",
    "test_11_2015 = filter_data(test_11_2015)\n",
    "test_12_2015 = filter_data(test_12_2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f3c63b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CREATING ENHANCED FEATURES...\n",
      "==================================================\n",
      "üìã Available columns: 25\n",
      "üìÖ Creating time-based features...\n",
      "üë• Creating demographic features...\n",
      "üè¶ Creating banking relationship features...\n",
      "üîó Creating interaction features...\n",
      "üéØ Creating behavioral features...\n",
      "‚öñÔ∏è Creating risk and stability features...\n",
      "‚úÖ Feature engineering completed!\n",
      "Original features: 25\n",
      "Enhanced features: 58\n",
      "New features added: 33\n",
      "üîß CREATING ENHANCED FEATURES...\n",
      "==================================================\n",
      "üìã Available columns: 25\n",
      "üìÖ Creating time-based features...\n",
      "üë• Creating demographic features...\n",
      "üè¶ Creating banking relationship features...\n",
      "üîó Creating interaction features...\n",
      "üéØ Creating behavioral features...\n",
      "‚öñÔ∏è Creating risk and stability features...\n",
      "‚úÖ Feature engineering completed!\n",
      "Original features: 25\n",
      "Enhanced features: 58\n",
      "New features added: 33\n",
      "üîß CREATING ENHANCED FEATURES...\n",
      "==================================================\n",
      "üìã Available columns: 25\n",
      "üìÖ Creating time-based features...\n",
      "üë• Creating demographic features...\n",
      "üè¶ Creating banking relationship features...\n",
      "üîó Creating interaction features...\n",
      "üéØ Creating behavioral features...\n",
      "‚öñÔ∏è Creating risk and stability features...\n",
      "‚úÖ Feature engineering completed!\n",
      "Original features: 25\n",
      "Enhanced features: 58\n",
      "New features added: 33\n"
     ]
    }
   ],
   "source": [
    "test_10_2015 = create_enhanced_features(test_10_2015)\n",
    "test_11_2015 = create_enhanced_features(test_11_2015)\n",
    "test_12_2015 = create_enhanced_features(test_12_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20fa9d",
   "metadata": {},
   "source": [
    "# Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9917f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=7):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    \n",
    "    Parameters:\n",
    "    actual : list - A list of actual relevant items\n",
    "    predicted : list - A list of predicted items ordered by rank\n",
    "    k : int - The maximum number of predicted elements\n",
    "    \n",
    "    Returns:\n",
    "    score : double - The average precision at k\n",
    "    \"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=7):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    \n",
    "    Parameters:\n",
    "    actual : list of lists - A list of lists of actual relevant items\n",
    "    predicted : list of lists - A list of lists of predicted items ordered by rank\n",
    "    k : int - The maximum number of predicted elements\n",
    "    \n",
    "    Returns:\n",
    "    score : double - The mean average precision at k\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "def evaluate_recommendations(actual_products, predicted_products, k=7):\n",
    "    \"\"\"\n",
    "    Evaluate recommendation system using MAP@k\n",
    "    \n",
    "    Parameters:\n",
    "    actual_products : dict - {user_id: [list of actual products]}\n",
    "    predicted_products : dict - {user_id: [list of predicted products]}\n",
    "    k : int - Number of recommendations to consider\n",
    "    \n",
    "    Returns:\n",
    "    map_score : float - MAP@k score\n",
    "    \"\"\"\n",
    "    actual_list = []\n",
    "    predicted_list = []\n",
    "    \n",
    "    for user_id in actual_products.keys():\n",
    "        if user_id in predicted_products:\n",
    "            actual_list.append(actual_products[user_id])\n",
    "            predicted_list.append(predicted_products[user_id])\n",
    "        else:\n",
    "            actual_list.append(actual_products[user_id])\n",
    "            predicted_list.append([])  # No predictions for this user\n",
    "    \n",
    "    map_score = mapk(actual_list, predicted_list, k)\n",
    "    \n",
    "    print(f\" MAP@{k}: {map_score:.4f}\")\n",
    "    print(f\" Users evaluated: {len(actual_list):,}\")\n",
    "    print(f\" Coverage: {len([p for p in predicted_list if p]) / len(predicted_list):.2%}\")\n",
    "    \n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3d16c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precision_at_k(actual, predicted, k=7):\n",
    "    \"\"\"\n",
    "    Computes the precision at k.\n",
    "\n",
    "    Parameters:\n",
    "    actual : list - A list of actual relevant items\n",
    "    predicted : list - A list of predicted items ordered by rank\n",
    "    k : int - The maximum number of predicted elements\n",
    "\n",
    "    Returns:\n",
    "    precision : double - The precision at k\n",
    "    \"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    \n",
    "    if not predicted:\n",
    "        return 0.0\n",
    "    \n",
    "    # ƒê·∫øm s·ªë d·ª± ƒëo√°n ƒë√∫ng trong top-k\n",
    "    correct = len(set(predicted) & set(actual))\n",
    "    return correct / k\n",
    "\n",
    "\n",
    "def evaluate_recommendations_with_precision(actual_products, predicted_products, k=7):\n",
    "    \"\"\"\n",
    "    Evaluate recommendation system using MAP@k and Precision@k.\n",
    "\n",
    "    Parameters:\n",
    "    actual_products : dict - {user_id: [list of actual products]}\n",
    "    predicted_products : dict - {user_id: [list of predicted products]}\n",
    "    k : int - Number of recommendations to consider\n",
    "\n",
    "    Returns:\n",
    "    metrics : dict - {'MAP@k': ..., 'Precision@k': ...}\n",
    "    \"\"\"\n",
    "    actual_list = []\n",
    "    predicted_list = []\n",
    "\n",
    "    for user_id in actual_products.keys():\n",
    "        if user_id in predicted_products:\n",
    "            actual_list.append(actual_products[user_id])\n",
    "            predicted_list.append(predicted_products[user_id])\n",
    "        else:\n",
    "            actual_list.append(actual_products[user_id])\n",
    "            predicted_list.append([])  # No predictions for this user\n",
    "\n",
    "    # MAP@k\n",
    "    map_score = mapk(actual_list, predicted_list, k)\n",
    "\n",
    "    # Precision@k trung b√¨nh\n",
    "    precision_scores = [\n",
    "        precision_at_k(a, p, k) for a, p in zip(actual_list, predicted_list)\n",
    "    ]\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "\n",
    "    print(f\" MAP@{k}: {map_score:.4f}\")\n",
    "    print(f\" Precision@{k}: {avg_precision:.4f}\")\n",
    "    print(f\" Users evaluated: {len(actual_list):,}\")\n",
    "    print(f\" Coverage: {len([p for p in predicted_list if p]) / len(predicted_list):.2%}\")\n",
    "\n",
    "    return {f\"MAP@{k}\": map_score, f\"Precision@{k}\": avg_precision}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9f5d0",
   "metadata": {},
   "source": [
    "# Data Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7dbb7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessor updated with:\n",
      "   - 24 numeric features\n",
      "   - 4 categorical features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Updated preprocessors\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),   # Fill missing values with mean\n",
    "    ('scaler', StandardScaler())                   # Scale numerical data\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing with most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))     # One-hot encode categorical data\n",
    "])\n",
    "\n",
    "# Updated ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, available_num_cols),\n",
    "        ('cat', categorical_transformer, available_cat_cols)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any remaining columns\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Preprocessor updated with:\")\n",
    "print(f\"   - {len(available_num_cols)} numeric features\")\n",
    "print(f\"   - {len(available_cat_cols)} categorical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da672c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XGBoost added to models (with fixed parameters)\n",
      "‚úÖ LightGBM added to models\n",
      "\n",
      "ü§ñ MODELS PREPARED: ['Random Forest', 'XGBoost', 'LightGBM']\n",
      "üìä Total models to train: 3\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# MULTI-LABEL CLASSIFICATION MODELS - FIXED\n",
    "# =============================\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, hamming_loss, jaccard_score\n",
    "import time\n",
    "\n",
    "# Define all models for multi-label classification\n",
    "models = {\n",
    "    'Random Forest': MultiOutputClassifier(\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    "}\n",
    "\n",
    "# Add XGBoost if available - FIXED with proper parameters\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    models['XGBoost'] = MultiOutputClassifier(\n",
    "        XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbosity=0,\n",
    "            base_score=0.5,  # Fix for the logistic loss error\n",
    "            objective='binary:logistic',  # Explicitly set objective\n",
    "            eval_metric='logloss'  # Set evaluation metric\n",
    "        )\n",
    "    )\n",
    "    print(\"‚úÖ XGBoost added to models (with fixed parameters)\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è XGBoost not available - install with: pip install xgboost\")\n",
    "\n",
    "# Add LightGBM if available - with proper parameters\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    models['LightGBM'] = MultiOutputClassifier(\n",
    "        LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1,\n",
    "            objective='binary',  # Explicitly set objective\n",
    "            boosting_type='gbdt'\n",
    "        )\n",
    "    )\n",
    "    print(\"‚úÖ LightGBM added to models\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LightGBM not available - install with: pip install lightgbm\")\n",
    "\n",
    "# Create complete pipelines for all models\n",
    "pipelines = {}\n",
    "for name, model in models.items():\n",
    "    pipelines[name] = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "print(f\"\\nü§ñ MODELS PREPARED: {list(pipelines.keys())}\")\n",
    "print(f\"üìä Total models to train: {len(pipelines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f841ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# ADDITIONAL METRIC FUNCTIONS - AUC@k AND HIT RATE@k\n",
    "# =============================\n",
    "\n",
    "def auc_at_k(actual, predicted_scores, k):\n",
    "    \"\"\"\n",
    "    Calculate AUC@k metric\n",
    "    \n",
    "    Parameters:\n",
    "    actual : list - List of actual relevant items\n",
    "    predicted_scores : list of tuples - List of (item, score) tuples sorted by score descending\n",
    "    k : int - Number of top predictions to consider\n",
    "    \n",
    "    Returns:\n",
    "    auc_score : float - AUC@k score\n",
    "    \"\"\"\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    \n",
    "    # Get top k predictions\n",
    "    top_k_predictions = predicted_scores[:k]\n",
    "    \n",
    "    # Create binary relevance labels for top k\n",
    "    relevance_labels = []\n",
    "    scores = []\n",
    "    \n",
    "    for item, score in top_k_predictions:\n",
    "        relevance_labels.append(1 if item in actual else 0)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # If all predictions are relevant or all are irrelevant, return appropriate score\n",
    "    if len(set(relevance_labels)) == 1:\n",
    "        return 1.0 if relevance_labels[0] == 1 else 0.0\n",
    "    \n",
    "    # Calculate AUC using sklearn\n",
    "    try:\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        auc_score = roc_auc_score(relevance_labels, scores)\n",
    "        return auc_score\n",
    "    except:\n",
    "        # Fallback manual calculation\n",
    "        return manual_auc_calculation(relevance_labels, scores)\n",
    "\n",
    "def manual_auc_calculation(labels, scores):\n",
    "    \"\"\"Manual AUC calculation as fallback\"\"\"\n",
    "    # Sort by scores descending\n",
    "    pairs = list(zip(labels, scores))\n",
    "    pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Count concordant and discordant pairs\n",
    "    concordant = 0\n",
    "    discordant = 0\n",
    "    \n",
    "    for i in range(len(pairs)):\n",
    "        for j in range(i + 1, len(pairs)):\n",
    "            if pairs[i][0] > pairs[j][0]:  # i is more relevant than j\n",
    "                if pairs[i][1] > pairs[j][1]:  # i has higher score\n",
    "                    concordant += 1\n",
    "                elif pairs[i][1] < pairs[j][1]:  # i has lower score\n",
    "                    discordant += 1\n",
    "    \n",
    "    if concordant + discordant == 0:\n",
    "        return 0.5\n",
    "    \n",
    "    return concordant / (concordant + discordant)\n",
    "\n",
    "def hit_rate_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Calculate Hit Rate@k metric\n",
    "    \n",
    "    Parameters:\n",
    "    actual : list - List of actual relevant items\n",
    "    predicted : list - List of predicted items (ranked)\n",
    "    k : int - Number of top predictions to consider\n",
    "    \n",
    "    Returns:\n",
    "    hit_rate : float - Hit Rate@k score (1 if any relevant item in top k, 0 otherwise)\n",
    "    \"\"\"\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    \n",
    "    # Get top k predictions\n",
    "    top_k = predicted[:k]\n",
    "    \n",
    "    # Check if any actual item is in top k\n",
    "    for item in actual:\n",
    "        if item in top_k:\n",
    "            return 1.0\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "# =============================\n",
    "# COMPREHENSIVE MODEL EVALUATION FUNCTION - UPDATED WITH AUC@k AND HIT RATE@k\n",
    "# =============================\n",
    "\n",
    "def evaluate_model_comprehensive(model_pipeline, model_name, processed_test_data, tar_cols):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation function for recommendation system\n",
    "    \n",
    "    Parameters:\n",
    "    model_pipeline : sklearn.pipeline.Pipeline - Trained model pipeline\n",
    "    model_name : str - Name of the model\n",
    "    processed_test_data : dict - Dictionary of processed test datasets\n",
    "    tar_cols : list - List of target column names\n",
    "    \n",
    "    Returns:\n",
    "    results_df : pd.DataFrame - Comprehensive results DataFrame\n",
    "    detailed_results : dict - Detailed results for further analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüß™ EVALUATING {model_name.upper()}...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_results = []\n",
    "    detailed_results = {\n",
    "        'model_name': model_name,\n",
    "        'monthly_results': {},\n",
    "        'overall_metrics': {},\n",
    "        'all_actual_products': {},\n",
    "        'all_predicted_products': {},\n",
    "        'all_predicted_scores': {}  # Store scores for AUC calculation\n",
    "    }\n",
    "    \n",
    "    customer_offset = 0\n",
    "    \n",
    "    for month_name, data in processed_test_data.items():\n",
    "        print(f\"\\nüìä Evaluating on {month_name}...\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        X_test_month = data['X']\n",
    "        y_test_month = data['y']\n",
    "        \n",
    "        if y_test_month is not None:\n",
    "            # Make predictions\n",
    "            start_time = time.time()\n",
    "            y_pred_month = model_pipeline.predict(X_test_month)\n",
    "            prediction_time = time.time() - start_time\n",
    "            \n",
    "            # Get prediction probabilities for ranking\n",
    "            y_pred_proba = model_pipeline.predict_proba(X_test_month)\n",
    "            \n",
    "            # Calculate standard classification metrics\n",
    "            test_accuracy = accuracy_score(y_test_month, y_pred_month)\n",
    "            hamming_loss_score = hamming_loss(y_test_month, y_pred_month)\n",
    "            jaccard_score_macro = jaccard_score(y_test_month, y_pred_month, average='macro')\n",
    "            \n",
    "            print(f\"   üìà Standard Metrics:\")\n",
    "            print(f\"      Accuracy: {test_accuracy:.4f}\")\n",
    "            print(f\"      Hamming Loss: {hamming_loss_score:.4f}\")\n",
    "            print(f\"      Jaccard (Macro): {jaccard_score_macro:.4f}\")\n",
    "            \n",
    "            # =============================\n",
    "            # PREPARE RECOMMENDATION DATA\n",
    "            # =============================\n",
    "            actual_products = {}\n",
    "            predicted_products = {}\n",
    "            predicted_scores = {}  # Store scores for AUC calculation\n",
    "            \n",
    "            for i in range(len(y_test_month)):\n",
    "                customer_id = i\n",
    "                \n",
    "                # Get actual products\n",
    "                actual = []\n",
    "                for j, product in enumerate(tar_cols):\n",
    "                    if y_test_month.iloc[i, j] == 1:\n",
    "                        actual.append(product)\n",
    "                actual_products[customer_id] = actual\n",
    "                \n",
    "                # Get predicted products (ranked by probability)\n",
    "                product_scores = []\n",
    "                for j, product in enumerate(tar_cols):\n",
    "                    # Get probability of class 1\n",
    "                    prob = y_pred_proba[j][i][1] if len(y_pred_proba[j][i]) > 1 else y_pred_proba[j][i][0]\n",
    "                    product_scores.append((product, prob))\n",
    "                \n",
    "                # Sort by probability (descending)\n",
    "                product_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                predicted = [product for product, score in product_scores]\n",
    "                predicted_products[customer_id] = predicted\n",
    "                predicted_scores[customer_id] = product_scores\n",
    "                \n",
    "                # Add to overall data with offset\n",
    "                detailed_results['all_actual_products'][customer_id + customer_offset] = actual\n",
    "                detailed_results['all_predicted_products'][customer_id + customer_offset] = predicted\n",
    "                detailed_results['all_predicted_scores'][customer_id + customer_offset] = product_scores\n",
    "            \n",
    "            customer_offset += len(actual_products)\n",
    "            \n",
    "            # =============================\n",
    "            # CALCULATE RECOMMENDATION METRICS - WITH AUC@k AND HIT RATE@k\n",
    "            # =============================\n",
    "            print(f\"\\n   üéØ Recommendation Metrics:\")\n",
    "            \n",
    "            month_metrics = {}\n",
    "            for k in [2, 3, 4, 5]:\n",
    "                # Calculate MAP@k and Precision@k\n",
    "                actual_list = list(actual_products.values())\n",
    "                predicted_list = list(predicted_products.values())\n",
    "                predicted_scores_list = list(predicted_scores.values())\n",
    "                \n",
    "                # MAP@k\n",
    "                map_score = mapk(actual_list, predicted_list, k)\n",
    "                \n",
    "                # Precision@k\n",
    "                precision_scores = [precision_at_k(a, p, k) for a, p in zip(actual_list, predicted_list)]\n",
    "                avg_precision = np.mean(precision_scores)\n",
    "                \n",
    "                # AUC@k\n",
    "                auc_scores = [auc_at_k(a, ps, k) for a, ps in zip(actual_list, predicted_scores_list)]\n",
    "                avg_auc = np.mean(auc_scores)\n",
    "                \n",
    "                # Hit Rate@k\n",
    "                hit_rate_scores = [hit_rate_at_k(a, p, k) for a, p in zip(actual_list, predicted_list)]\n",
    "                avg_hit_rate = np.mean(hit_rate_scores)\n",
    "                \n",
    "                month_metrics[f'MAP@{k}'] = map_score\n",
    "                month_metrics[f'Precision@{k}'] = avg_precision\n",
    "                month_metrics[f'AUC@{k}'] = avg_auc\n",
    "                month_metrics[f'HitRate@{k}'] = avg_hit_rate\n",
    "                \n",
    "                print(f\"      k={k}: MAP={map_score:.4f}, Precision={avg_precision:.4f}, AUC={avg_auc:.4f}, HitRate={avg_hit_rate:.4f}\")\n",
    "            \n",
    "            # =============================\n",
    "            # MONTHLY STATISTICS\n",
    "            # =============================\n",
    "            total_customers = len(actual_products)\n",
    "            customers_with_products = len([p for p in actual_products.values() if p])\n",
    "            avg_products_per_customer = np.mean([len(p) for p in actual_products.values()])\n",
    "            \n",
    "            # Product distribution\n",
    "            product_counts = {}\n",
    "            for products in actual_products.values():\n",
    "                for product in products:\n",
    "                    product_counts[product] = product_counts.get(product, 0) + 1\n",
    "            \n",
    "            # Store monthly result\n",
    "            month_result = {\n",
    "                'Model': model_name,\n",
    "                'Month': month_name,\n",
    "                'Total_Customers': total_customers,\n",
    "                'Customers_With_Products': customers_with_products,\n",
    "                'Coverage_Rate': customers_with_products / total_customers,\n",
    "                'Avg_Products_Per_Customer': avg_products_per_customer,\n",
    "                'Test_Accuracy': test_accuracy,\n",
    "                'Hamming_Loss': hamming_loss_score,\n",
    "                'Jaccard_Macro': jaccard_score_macro,\n",
    "                'Prediction_Time': prediction_time,\n",
    "                **month_metrics  # Add all MAP@k, Precision@k, AUC@k, HitRate@k metrics\n",
    "            }\n",
    "            \n",
    "            all_results.append(month_result)\n",
    "            \n",
    "            # Store detailed results\n",
    "            detailed_results['monthly_results'][month_name] = {\n",
    "                'actual_products': actual_products,\n",
    "                'predicted_products': predicted_products,\n",
    "                'predicted_scores': predicted_scores,\n",
    "                'metrics': month_metrics,\n",
    "                'stats': month_result,\n",
    "                'product_distribution': product_counts\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚è±Ô∏è Time: {prediction_time:.2f}s\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No ground truth available for {month_name}\")\n",
    "    \n",
    "    # =============================\n",
    "    # CALCULATE OVERALL METRICS - WITH AUC@k AND HIT RATE@k\n",
    "    # =============================\n",
    "    if detailed_results['all_actual_products'] and detailed_results['all_predicted_products']:\n",
    "        print(f\"\\nüéØ Overall Performance:\")\n",
    "        \n",
    "        overall_metrics = {}\n",
    "        for k in [2, 3, 4, 5]:\n",
    "            actual_list = list(detailed_results['all_actual_products'].values())\n",
    "            predicted_list = list(detailed_results['all_predicted_products'].values())\n",
    "            predicted_scores_list = list(detailed_results['all_predicted_scores'].values())\n",
    "            \n",
    "            # Overall MAP@k and Precision@k\n",
    "            map_score = mapk(actual_list, predicted_list, k)\n",
    "            precision_scores = [precision_at_k(a, p, k) for a, p in zip(actual_list, predicted_list)]\n",
    "            avg_precision = np.mean(precision_scores)\n",
    "            \n",
    "            # Overall AUC@k\n",
    "            auc_scores = [auc_at_k(a, ps, k) for a, ps in zip(actual_list, predicted_scores_list)]\n",
    "            avg_auc = np.mean(auc_scores)\n",
    "            \n",
    "            # Overall Hit Rate@k\n",
    "            hit_rate_scores = [hit_rate_at_k(a, p, k) for a, p in zip(actual_list, predicted_list)]\n",
    "            avg_hit_rate = np.mean(hit_rate_scores)\n",
    "            \n",
    "            overall_metrics[f'Overall_MAP@{k}'] = map_score\n",
    "            overall_metrics[f'Overall_Precision@{k}'] = avg_precision\n",
    "            overall_metrics[f'Overall_AUC@{k}'] = avg_auc\n",
    "            overall_metrics[f'Overall_HitRate@{k}'] = avg_hit_rate\n",
    "            \n",
    "            print(f\"   Overall k={k}: MAP={map_score:.4f}, Precision={avg_precision:.4f}, AUC={avg_auc:.4f}, HitRate={avg_hit_rate:.4f}\")\n",
    "        \n",
    "        # Add overall row\n",
    "        overall_result = {\n",
    "            'Model': model_name,\n",
    "            'Month': 'OVERALL',\n",
    "            'Total_Customers': len(detailed_results['all_actual_products']),\n",
    "            'Customers_With_Products': len([p for p in detailed_results['all_actual_products'].values() if p]),\n",
    "            'Coverage_Rate': len([p for p in detailed_results['all_actual_products'].values() if p]) / len(detailed_results['all_actual_products']),\n",
    "            'Avg_Products_Per_Customer': np.mean([len(p) for p in detailed_results['all_actual_products'].values()]),\n",
    "            'Test_Accuracy': np.mean([r['Test_Accuracy'] for r in all_results]),\n",
    "            'Hamming_Loss': np.mean([r['Hamming_Loss'] for r in all_results]),\n",
    "            'Jaccard_Macro': np.mean([r['Jaccard_Macro'] for r in all_results]),\n",
    "            'Prediction_Time': sum([r['Prediction_Time'] for r in all_results]),\n",
    "        }\n",
    "        \n",
    "        # Add overall metrics - WITH AUC@k AND HIT RATE@k\n",
    "        for k in [2, 3, 4, 5]:\n",
    "            overall_result[f'MAP@{k}'] = overall_metrics[f'Overall_MAP@{k}']\n",
    "            overall_result[f'Precision@{k}'] = overall_metrics[f'Overall_Precision@{k}']\n",
    "            overall_result[f'AUC@{k}'] = overall_metrics[f'Overall_AUC@{k}']\n",
    "            overall_result[f'HitRate@{k}'] = overall_metrics[f'Overall_HitRate@{k}']\n",
    "        \n",
    "        all_results.append(overall_result)\n",
    "        detailed_results['overall_metrics'] = overall_metrics\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    return results_df, detailed_results\n",
    "\n",
    "# =============================\n",
    "# EXAMPLE USAGE WITH VISUALIZATION\n",
    "# =============================\n",
    "\n",
    "def visualize_metrics_comparison(results_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize comparison of different metrics across models and months\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create subplots for different metric types\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    fig.suptitle('Recommendation System Performance Metrics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Filter out OVERALL rows for monthly comparison\n",
    "    monthly_data = results_df[results_df['Month'] != 'OVERALL']\n",
    "    \n",
    "    # 1. MAP@k comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    map_cols = [col for col in results_df.columns if col.startswith('MAP@')]\n",
    "    for col in map_cols:\n",
    "        if col in monthly_data.columns:\n",
    "            ax1.plot(monthly_data['Month'], monthly_data[col], marker='o', linewidth=2, label=col)\n",
    "    ax1.set_title('Mean Average Precision @k', fontweight='bold')\n",
    "    ax1.set_xlabel('Month')\n",
    "    ax1.set_ylabel('MAP Score')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 2. AUC@k comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    auc_cols = [col for col in results_df.columns if col.startswith('AUC@')]\n",
    "    for col in auc_cols:\n",
    "        if col in monthly_data.columns:\n",
    "            ax2.plot(monthly_data['Month'], monthly_data[col], marker='s', linewidth=2, label=col)\n",
    "    ax2.set_title('Area Under Curve @k', fontweight='bold')\n",
    "    ax2.set_xlabel('Month')\n",
    "    ax2.set_ylabel('AUC Score')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    plt.setp(ax2.get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 3. Hit Rate@k comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    hitrate_cols = [col for col in results_df.columns if col.startswith('HitRate@')]\n",
    "    for col in hitrate_cols:\n",
    "        if col in monthly_data.columns:\n",
    "            ax3.plot(monthly_data['Month'], monthly_data[col], marker='^', linewidth=2, label=col)\n",
    "    ax3.set_title('Hit Rate @k', fontweight='bold')\n",
    "    ax3.set_xlabel('Month')\n",
    "    ax3.set_ylabel('Hit Rate')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    plt.setp(ax3.get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # 4. Precision@k comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    precision_cols = [col for col in results_df.columns if col.startswith('Precision@')]\n",
    "    for col in precision_cols:\n",
    "        if col in monthly_data.columns:\n",
    "            ax4.plot(monthly_data['Month'], monthly_data[col], marker='d', linewidth=2, label=col)\n",
    "    ax4.set_title('Precision @k', fontweight='bold')\n",
    "    ax4.set_xlabel('Month')\n",
    "    ax4.set_ylabel('Precision Score')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    plt.setp(ax4.get_xticklabels(), rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def print_metrics_summary(results_df):\n",
    "    \"\"\"\n",
    "    Print a comprehensive summary of all metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä COMPREHENSIVE METRICS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall performance\n",
    "    overall_row = results_df[results_df['Month'] == 'OVERALL']\n",
    "    if not overall_row.empty:\n",
    "        print(f\"\\nüéØ OVERALL PERFORMANCE:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for k in [2, 3, 4, 5]:\n",
    "            print(f\"\\nTop-{k} Recommendations:\")\n",
    "            print(f\"  MAP@{k}:       {overall_row[f'MAP@{k}'].iloc[0]:.4f}\")\n",
    "            print(f\"  Precision@{k}: {overall_row[f'Precision@{k}'].iloc[0]:.4f}\")\n",
    "            print(f\"  AUC@{k}:       {overall_row[f'AUC@{k}'].iloc[0]:.4f}\")\n",
    "            print(f\"  Hit Rate@{k}:  {overall_row[f'HitRate@{k}'].iloc[0]:.4f}\")\n",
    "    \n",
    "    # Best performing k\n",
    "    print(f\"\\nüèÜ BEST PERFORMING K VALUES:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for metric_prefix in ['MAP', 'Precision', 'AUC', 'HitRate']:\n",
    "        metric_cols = [col for col in overall_row.columns if col.startswith(f'{metric_prefix}@')]\n",
    "        if metric_cols:\n",
    "            best_col = overall_row[metric_cols].idxmax(axis=1).iloc[0]\n",
    "            best_value = overall_row[best_col].iloc[0]\n",
    "            k_value = best_col.split('@')[1]\n",
    "            print(f\"  {metric_prefix}: k={k_value} (score={best_value:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5606ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Formatting test data: 1,017,191 samples\n",
      "   ‚úÖ Ground truth available: (1017191, 9)\n",
      "üìã Formatting test data: 1,223,624 samples\n",
      "   ‚úÖ Ground truth available: (1223624, 9)\n",
      "üìã Formatting test data: 1,425,848 samples\n",
      "   ‚úÖ Ground truth available: (1425848, 9)\n",
      "\n",
      "‚úÖ TEST DATA FORMATTED FOR EVALUATION!\n",
      "   October 2015: X=(1017191, 28), y=(1017191, 9)\n",
      "   November 2015: X=(1223624, 28), y=(1223624, 9)\n",
      "   December 2015: X=(1425848, 28), y=(1425848, 9)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# PREPARE TEST DATA FORMAT FOR EVALUATION\n",
    "# =============================\n",
    "\n",
    "def format_test_data_for_evaluation(test_df, available_num_cols, available_cat_cols, tar_cols):\n",
    "    \"\"\"\n",
    "    Format processed test data ƒë·ªÉ c√≥ structure ph√π h·ª£p v·ªõi evaluation function\n",
    "    \"\"\"\n",
    "    print(f\"üìã Formatting test data: {len(test_df):,} samples\")\n",
    "    \n",
    "    # Th√™m missing features n·∫øu c·∫ßn\n",
    "    test_formatted = test_df.copy()\n",
    "    \n",
    "    # Add missing numeric features\n",
    "    for feature in available_num_cols:\n",
    "        if feature not in test_formatted.columns:\n",
    "            test_formatted[feature] = 0\n",
    "            print(f\"   ‚ö†Ô∏è Added missing numeric: {feature}\")\n",
    "    \n",
    "    # Add missing categorical features  \n",
    "    for feature in available_cat_cols:\n",
    "        if feature not in test_formatted.columns:\n",
    "            test_formatted[feature] = 'Unknown'\n",
    "            print(f\"   ‚ö†Ô∏è Added missing categorical: {feature}\")\n",
    "    \n",
    "    # Prepare X v√† y\n",
    "    X_test = test_formatted[available_num_cols + available_cat_cols]\n",
    "    \n",
    "    # Check target columns\n",
    "    if all(col in test_formatted.columns for col in tar_cols):\n",
    "        y_test = test_formatted[tar_cols]\n",
    "        print(f\"   ‚úÖ Ground truth available: {y_test.shape}\")\n",
    "    else:\n",
    "        y_test = None\n",
    "        print(f\"   ‚ö†Ô∏è No ground truth available\")\n",
    "    \n",
    "    return {'X': X_test, 'y': y_test}\n",
    "\n",
    "# Format test datasets\n",
    "processed_test_data = {\n",
    "    'October 2015': format_test_data_for_evaluation(\n",
    "        test_10_2015, available_num_cols, available_cat_cols, tar_cols\n",
    "    ),\n",
    "    'November 2015': format_test_data_for_evaluation(\n",
    "        test_11_2015, available_num_cols, available_cat_cols, tar_cols\n",
    "    ),\n",
    "    'December 2015': format_test_data_for_evaluation(\n",
    "        test_12_2015, available_num_cols, available_cat_cols, tar_cols\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úÖ TEST DATA FORMATTED FOR EVALUATION!\")\n",
    "for month, data in processed_test_data.items():\n",
    "    x_shape = data['X'].shape\n",
    "    y_shape = data['y'].shape if data['y'] is not None else 'None'\n",
    "    print(f\"   {month}: X={x_shape}, y={y_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "671e9243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ TRAINING AND EVALUATING ALL MODELS...\n",
      "======================================================================\n",
      "\n",
      "ü§ñ TRAINING RANDOM FOREST...\n",
      "--------------------------------------------------\n",
      "üìä Calculating training metrics...\n",
      "   ‚úÖ Training completed in 81.68s\n",
      "   üìà Train Accuracy: 0.9509\n",
      "\n",
      "üß™ EVALUATING RANDOM FOREST...\n",
      "============================================================\n",
      "\n",
      "üìä Evaluating on October 2015...\n",
      "----------------------------------------\n",
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9675\n",
      "      Hamming Loss: 0.0041\n",
      "      Jaccard (Macro): 0.0101\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0118, Precision=0.0123, AUC=0.0000, HitRate=0.0246\n",
      "      k=3: MAP=0.0129, Precision=0.0097, AUC=0.0118, HitRate=0.0281\n",
      "      k=4: MAP=0.0136, Precision=0.0080, AUC=0.0168, HitRate=0.0294\n",
      "      k=5: MAP=0.0140, Precision=0.0068, AUC=0.0197, HitRate=0.0301\n",
      "   ‚è±Ô∏è Time: 11.80s\n",
      "\n",
      "üìä Evaluating on November 2015...\n",
      "----------------------------------------\n",
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9696\n",
      "      Hamming Loss: 0.0038\n",
      "      Jaccard (Macro): 0.0102\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0108, Precision=0.0112, AUC=0.0000, HitRate=0.0225\n",
      "      k=3: MAP=0.0118, Precision=0.0088, AUC=0.0109, HitRate=0.0258\n",
      "      k=4: MAP=0.0124, Precision=0.0072, AUC=0.0155, HitRate=0.0270\n",
      "      k=5: MAP=0.0128, Precision=0.0061, AUC=0.0182, HitRate=0.0277\n",
      "   ‚è±Ô∏è Time: 13.87s\n",
      "\n",
      "üìä Evaluating on December 2015...\n",
      "----------------------------------------\n",
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9753\n",
      "      Hamming Loss: 0.0030\n",
      "      Jaccard (Macro): 0.0097\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0090, Precision=0.0093, AUC=0.0000, HitRate=0.0186\n",
      "      k=3: MAP=0.0096, Precision=0.0070, AUC=0.0091, HitRate=0.0205\n",
      "      k=4: MAP=0.0100, Precision=0.0057, AUC=0.0126, HitRate=0.0213\n",
      "      k=5: MAP=0.0103, Precision=0.0048, AUC=0.0146, HitRate=0.0219\n",
      "   ‚è±Ô∏è Time: 16.19s\n",
      "\n",
      "üéØ Overall Performance:\n",
      "   Overall k=2: MAP=0.0103, Precision=0.0108, AUC=0.0000, HitRate=0.0216\n",
      "   Overall k=3: MAP=0.0113, Precision=0.0083, AUC=0.0105, HitRate=0.0244\n",
      "   Overall k=4: MAP=0.0118, Precision=0.0068, AUC=0.0147, HitRate=0.0255\n",
      "   Overall k=5: MAP=0.0121, Precision=0.0058, AUC=0.0172, HitRate=0.0261\n",
      "\n",
      "‚úÖ Random Forest evaluation completed!\n",
      "üîß Pipeline saved for future use\n",
      "\n",
      "ü§ñ TRAINING XGBOOST...\n",
      "--------------------------------------------------\n",
      "üìä Calculating training metrics...\n",
      "   ‚úÖ Training completed in 8.87s\n",
      "   üìà Train Accuracy: 0.9511\n",
      "\n",
      "üß™ EVALUATING XGBOOST...\n",
      "============================================================\n",
      "\n",
      "üìä Evaluating on October 2015...\n",
      "----------------------------------------\n",
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9682\n",
      "      Hamming Loss: 0.0040\n",
      "      Jaccard (Macro): 0.0107\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0254, Precision=0.0145, AUC=0.0245, HitRate=0.0280\n",
      "      k=3: MAP=0.0261, Precision=0.0106, AUC=0.0257, HitRate=0.0293\n",
      "      k=4: MAP=0.0266, Precision=0.0084, AUC=0.0266, HitRate=0.0300\n",
      "      k=5: MAP=0.0268, Precision=0.0070, AUC=0.0273, HitRate=0.0306\n",
      "   ‚è±Ô∏è Time: 2.45s\n",
      "\n",
      "üìä Evaluating on November 2015...\n",
      "----------------------------------------\n",
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9703\n",
      "      Hamming Loss: 0.0037\n",
      "      Jaccard (Macro): 0.0117\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0233, Precision=0.0132, AUC=0.0225, HitRate=0.0257\n",
      "      k=3: MAP=0.0239, Precision=0.0096, AUC=0.0236, HitRate=0.0270\n",
      "      k=4: MAP=0.0244, Precision=0.0076, AUC=0.0245, HitRate=0.0277\n",
      "      k=5: MAP=0.0246, Precision=0.0063, AUC=0.0251, HitRate=0.0281\n",
      "   ‚è±Ô∏è Time: 3.45s\n",
      "\n",
      "üìä Evaluating on December 2015...\n",
      "----------------------------------------\n",
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9760\n",
      "      Hamming Loss: 0.0030\n",
      "      Jaccard (Macro): 0.0114\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0190, Precision=0.0106, AUC=0.0186, HitRate=0.0206\n",
      "      k=3: MAP=0.0194, Precision=0.0076, AUC=0.0192, HitRate=0.0213\n",
      "      k=4: MAP=0.0197, Precision=0.0060, AUC=0.0197, HitRate=0.0219\n",
      "      k=5: MAP=0.0199, Precision=0.0050, AUC=0.0202, HitRate=0.0223\n",
      "   ‚è±Ô∏è Time: 4.04s\n",
      "\n",
      "üéØ Overall Performance:\n",
      "   Overall k=2: MAP=0.0222, Precision=0.0125, AUC=0.0216, HitRate=0.0244\n",
      "   Overall k=3: MAP=0.0228, Precision=0.0091, AUC=0.0225, HitRate=0.0254\n",
      "   Overall k=4: MAP=0.0232, Precision=0.0072, AUC=0.0232, HitRate=0.0261\n",
      "   Overall k=5: MAP=0.0234, Precision=0.0060, AUC=0.0238, HitRate=0.0265\n",
      "\n",
      "‚úÖ XGBoost evaluation completed!\n",
      "üîß Pipeline saved for future use\n",
      "\n",
      "ü§ñ TRAINING LIGHTGBM...\n",
      "--------------------------------------------------\n",
      "üìä Calculating training metrics...\n",
      "   ‚úÖ Training completed in 7.05s\n",
      "   üìà Train Accuracy: 0.9398\n",
      "\n",
      "üß™ EVALUATING LIGHTGBM...\n",
      "============================================================\n",
      "\n",
      "üìä Evaluating on October 2015...\n",
      "----------------------------------------\n",
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9235\n",
      "      Hamming Loss: 0.0091\n",
      "      Jaccard (Macro): 0.0056\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0245, Precision=0.0142, AUC=0.0232, HitRate=0.0275\n",
      "      k=3: MAP=0.0253, Precision=0.0105, AUC=0.0247, HitRate=0.0291\n",
      "      k=4: MAP=0.0257, Precision=0.0084, AUC=0.0259, HitRate=0.0299\n",
      "      k=5: MAP=0.0259, Precision=0.0069, AUC=0.0268, HitRate=0.0303\n",
      "   ‚è±Ô∏è Time: 4.66s\n",
      "\n",
      "üìä Evaluating on November 2015...\n",
      "----------------------------------------\n",
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9165\n",
      "      Hamming Loss: 0.0098\n",
      "      Jaccard (Macro): 0.0044\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0223, Precision=0.0129, AUC=0.0210, HitRate=0.0253\n",
      "      k=3: MAP=0.0230, Precision=0.0095, AUC=0.0226, HitRate=0.0267\n",
      "      k=4: MAP=0.0234, Precision=0.0076, AUC=0.0237, HitRate=0.0275\n",
      "      k=5: MAP=0.0236, Precision=0.0062, AUC=0.0245, HitRate=0.0279\n",
      "   ‚è±Ô∏è Time: 5.84s\n",
      "\n",
      "üìä Evaluating on December 2015...\n",
      "----------------------------------------\n",
      "   üìà Standard Metrics:\n",
      "      Accuracy: 0.9287\n",
      "      Hamming Loss: 0.0083\n",
      "      Jaccard (Macro): 0.0035\n",
      "\n",
      "   üéØ Recommendation Metrics:\n",
      "      k=2: MAP=0.0181, Precision=0.0103, AUC=0.0174, HitRate=0.0201\n",
      "      k=3: MAP=0.0186, Precision=0.0075, AUC=0.0184, HitRate=0.0211\n",
      "      k=4: MAP=0.0190, Precision=0.0060, AUC=0.0191, HitRate=0.0218\n",
      "      k=5: MAP=0.0191, Precision=0.0049, AUC=0.0197, HitRate=0.0221\n",
      "   ‚è±Ô∏è Time: 6.84s\n",
      "\n",
      "üéØ Overall Performance:\n",
      "   Overall k=2: MAP=0.0213, Precision=0.0123, AUC=0.0202, HitRate=0.0239\n",
      "   Overall k=3: MAP=0.0219, Precision=0.0090, AUC=0.0216, HitRate=0.0252\n",
      "   Overall k=4: MAP=0.0223, Precision=0.0072, AUC=0.0225, HitRate=0.0259\n",
      "   Overall k=5: MAP=0.0225, Precision=0.0059, AUC=0.0233, HitRate=0.0263\n",
      "\n",
      "‚úÖ LightGBM evaluation completed!\n",
      "üîß Pipeline saved for future use\n",
      "\n",
      "üéâ ALL MODELS TRAINED AND EVALUATED!\n",
      "üìä SUMMARY:\n",
      "   Models evaluated: 3\n",
      "   Total result rows: 12\n",
      "   Test datasets: 3\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# TRAIN ALL MODELS AND EVALUATE COMPREHENSIVELY\n",
    "# =============================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train = df_enhanced[available_num_cols + available_cat_cols]\n",
    "y_train = df_enhanced[tar_cols]\n",
    "\n",
    "# Storage for all results\n",
    "all_model_results = []\n",
    "all_detailed_results = {}\n",
    "training_results = {}\n",
    "\n",
    "print(\"üöÄ TRAINING AND EVALUATING ALL MODELS...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"\\nü§ñ TRAINING {model_name.upper()}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # =============================\n",
    "    # TRAIN MODEL\n",
    "    # =============================\n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Training metrics\n",
    "    print(\"üìä Calculating training metrics...\")\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    \n",
    "    print(f\"   ‚úÖ Training completed in {training_time:.2f}s\")\n",
    "    print(f\"   üìà Train Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Store training results\n",
    "    training_results[model_name] = {\n",
    "        'pipeline': pipeline,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    # =============================\n",
    "    # EVALUATE ON TEST SETS\n",
    "    # =============================\n",
    "    model_results_df, detailed_results = evaluate_model_comprehensive(\n",
    "        pipeline, model_name, processed_test_data, tar_cols\n",
    "    )\n",
    "    \n",
    "    # Add training info to results\n",
    "    model_results_df['Training_Time'] = training_time\n",
    "    model_results_df['Train_Accuracy'] = train_accuracy\n",
    "    \n",
    "    # Store results\n",
    "    all_model_results.append(model_results_df)\n",
    "    all_detailed_results[model_name] = detailed_results\n",
    "    \n",
    "    print(f\"\\n‚úÖ {model_name} evaluation completed!\")\n",
    "    print(f\"üîß Pipeline saved for future use\")\n",
    "\n",
    "# =============================\n",
    "# COMBINE ALL RESULTS\n",
    "# =============================\n",
    "print(f\"\\nüéâ ALL MODELS TRAINED AND EVALUATED!\")\n",
    "final_results_df = pd.concat(all_model_results, ignore_index=True)\n",
    "\n",
    "print(f\"üìä SUMMARY:\")\n",
    "print(f\"   Models evaluated: {len(pipelines)}\")\n",
    "print(f\"   Total result rows: {len(final_results_df)}\")\n",
    "print(f\"   Test datasets: {len(processed_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01d489a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DETAILED RESULTS TABLE:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Month</th>\n",
       "      <th>Total_Customers</th>\n",
       "      <th>Customers_With_Products</th>\n",
       "      <th>Coverage_Rate</th>\n",
       "      <th>Avg_Products_Per_Customer</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Hamming_Loss</th>\n",
       "      <th>Jaccard_Macro</th>\n",
       "      <th>Prediction_Time</th>\n",
       "      <th>...</th>\n",
       "      <th>MAP@4</th>\n",
       "      <th>Precision@4</th>\n",
       "      <th>AUC@4</th>\n",
       "      <th>HitRate@4</th>\n",
       "      <th>MAP@5</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>AUC@5</th>\n",
       "      <th>HitRate@5</th>\n",
       "      <th>Training_Time</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>1017191</td>\n",
       "      <td>31170</td>\n",
       "      <td>0.030643</td>\n",
       "      <td>0.035039</td>\n",
       "      <td>0.967542</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.010136</td>\n",
       "      <td>11.795192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013595</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.016758</td>\n",
       "      <td>0.029385</td>\n",
       "      <td>0.013956</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.019720</td>\n",
       "      <td>0.030114</td>\n",
       "      <td>81.67847</td>\n",
       "      <td>0.950896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>November 2015</td>\n",
       "      <td>1223624</td>\n",
       "      <td>34541</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>0.031845</td>\n",
       "      <td>0.969619</td>\n",
       "      <td>0.003778</td>\n",
       "      <td>0.010163</td>\n",
       "      <td>13.866546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>0.015498</td>\n",
       "      <td>0.027032</td>\n",
       "      <td>0.012762</td>\n",
       "      <td>0.006121</td>\n",
       "      <td>0.018161</td>\n",
       "      <td>0.027705</td>\n",
       "      <td>81.67847</td>\n",
       "      <td>0.950896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>December 2015</td>\n",
       "      <td>1425848</td>\n",
       "      <td>31935</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.024978</td>\n",
       "      <td>0.975345</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>0.009715</td>\n",
       "      <td>16.190880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010012</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>0.012581</td>\n",
       "      <td>0.021348</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.014649</td>\n",
       "      <td>0.021943</td>\n",
       "      <td>81.67847</td>\n",
       "      <td>0.950896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>OVERALL</td>\n",
       "      <td>3666663</td>\n",
       "      <td>97646</td>\n",
       "      <td>0.026631</td>\n",
       "      <td>0.030061</td>\n",
       "      <td>0.970835</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>0.010005</td>\n",
       "      <td>41.852618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011811</td>\n",
       "      <td>0.006843</td>\n",
       "      <td>0.014713</td>\n",
       "      <td>0.025474</td>\n",
       "      <td>0.012119</td>\n",
       "      <td>0.005787</td>\n",
       "      <td>0.017228</td>\n",
       "      <td>0.026133</td>\n",
       "      <td>81.67847</td>\n",
       "      <td>0.950896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>1017191</td>\n",
       "      <td>31170</td>\n",
       "      <td>0.030643</td>\n",
       "      <td>0.035039</td>\n",
       "      <td>0.968173</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>0.010722</td>\n",
       "      <td>2.449513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026565</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>0.026566</td>\n",
       "      <td>0.030007</td>\n",
       "      <td>0.026796</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>0.027307</td>\n",
       "      <td>0.030572</td>\n",
       "      <td>8.86970</td>\n",
       "      <td>0.951054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>November 2015</td>\n",
       "      <td>1223624</td>\n",
       "      <td>34541</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>0.031845</td>\n",
       "      <td>0.970274</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.011692</td>\n",
       "      <td>3.454211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024351</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.024453</td>\n",
       "      <td>0.027668</td>\n",
       "      <td>0.024562</td>\n",
       "      <td>0.006330</td>\n",
       "      <td>0.025108</td>\n",
       "      <td>0.028137</td>\n",
       "      <td>8.86970</td>\n",
       "      <td>0.951054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>December 2015</td>\n",
       "      <td>1425848</td>\n",
       "      <td>31935</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.024978</td>\n",
       "      <td>0.975998</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.011390</td>\n",
       "      <td>4.039769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019719</td>\n",
       "      <td>0.006013</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>0.021895</td>\n",
       "      <td>0.019865</td>\n",
       "      <td>0.004958</td>\n",
       "      <td>0.020194</td>\n",
       "      <td>0.022293</td>\n",
       "      <td>8.86970</td>\n",
       "      <td>0.951054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>OVERALL</td>\n",
       "      <td>3666663</td>\n",
       "      <td>97646</td>\n",
       "      <td>0.026631</td>\n",
       "      <td>0.030061</td>\n",
       "      <td>0.971482</td>\n",
       "      <td>0.003561</td>\n",
       "      <td>0.011268</td>\n",
       "      <td>9.943493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023164</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.023205</td>\n",
       "      <td>0.026072</td>\n",
       "      <td>0.023355</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>0.026540</td>\n",
       "      <td>8.86970</td>\n",
       "      <td>0.951054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>1017191</td>\n",
       "      <td>31170</td>\n",
       "      <td>0.030643</td>\n",
       "      <td>0.035039</td>\n",
       "      <td>0.923488</td>\n",
       "      <td>0.009106</td>\n",
       "      <td>0.005627</td>\n",
       "      <td>4.658453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025747</td>\n",
       "      <td>0.008376</td>\n",
       "      <td>0.025869</td>\n",
       "      <td>0.029910</td>\n",
       "      <td>0.025941</td>\n",
       "      <td>0.006896</td>\n",
       "      <td>0.026771</td>\n",
       "      <td>0.030306</td>\n",
       "      <td>7.04811</td>\n",
       "      <td>0.939831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>November 2015</td>\n",
       "      <td>1223624</td>\n",
       "      <td>34541</td>\n",
       "      <td>0.028228</td>\n",
       "      <td>0.031845</td>\n",
       "      <td>0.916547</td>\n",
       "      <td>0.009815</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>5.838314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023432</td>\n",
       "      <td>0.007581</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.027503</td>\n",
       "      <td>0.023617</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>0.027876</td>\n",
       "      <td>7.04811</td>\n",
       "      <td>0.939831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>December 2015</td>\n",
       "      <td>1425848</td>\n",
       "      <td>31935</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.024978</td>\n",
       "      <td>0.928736</td>\n",
       "      <td>0.008319</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>6.844716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018952</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>0.019085</td>\n",
       "      <td>0.021778</td>\n",
       "      <td>0.019089</td>\n",
       "      <td>0.004908</td>\n",
       "      <td>0.019682</td>\n",
       "      <td>0.022124</td>\n",
       "      <td>7.04811</td>\n",
       "      <td>0.939831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>OVERALL</td>\n",
       "      <td>3666663</td>\n",
       "      <td>97646</td>\n",
       "      <td>0.026631</td>\n",
       "      <td>0.030061</td>\n",
       "      <td>0.922924</td>\n",
       "      <td>0.009080</td>\n",
       "      <td>0.004523</td>\n",
       "      <td>17.341482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022332</td>\n",
       "      <td>0.007173</td>\n",
       "      <td>0.022511</td>\n",
       "      <td>0.025944</td>\n",
       "      <td>0.022501</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0.023265</td>\n",
       "      <td>0.026313</td>\n",
       "      <td>7.04811</td>\n",
       "      <td>0.939831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model          Month  Total_Customers  Customers_With_Products  \\\n",
       "0   Random Forest   October 2015          1017191                    31170   \n",
       "1   Random Forest  November 2015          1223624                    34541   \n",
       "2   Random Forest  December 2015          1425848                    31935   \n",
       "3   Random Forest        OVERALL          3666663                    97646   \n",
       "4         XGBoost   October 2015          1017191                    31170   \n",
       "5         XGBoost  November 2015          1223624                    34541   \n",
       "6         XGBoost  December 2015          1425848                    31935   \n",
       "7         XGBoost        OVERALL          3666663                    97646   \n",
       "8        LightGBM   October 2015          1017191                    31170   \n",
       "9        LightGBM  November 2015          1223624                    34541   \n",
       "10       LightGBM  December 2015          1425848                    31935   \n",
       "11       LightGBM        OVERALL          3666663                    97646   \n",
       "\n",
       "    Coverage_Rate  Avg_Products_Per_Customer  Test_Accuracy  Hamming_Loss  \\\n",
       "0        0.030643                   0.035039       0.967542      0.004093   \n",
       "1        0.028228                   0.031845       0.969619      0.003778   \n",
       "2        0.022397                   0.024978       0.975345      0.003025   \n",
       "3        0.026631                   0.030061       0.970835      0.003632   \n",
       "4        0.030643                   0.035039       0.968173      0.004024   \n",
       "5        0.028228                   0.031845       0.970274      0.003706   \n",
       "6        0.022397                   0.024978       0.975998      0.002953   \n",
       "7        0.026631                   0.030061       0.971482      0.003561   \n",
       "8        0.030643                   0.035039       0.923488      0.009106   \n",
       "9        0.028228                   0.031845       0.916547      0.009815   \n",
       "10       0.022397                   0.024978       0.928736      0.008319   \n",
       "11       0.026631                   0.030061       0.922924      0.009080   \n",
       "\n",
       "    Jaccard_Macro  Prediction_Time  ...     MAP@4  Precision@4     AUC@4  \\\n",
       "0        0.010136        11.795192  ...  0.013595     0.007980  0.016758   \n",
       "1        0.010163        13.866546  ...  0.012426     0.007224  0.015498   \n",
       "2        0.009715        16.190880  ...  0.010012     0.005705  0.012581   \n",
       "3        0.010005        41.852618  ...  0.011811     0.006843  0.014713   \n",
       "4        0.010722         2.449513  ...  0.026565     0.008429  0.026566   \n",
       "5        0.011692         3.454211  ...  0.024351     0.007650  0.024453   \n",
       "6        0.011390         4.039769  ...  0.019719     0.006013  0.019737   \n",
       "7        0.011268         9.943493  ...  0.023164     0.007229  0.023205   \n",
       "8        0.005627         4.658453  ...  0.025747     0.008376  0.025869   \n",
       "9        0.004431         5.838314  ...  0.023432     0.007581  0.023710   \n",
       "10       0.003510         6.844716  ...  0.018952     0.005964  0.019085   \n",
       "11       0.004523        17.341482  ...  0.022332     0.007173  0.022511   \n",
       "\n",
       "    HitRate@4     MAP@5  Precision@5     AUC@5  HitRate@5  Training_Time  \\\n",
       "0    0.029385  0.013956     0.006753  0.019720   0.030114       81.67847   \n",
       "1    0.027032  0.012762     0.006121  0.018161   0.027705       81.67847   \n",
       "2    0.021348  0.010258     0.004812  0.014649   0.021943       81.67847   \n",
       "3    0.025474  0.012119     0.005787  0.017228   0.026133       81.67847   \n",
       "4    0.030007  0.026796     0.006975  0.027307   0.030572        8.86970   \n",
       "5    0.027668  0.024562     0.006330  0.025108   0.028137        8.86970   \n",
       "6    0.021895  0.019865     0.004958  0.020194   0.022293        8.86970   \n",
       "7    0.026072  0.023355     0.005975  0.023807   0.026540        8.86970   \n",
       "8    0.029910  0.025941     0.006896  0.026771   0.030306        7.04811   \n",
       "9    0.027503  0.023617     0.006250  0.024526   0.027876        7.04811   \n",
       "10   0.021778  0.019089     0.004908  0.019682   0.022124        7.04811   \n",
       "11   0.025944  0.022501     0.005907  0.023265   0.026313        7.04811   \n",
       "\n",
       "    Train_Accuracy  \n",
       "0         0.950896  \n",
       "1         0.950896  \n",
       "2         0.950896  \n",
       "3         0.950896  \n",
       "4         0.951054  \n",
       "5         0.951054  \n",
       "6         0.951054  \n",
       "7         0.951054  \n",
       "8         0.939831  \n",
       "9         0.939831  \n",
       "10        0.939831  \n",
       "11        0.939831  \n",
       "\n",
       "[12 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ MODEL PERFORMANCE SUMMARY:\n",
      "================================================================================\n",
      "\n",
      "üéØ RANKING BY MAP@5:\n",
      "--------------------------------------------------\n",
      "1. XGBoost: MAP@5 = 0.0234\n",
      "2. LightGBM: MAP@5 = 0.0225\n",
      "3. Random Forest: MAP@5 = 0.0121\n",
      "\n",
      "üìã COMPREHENSIVE METRICS COMPARISON:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAP@2</th>\n",
       "      <th>MAP@3</th>\n",
       "      <th>MAP@4</th>\n",
       "      <th>MAP@5</th>\n",
       "      <th>Precision@2</th>\n",
       "      <th>Precision@3</th>\n",
       "      <th>Precision@4</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>AUC@2</th>\n",
       "      <th>AUC@3</th>\n",
       "      <th>AUC@4</th>\n",
       "      <th>AUC@5</th>\n",
       "      <th>HitRate@2</th>\n",
       "      <th>HitRate@3</th>\n",
       "      <th>HitRate@4</th>\n",
       "      <th>HitRate@5</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Training_Time</th>\n",
       "      <th>Prediction_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.9708</td>\n",
       "      <td>81.6785</td>\n",
       "      <td>41.8526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>8.8697</td>\n",
       "      <td>9.9435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0259</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.9229</td>\n",
       "      <td>7.0481</td>\n",
       "      <td>17.3415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model   MAP@2   MAP@3   MAP@4   MAP@5  Precision@2  Precision@3  \\\n",
       "3   Random Forest  0.0103  0.0113  0.0118  0.0121       0.0108       0.0083   \n",
       "7         XGBoost  0.0222  0.0228  0.0232  0.0234       0.0125       0.0091   \n",
       "11       LightGBM  0.0213  0.0219  0.0223  0.0225       0.0123       0.0090   \n",
       "\n",
       "    Precision@4  Precision@5   AUC@2   AUC@3   AUC@4   AUC@5  HitRate@2  \\\n",
       "3        0.0068       0.0058  0.0000  0.0105  0.0147  0.0172     0.0216   \n",
       "7        0.0072       0.0060  0.0216  0.0225  0.0232  0.0238     0.0244   \n",
       "11       0.0072       0.0059  0.0202  0.0216  0.0225  0.0233     0.0239   \n",
       "\n",
       "    HitRate@3  HitRate@4  HitRate@5  Test_Accuracy  Training_Time  \\\n",
       "3      0.0244     0.0255     0.0261         0.9708        81.6785   \n",
       "7      0.0254     0.0261     0.0265         0.9715         8.8697   \n",
       "11     0.0252     0.0259     0.0263         0.9229         7.0481   \n",
       "\n",
       "    Prediction_Time  \n",
       "3           41.8526  \n",
       "7            9.9435  \n",
       "11          17.3415  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç DETAILED PERFORMANCE ANALYSIS:\n",
      "================================================================================\n",
      "\n",
      "ü§ñ RANDOM FOREST PERFORMANCE:\n",
      "------------------------------------------------------------\n",
      "   üìà MAP Scores:\n",
      "      MAP@2: 0.0103\n",
      "      MAP@3: 0.0113\n",
      "      MAP@4: 0.0118\n",
      "      MAP@5: 0.0121\n",
      "   üéØ Precision Scores:\n",
      "      Precision@2: 0.0108\n",
      "      Precision@3: 0.0083\n",
      "      Precision@4: 0.0068\n",
      "      Precision@5: 0.0058\n",
      "   üìä AUC Scores:\n",
      "      AUC@2: 0.0000\n",
      "      AUC@3: 0.0105\n",
      "      AUC@4: 0.0147\n",
      "      AUC@5: 0.0172\n",
      "   üé≤ Hit Rate Scores:\n",
      "      HitRate@2: 0.0216\n",
      "      HitRate@3: 0.0244\n",
      "      HitRate@4: 0.0255\n",
      "      HitRate@5: 0.0261\n",
      "   ‚ö° Performance Metrics:\n",
      "      Test Accuracy: 0.9708\n",
      "      Training Time: 81.68s\n",
      "      Prediction Time: 41.85s\n",
      "      Total Customers: 3,666,663\n",
      "\n",
      "ü§ñ XGBOOST PERFORMANCE:\n",
      "------------------------------------------------------------\n",
      "   üìà MAP Scores:\n",
      "      MAP@2: 0.0222\n",
      "      MAP@3: 0.0228\n",
      "      MAP@4: 0.0232\n",
      "      MAP@5: 0.0234\n",
      "   üéØ Precision Scores:\n",
      "      Precision@2: 0.0125\n",
      "      Precision@3: 0.0091\n",
      "      Precision@4: 0.0072\n",
      "      Precision@5: 0.0060\n",
      "   üìä AUC Scores:\n",
      "      AUC@2: 0.0216\n",
      "      AUC@3: 0.0225\n",
      "      AUC@4: 0.0232\n",
      "      AUC@5: 0.0238\n",
      "   üé≤ Hit Rate Scores:\n",
      "      HitRate@2: 0.0244\n",
      "      HitRate@3: 0.0254\n",
      "      HitRate@4: 0.0261\n",
      "      HitRate@5: 0.0265\n",
      "   ‚ö° Performance Metrics:\n",
      "      Test Accuracy: 0.9715\n",
      "      Training Time: 8.87s\n",
      "      Prediction Time: 9.94s\n",
      "      Total Customers: 3,666,663\n",
      "\n",
      "ü§ñ LIGHTGBM PERFORMANCE:\n",
      "------------------------------------------------------------\n",
      "   üìà MAP Scores:\n",
      "      MAP@2: 0.0213\n",
      "      MAP@3: 0.0219\n",
      "      MAP@4: 0.0223\n",
      "      MAP@5: 0.0225\n",
      "   üéØ Precision Scores:\n",
      "      Precision@2: 0.0123\n",
      "      Precision@3: 0.0090\n",
      "      Precision@4: 0.0072\n",
      "      Precision@5: 0.0059\n",
      "   üìä AUC Scores:\n",
      "      AUC@2: 0.0202\n",
      "      AUC@3: 0.0216\n",
      "      AUC@4: 0.0225\n",
      "      AUC@5: 0.0233\n",
      "   üé≤ Hit Rate Scores:\n",
      "      HitRate@2: 0.0239\n",
      "      HitRate@3: 0.0252\n",
      "      HitRate@4: 0.0259\n",
      "      HitRate@5: 0.0263\n",
      "   ‚ö° Performance Metrics:\n",
      "      Test Accuracy: 0.9229\n",
      "      Training Time: 7.05s\n",
      "      Prediction Time: 17.34s\n",
      "      Total Customers: 3,666,663\n",
      "\n",
      "üìÖ MONTHLY PERFORMANCE BREAKDOWN:\n",
      "================================================================================\n",
      "\n",
      "üìä October 2015:\n",
      "----------------------------------------\n",
      "   XGBoost:\n",
      "      MAP@5=0.0268, Precision@5=0.0070\n",
      "      AUC@5=0.0273, HitRate@5=0.0306\n",
      "      Accuracy=0.9682\n",
      "\n",
      "   LightGBM:\n",
      "      MAP@5=0.0259, Precision@5=0.0069\n",
      "      AUC@5=0.0268, HitRate@5=0.0303\n",
      "      Accuracy=0.9235\n",
      "\n",
      "   Random Forest:\n",
      "      MAP@5=0.0140, Precision@5=0.0068\n",
      "      AUC@5=0.0197, HitRate@5=0.0301\n",
      "      Accuracy=0.9675\n",
      "\n",
      "\n",
      "üìä November 2015:\n",
      "----------------------------------------\n",
      "   XGBoost:\n",
      "      MAP@5=0.0246, Precision@5=0.0063\n",
      "      AUC@5=0.0251, HitRate@5=0.0281\n",
      "      Accuracy=0.9703\n",
      "\n",
      "   LightGBM:\n",
      "      MAP@5=0.0236, Precision@5=0.0062\n",
      "      AUC@5=0.0245, HitRate@5=0.0279\n",
      "      Accuracy=0.9165\n",
      "\n",
      "   Random Forest:\n",
      "      MAP@5=0.0128, Precision@5=0.0061\n",
      "      AUC@5=0.0182, HitRate@5=0.0277\n",
      "      Accuracy=0.9696\n",
      "\n",
      "\n",
      "üìä December 2015:\n",
      "----------------------------------------\n",
      "   XGBoost:\n",
      "      MAP@5=0.0199, Precision@5=0.0050\n",
      "      AUC@5=0.0202, HitRate@5=0.0223\n",
      "      Accuracy=0.9760\n",
      "\n",
      "   LightGBM:\n",
      "      MAP@5=0.0191, Precision@5=0.0049\n",
      "      AUC@5=0.0197, HitRate@5=0.0221\n",
      "      Accuracy=0.9287\n",
      "\n",
      "   Random Forest:\n",
      "      MAP@5=0.0103, Precision@5=0.0048\n",
      "      AUC@5=0.0146, HitRate@5=0.0219\n",
      "      Accuracy=0.9753\n",
      "\n",
      "\n",
      "üèÖ BEST MODEL IDENTIFICATION:\n",
      "================================================================================\n",
      "üèÜ BEST PERFORMANCE BY METRIC:\n",
      "--------------------------------------------------\n",
      "   MAP@2: XGBoost (0.0222)\n",
      "   MAP@3: XGBoost (0.0228)\n",
      "   MAP@4: XGBoost (0.0232)\n",
      "   MAP@5: XGBoost (0.0234)\n",
      "   Precision@2: XGBoost (0.0125)\n",
      "   Precision@3: XGBoost (0.0091)\n",
      "   Precision@4: XGBoost (0.0072)\n",
      "   Precision@5: XGBoost (0.0060)\n",
      "   AUC@2: XGBoost (0.0216)\n",
      "   AUC@3: XGBoost (0.0225)\n",
      "   AUC@4: XGBoost (0.0232)\n",
      "   AUC@5: XGBoost (0.0238)\n",
      "   HitRate@2: XGBoost (0.0244)\n",
      "   HitRate@3: XGBoost (0.0254)\n",
      "   HitRate@4: XGBoost (0.0261)\n",
      "   HitRate@5: XGBoost (0.0265)\n",
      "   Test_Accuracy: XGBoost (0.9715)\n",
      "\n",
      "üéñÔ∏è OVERALL WINNER: XGBoost (17 best scores out of 17)\n",
      "\n",
      "üìä WINS BREAKDOWN:\n",
      "------------------------------\n",
      "   XGBoost: 17 wins (100.0%)\n",
      "\n",
      "üí° PERFORMANCE INSIGHTS:\n",
      "================================================================================\n",
      "üìà MAP@k Trends:\n",
      "------------------------------\n",
      "   Random Forest: üìà Improving (MAP@2=0.010 ‚Üí MAP@5=0.012)\n",
      "   XGBoost: üìà Improving (MAP@2=0.022 ‚Üí MAP@5=0.023)\n",
      "   LightGBM: üìà Improving (MAP@2=0.021 ‚Üí MAP@5=0.023)\n",
      "\n",
      "üìä AUC@k Trends:\n",
      "------------------------------\n",
      "   Random Forest: üìà Improving (AUC@2=0.000 ‚Üí AUC@5=0.017)\n",
      "   XGBoost: üìà Improving (AUC@2=0.022 ‚Üí AUC@5=0.024)\n",
      "   LightGBM: üìà Improving (AUC@2=0.020 ‚Üí AUC@5=0.023)\n",
      "\n",
      "üé≤ Hit Rate@k Trends:\n",
      "------------------------------\n",
      "   Random Forest: üìà Improving (HitRate@2=0.022 ‚Üí HitRate@5=0.026)\n",
      "   XGBoost: üìà Improving (HitRate@2=0.024 ‚Üí HitRate@5=0.027)\n",
      "   LightGBM: üìà Improving (HitRate@2=0.024 ‚Üí HitRate@5=0.026)\n",
      "\n",
      "‚ö° Speed vs Performance Analysis:\n",
      "----------------------------------------\n",
      "   Random Forest:\n",
      "      Prediction Efficiency: 0.29 (MAP@5 per ms)\n",
      "      Total Efficiency: 0.10 (MAP@5 per ms total)\n",
      "   XGBoost:\n",
      "      Prediction Efficiency: 2.35 (MAP@5 per ms)\n",
      "      Total Efficiency: 1.24 (MAP@5 per ms total)\n",
      "   LightGBM:\n",
      "      Prediction Efficiency: 1.30 (MAP@5 per ms)\n",
      "      Total Efficiency: 0.92 (MAP@5 per ms total)\n",
      "\n",
      "üîó METRIC CORRELATION ANALYSIS:\n",
      "================================================================================\n",
      "üìä High Correlations (>0.8) between metrics:\n",
      "--------------------------------------------------\n",
      "   MAP@2 ‚Üî Precision@2: 0.997\n",
      "   MAP@2 ‚Üî AUC@2: 1.000\n",
      "   MAP@2 ‚Üî HitRate@2: 0.996\n",
      "   MAP@2 ‚Üî MAP@3: 1.000\n",
      "   MAP@2 ‚Üî Precision@3: 0.997\n",
      "   MAP@2 ‚Üî AUC@3: 1.000\n",
      "   MAP@2 ‚Üî HitRate@3: 0.986\n",
      "   MAP@2 ‚Üî MAP@4: 1.000\n",
      "   MAP@2 ‚Üî Precision@4: 0.998\n",
      "   MAP@2 ‚Üî AUC@4: 1.000\n",
      "   MAP@2 ‚Üî HitRate@4: 0.991\n",
      "   MAP@2 ‚Üî MAP@5: 1.000\n",
      "   MAP@2 ‚Üî Precision@5: 0.957\n",
      "   MAP@2 ‚Üî AUC@5: 1.000\n",
      "   MAP@2 ‚Üî HitRate@5: 0.868\n",
      "   Precision@2 ‚Üî AUC@2: 0.996\n",
      "   Precision@2 ‚Üî HitRate@2: 1.000\n",
      "   Precision@2 ‚Üî MAP@3: 0.997\n",
      "   Precision@2 ‚Üî Precision@3: 1.000\n",
      "   Precision@2 ‚Üî AUC@3: 0.997\n",
      "   Precision@2 ‚Üî HitRate@3: 0.996\n",
      "   Precision@2 ‚Üî MAP@4: 0.997\n",
      "   Precision@2 ‚Üî Precision@4: 1.000\n",
      "   Precision@2 ‚Üî AUC@4: 0.997\n",
      "   Precision@2 ‚Üî HitRate@4: 0.998\n",
      "   Precision@2 ‚Üî MAP@5: 0.997\n",
      "   Precision@2 ‚Üî Precision@5: 0.976\n",
      "   Precision@2 ‚Üî AUC@5: 0.997\n",
      "   Precision@2 ‚Üî HitRate@5: 0.903\n",
      "   AUC@2 ‚Üî HitRate@2: 0.995\n",
      "   AUC@2 ‚Üî MAP@3: 1.000\n",
      "   AUC@2 ‚Üî Precision@3: 0.996\n",
      "   AUC@2 ‚Üî AUC@3: 1.000\n",
      "   AUC@2 ‚Üî HitRate@3: 0.984\n",
      "   AUC@2 ‚Üî MAP@4: 1.000\n",
      "   AUC@2 ‚Üî Precision@4: 0.997\n",
      "   AUC@2 ‚Üî AUC@4: 1.000\n",
      "   AUC@2 ‚Üî HitRate@4: 0.989\n",
      "   AUC@2 ‚Üî MAP@5: 1.000\n",
      "   AUC@2 ‚Üî Precision@5: 0.953\n",
      "   AUC@2 ‚Üî AUC@5: 1.000\n",
      "   AUC@2 ‚Üî HitRate@5: 0.861\n",
      "   HitRate@2 ‚Üî MAP@3: 0.996\n",
      "   HitRate@2 ‚Üî Precision@3: 1.000\n",
      "   HitRate@2 ‚Üî AUC@3: 0.996\n",
      "   HitRate@2 ‚Üî HitRate@3: 0.997\n",
      "   HitRate@2 ‚Üî MAP@4: 0.996\n",
      "   HitRate@2 ‚Üî Precision@4: 1.000\n",
      "   HitRate@2 ‚Üî AUC@4: 0.997\n",
      "   HitRate@2 ‚Üî HitRate@4: 0.999\n",
      "   HitRate@2 ‚Üî MAP@5: 0.996\n",
      "   HitRate@2 ‚Üî Precision@5: 0.978\n",
      "   HitRate@2 ‚Üî AUC@5: 0.997\n",
      "   HitRate@2 ‚Üî HitRate@5: 0.907\n",
      "   MAP@3 ‚Üî Precision@3: 0.997\n",
      "   MAP@3 ‚Üî AUC@3: 1.000\n",
      "   MAP@3 ‚Üî HitRate@3: 0.986\n",
      "   MAP@3 ‚Üî MAP@4: 1.000\n",
      "   MAP@3 ‚Üî Precision@4: 0.998\n",
      "   MAP@3 ‚Üî AUC@4: 1.000\n",
      "   MAP@3 ‚Üî HitRate@4: 0.991\n",
      "   MAP@3 ‚Üî MAP@5: 1.000\n",
      "   MAP@3 ‚Üî Precision@5: 0.956\n",
      "   MAP@3 ‚Üî AUC@5: 1.000\n",
      "   MAP@3 ‚Üî HitRate@5: 0.867\n",
      "   Precision@3 ‚Üî AUC@3: 0.997\n",
      "   Precision@3 ‚Üî HitRate@3: 0.995\n",
      "   Precision@3 ‚Üî MAP@4: 0.997\n",
      "   Precision@3 ‚Üî Precision@4: 1.000\n",
      "   Precision@3 ‚Üî AUC@4: 0.998\n",
      "   Precision@3 ‚Üî HitRate@4: 0.998\n",
      "   Precision@3 ‚Üî MAP@5: 0.997\n",
      "   Precision@3 ‚Üî Precision@5: 0.975\n",
      "   Precision@3 ‚Üî AUC@5: 0.998\n",
      "   Precision@3 ‚Üî HitRate@5: 0.902\n",
      "   AUC@3 ‚Üî HitRate@3: 0.986\n",
      "   AUC@3 ‚Üî MAP@4: 1.000\n",
      "   AUC@3 ‚Üî Precision@4: 0.998\n",
      "   AUC@3 ‚Üî AUC@4: 1.000\n",
      "   AUC@3 ‚Üî HitRate@4: 0.991\n",
      "   AUC@3 ‚Üî MAP@5: 1.000\n",
      "   AUC@3 ‚Üî Precision@5: 0.956\n",
      "   AUC@3 ‚Üî AUC@5: 1.000\n",
      "   AUC@3 ‚Üî HitRate@5: 0.868\n",
      "   HitRate@3 ‚Üî MAP@4: 0.985\n",
      "   HitRate@3 ‚Üî Precision@4: 0.995\n",
      "   HitRate@3 ‚Üî AUC@4: 0.987\n",
      "   HitRate@3 ‚Üî HitRate@4: 0.999\n",
      "   HitRate@3 ‚Üî MAP@5: 0.986\n",
      "   HitRate@3 ‚Üî Precision@5: 0.992\n",
      "   HitRate@3 ‚Üî AUC@5: 0.987\n",
      "   HitRate@3 ‚Üî HitRate@5: 0.939\n",
      "   MAP@4 ‚Üî Precision@4: 0.998\n",
      "   MAP@4 ‚Üî AUC@4: 1.000\n",
      "   MAP@4 ‚Üî HitRate@4: 0.990\n",
      "   MAP@4 ‚Üî MAP@5: 1.000\n",
      "   MAP@4 ‚Üî Precision@5: 0.956\n",
      "   MAP@4 ‚Üî AUC@5: 1.000\n",
      "   MAP@4 ‚Üî HitRate@5: 0.866\n",
      "   Precision@4 ‚Üî AUC@4: 0.998\n",
      "   Precision@4 ‚Üî HitRate@4: 0.998\n",
      "   Precision@4 ‚Üî MAP@5: 0.998\n",
      "   Precision@4 ‚Üî Precision@5: 0.974\n",
      "   Precision@4 ‚Üî AUC@5: 0.998\n",
      "   Precision@4 ‚Üî HitRate@5: 0.899\n",
      "   AUC@4 ‚Üî HitRate@4: 0.991\n",
      "   AUC@4 ‚Üî MAP@5: 1.000\n",
      "   AUC@4 ‚Üî Precision@5: 0.958\n",
      "   AUC@4 ‚Üî AUC@5: 1.000\n",
      "   AUC@4 ‚Üî HitRate@5: 0.870\n",
      "   HitRate@4 ‚Üî MAP@5: 0.991\n",
      "   HitRate@4 ‚Üî Precision@5: 0.987\n",
      "   HitRate@4 ‚Üî AUC@5: 0.992\n",
      "   HitRate@4 ‚Üî HitRate@5: 0.927\n",
      "   MAP@5 ‚Üî Precision@5: 0.956\n",
      "   MAP@5 ‚Üî AUC@5: 1.000\n",
      "   MAP@5 ‚Üî HitRate@5: 0.868\n",
      "   Precision@5 ‚Üî AUC@5: 0.958\n",
      "   Precision@5 ‚Üî HitRate@5: 0.975\n",
      "   AUC@5 ‚Üî HitRate@5: 0.871\n",
      "\n",
      "üéÅ RECOMMENDATION SUMMARY:\n",
      "================================================================================\n",
      "üåü RECOMMENDED MODEL: XGBoost\n",
      "   Primary Reason: Best MAP@5 score (0.0234)\n",
      "\n",
      "üìã Key Performance Metrics for XGBoost:\n",
      "--------------------------------------------------\n",
      "   k=2: MAP@2=0.0222, Precision@2=0.0125, AUC@2=0.0216, HitRate@2=0.0244\n",
      "   k=3: MAP@3=0.0228, Precision@3=0.0091, AUC@3=0.0225, HitRate@3=0.0254\n",
      "   k=4: MAP@4=0.0232, Precision@4=0.0072, AUC@4=0.0232, HitRate@4=0.0261\n",
      "   k=5: MAP@5=0.0234, Precision@5=0.0060, AUC@5=0.0238, HitRate@5=0.0265\n",
      "\n",
      "‚ö° Performance: Accuracy=0.9715, Prediction Time=9.94s\n",
      "\n",
      "üíº Business Impact Estimate:\n",
      "   Expected customers with good top-5 recommendations: 97,313 (2.7% of 3,666,663)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ COMPREHENSIVE ANALYSIS COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# COMPREHENSIVE MODEL COMPARISON AND SUMMARY - WITH ALL METRICS\n",
    "# =============================\n",
    "\n",
    "# Display final results\n",
    "print(\"üìä DETAILED RESULTS TABLE:\")\n",
    "print(\"=\"*80)\n",
    "display(final_results_df)\n",
    "\n",
    "# =============================\n",
    "# MODEL PERFORMANCE SUMMARY - ENHANCED WITH ALL METRICS\n",
    "# =============================\n",
    "\n",
    "print(\"\\nüèÜ MODEL PERFORMANCE SUMMARY:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall performance comparison (filter OVERALL rows only)\n",
    "overall_results = final_results_df[final_results_df['Month'] == 'OVERALL'].copy()\n",
    "\n",
    "if not overall_results.empty:\n",
    "    # Sort by MAP@5 (primary metric)\n",
    "    overall_results_sorted = overall_results.sort_values('MAP@5', ascending=False)\n",
    "    \n",
    "    print(\"\\nüéØ RANKING BY MAP@5:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (_, row) in enumerate(overall_results_sorted.iterrows(), 1):\n",
    "        print(f\"{i}. {row['Model']}: MAP@5 = {row['MAP@5']:.4f}\")\n",
    "    \n",
    "    # Comprehensive metrics comparison table\n",
    "    print(f\"\\nüìã COMPREHENSIVE METRICS COMPARISON:\")\n",
    "    comparison_metrics = ['Model', \n",
    "                         'MAP@2', 'MAP@3', 'MAP@4', 'MAP@5',\n",
    "                         'Precision@2', 'Precision@3', 'Precision@4', 'Precision@5',\n",
    "                         'AUC@2', 'AUC@3', 'AUC@4', 'AUC@5',\n",
    "                         'HitRate@2', 'HitRate@3', 'HitRate@4', 'HitRate@5',\n",
    "                         'Test_Accuracy', 'Training_Time', 'Prediction_Time']\n",
    "    \n",
    "    # Check which metrics are available\n",
    "    available_metrics = [metric for metric in comparison_metrics if metric in overall_results.columns]\n",
    "    summary_table = overall_results[available_metrics].round(4)\n",
    "    display(summary_table)\n",
    "\n",
    "# =============================\n",
    "# DETAILED PERFORMANCE ANALYSIS - ALL METRICS\n",
    "# =============================\n",
    "\n",
    "print(f\"\\nüîç DETAILED PERFORMANCE ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in overall_results['Model'].values:\n",
    "    print(f\"\\nü§ñ {model_name.upper()} PERFORMANCE:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    model_data = overall_results[overall_results['Model'] == model_name].iloc[0]\n",
    "    \n",
    "    # MAP@k scores\n",
    "    print(f\"   üìà MAP Scores:\")\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        if f'MAP@{k}' in model_data:\n",
    "            map_score = model_data[f'MAP@{k}']\n",
    "            print(f\"      MAP@{k}: {map_score:.4f}\")\n",
    "    \n",
    "    # Precision@k scores  \n",
    "    print(f\"   üéØ Precision Scores:\")\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        if f'Precision@{k}' in model_data:\n",
    "            precision_score = model_data[f'Precision@{k}']\n",
    "            print(f\"      Precision@{k}: {precision_score:.4f}\")\n",
    "    \n",
    "    # AUC@k scores\n",
    "    print(f\"   üìä AUC Scores:\")\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        if f'AUC@{k}' in model_data:\n",
    "            auc_score = model_data[f'AUC@{k}']\n",
    "            print(f\"      AUC@{k}: {auc_score:.4f}\")\n",
    "    \n",
    "    # Hit Rate@k scores\n",
    "    print(f\"   üé≤ Hit Rate Scores:\")\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        if f'HitRate@{k}' in model_data:\n",
    "            hitrate_score = model_data[f'HitRate@{k}']\n",
    "            print(f\"      HitRate@{k}: {hitrate_score:.4f}\")\n",
    "    \n",
    "    # Other metrics\n",
    "    print(f\"   ‚ö° Performance Metrics:\")\n",
    "    print(f\"      Test Accuracy: {model_data['Test_Accuracy']:.4f}\")\n",
    "    if 'Training_Time' in model_data:\n",
    "        print(f\"      Training Time: {model_data['Training_Time']:.2f}s\")\n",
    "    print(f\"      Prediction Time: {model_data['Prediction_Time']:.2f}s\")\n",
    "    print(f\"      Total Customers: {model_data['Total_Customers']:,}\")\n",
    "\n",
    "# =============================\n",
    "# MONTHLY PERFORMANCE BREAKDOWN - ALL METRICS\n",
    "# =============================\n",
    "\n",
    "print(f\"\\nüìÖ MONTHLY PERFORMANCE BREAKDOWN:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "monthly_results = final_results_df[final_results_df['Month'] != 'OVERALL'].copy()\n",
    "\n",
    "if not monthly_results.empty:\n",
    "    # Group by month and show key metrics for each model\n",
    "    for month in monthly_results['Month'].unique():\n",
    "        print(f\"\\nüìä {month}:\")\n",
    "        print(\"-\" * 40)\n",
    "        month_data = monthly_results[monthly_results['Month'] == month]\n",
    "        month_sorted = month_data.sort_values('MAP@5', ascending=False)\n",
    "        \n",
    "        for _, row in month_sorted.iterrows():\n",
    "            print(f\"   {row['Model']}:\")\n",
    "            print(f\"      MAP@5={row['MAP@5']:.4f}, Precision@5={row['Precision@5']:.4f}\")\n",
    "            if 'AUC@5' in row:\n",
    "                print(f\"      AUC@5={row['AUC@5']:.4f}, HitRate@5={row['HitRate@5']:.4f}\")\n",
    "            print(f\"      Accuracy={row['Test_Accuracy']:.4f}\")\n",
    "            print()\n",
    "\n",
    "# =============================\n",
    "# BEST MODEL IDENTIFICATION - ALL METRICS\n",
    "# =============================\n",
    "\n",
    "print(f\"\\nüèÖ BEST MODEL IDENTIFICATION:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not overall_results.empty:\n",
    "    # Best model by different metrics\n",
    "    best_models = {}\n",
    "    \n",
    "    metrics_to_check = []\n",
    "    # Add MAP metrics\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        if f'MAP@{k}' in overall_results.columns:\n",
    "            metrics_to_check.append(f'MAP@{k}')\n",
    "    \n",
    "    # Add Precision metrics\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        if f'Precision@{k}' in overall_results.columns:\n",
    "            metrics_to_check.append(f'Precision@{k}')\n",
    "    \n",
    "    # Add AUC metrics\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        if f'AUC@{k}' in overall_results.columns:\n",
    "            metrics_to_check.append(f'AUC@{k}')\n",
    "    \n",
    "    # Add Hit Rate metrics\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        if f'HitRate@{k}' in overall_results.columns:\n",
    "            metrics_to_check.append(f'HitRate@{k}')\n",
    "    \n",
    "    # Add other metrics\n",
    "    if 'Test_Accuracy' in overall_results.columns:\n",
    "        metrics_to_check.append('Test_Accuracy')\n",
    "    \n",
    "    print(f\"üèÜ BEST PERFORMANCE BY METRIC:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for metric in metrics_to_check:\n",
    "        best_idx = overall_results[metric].idxmax()\n",
    "        best_model = overall_results.loc[best_idx, 'Model']\n",
    "        best_score = overall_results.loc[best_idx, metric]\n",
    "        best_models[metric] = (best_model, best_score)\n",
    "        print(f\"   {metric}: {best_model} ({best_score:.4f})\")\n",
    "    \n",
    "    # Overall winner (most wins)\n",
    "    model_wins = {}\n",
    "    for metric, (model, score) in best_models.items():\n",
    "        model_wins[model] = model_wins.get(model, 0) + 1\n",
    "    \n",
    "    overall_winner = max(model_wins.items(), key=lambda x: x[1])\n",
    "    print(f\"\\nüéñÔ∏è OVERALL WINNER: {overall_winner[0]} ({overall_winner[1]} best scores out of {len(best_models)})\")\n",
    "    \n",
    "    # Show wins breakdown\n",
    "    print(f\"\\nüìä WINS BREAKDOWN:\")\n",
    "    print(\"-\" * 30)\n",
    "    sorted_wins = sorted(model_wins.items(), key=lambda x: x[1], reverse=True)\n",
    "    for model, wins in sorted_wins:\n",
    "        win_percentage = (wins / len(best_models)) * 100\n",
    "        print(f\"   {model}: {wins} wins ({win_percentage:.1f}%)\")\n",
    "\n",
    "# =============================\n",
    "# PERFORMANCE INSIGHTS - ENHANCED\n",
    "# =============================\n",
    "\n",
    "print(f\"\\nüí° PERFORMANCE INSIGHTS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not overall_results.empty:\n",
    "    # MAP@k trend analysis\n",
    "    print(f\"üìà MAP@k Trends:\")\n",
    "    print(\"-\" * 30)\n",
    "    for _, row in overall_results.iterrows():\n",
    "        model = row['Model']\n",
    "        map_scores = [row[f'MAP@{k}'] for k in [2, 3, 4, 5] if f'MAP@{k}' in row]\n",
    "        if len(map_scores) >= 2:\n",
    "            trend = \"üìà Improving\" if all(map_scores[i] <= map_scores[i+1] for i in range(len(map_scores)-1)) \\\n",
    "                    else \"üìâ Declining\" if all(map_scores[i] >= map_scores[i+1] for i in range(len(map_scores)-1)) \\\n",
    "                    else \"üìä Mixed\"\n",
    "            print(f\"   {model}: {trend} (MAP@2={map_scores[0]:.3f} ‚Üí MAP@5={map_scores[-1]:.3f})\")\n",
    "    \n",
    "    # AUC@k trend analysis\n",
    "    if any(f'AUC@{k}' in overall_results.columns for k in [2, 3, 4, 5]):\n",
    "        print(f\"\\nüìä AUC@k Trends:\")\n",
    "        print(\"-\" * 30)\n",
    "        for _, row in overall_results.iterrows():\n",
    "            model = row['Model']\n",
    "            auc_scores = [row[f'AUC@{k}'] for k in [2, 3, 4, 5] if f'AUC@{k}' in row]\n",
    "            if len(auc_scores) >= 2:\n",
    "                trend = \"üìà Improving\" if all(auc_scores[i] <= auc_scores[i+1] for i in range(len(auc_scores)-1)) \\\n",
    "                        else \"üìâ Declining\" if all(auc_scores[i] >= auc_scores[i+1] for i in range(len(auc_scores)-1)) \\\n",
    "                        else \"üìä Mixed\"\n",
    "                print(f\"   {model}: {trend} (AUC@2={auc_scores[0]:.3f} ‚Üí AUC@5={auc_scores[-1]:.3f})\")\n",
    "    \n",
    "    # Hit Rate@k trend analysis\n",
    "    if any(f'HitRate@{k}' in overall_results.columns for k in [2, 3, 4, 5]):\n",
    "        print(f\"\\nüé≤ Hit Rate@k Trends:\")\n",
    "        print(\"-\" * 30)\n",
    "        for _, row in overall_results.iterrows():\n",
    "            model = row['Model']\n",
    "            hitrate_scores = [row[f'HitRate@{k}'] for k in [2, 3, 4, 5] if f'HitRate@{k}' in row]\n",
    "            if len(hitrate_scores) >= 2:\n",
    "                trend = \"üìà Improving\" if all(hitrate_scores[i] <= hitrate_scores[i+1] for i in range(len(hitrate_scores)-1)) \\\n",
    "                        else \"üìâ Declining\" if all(hitrate_scores[i] >= hitrate_scores[i+1] for i in range(len(hitrate_scores)-1)) \\\n",
    "                        else \"üìä Mixed\"\n",
    "                print(f\"   {model}: {trend} (HitRate@2={hitrate_scores[0]:.3f} ‚Üí HitRate@5={hitrate_scores[-1]:.3f})\")\n",
    "    \n",
    "    # Speed vs Accuracy analysis\n",
    "    print(f\"\\n‚ö° Speed vs Performance Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    for _, row in overall_results.iterrows():\n",
    "        model = row['Model']\n",
    "        prediction_time = row['Prediction_Time']\n",
    "        map5_score = row['MAP@5']\n",
    "        \n",
    "        # Calculate efficiency metrics\n",
    "        efficiency_ratio = map5_score / prediction_time * 1000 if prediction_time > 0 else 0\n",
    "        \n",
    "        if 'Training_Time' in row:\n",
    "            training_time = row['Training_Time']\n",
    "            total_efficiency = map5_score / (training_time + prediction_time) * 1000 if (training_time + prediction_time) > 0 else 0\n",
    "            print(f\"   {model}:\")\n",
    "            print(f\"      Prediction Efficiency: {efficiency_ratio:.2f} (MAP@5 per ms)\")\n",
    "            print(f\"      Total Efficiency: {total_efficiency:.2f} (MAP@5 per ms total)\")\n",
    "        else:\n",
    "            print(f\"   {model}: Prediction Efficiency = {efficiency_ratio:.2f} (MAP@5 per ms)\")\n",
    "\n",
    "# =============================\n",
    "# METRIC CORRELATION ANALYSIS\n",
    "# =============================\n",
    "\n",
    "print(f\"\\nüîó METRIC CORRELATION ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not overall_results.empty and len(overall_results) > 1:\n",
    "    import numpy as np\n",
    "    \n",
    "    # Get all numerical metric columns\n",
    "    metric_columns = []\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        for metric_type in ['MAP', 'Precision', 'AUC', 'HitRate']:\n",
    "            col_name = f'{metric_type}@{k}'\n",
    "            if col_name in overall_results.columns:\n",
    "                metric_columns.append(col_name)\n",
    "    \n",
    "    if 'Test_Accuracy' in overall_results.columns:\n",
    "        metric_columns.append('Test_Accuracy')\n",
    "    \n",
    "    if len(metric_columns) >= 2:\n",
    "        print(f\"üìä High Correlations (>0.8) between metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = overall_results[metric_columns].corr()\n",
    "        \n",
    "        # Find high correlations\n",
    "        high_correlations = []\n",
    "        for i in range(len(metric_columns)):\n",
    "            for j in range(i+1, len(metric_columns)):\n",
    "                corr_value = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_value) > 0.8:\n",
    "                    high_correlations.append((metric_columns[i], metric_columns[j], corr_value))\n",
    "        \n",
    "        if high_correlations:\n",
    "            for metric1, metric2, corr in high_correlations:\n",
    "                print(f\"   {metric1} ‚Üî {metric2}: {corr:.3f}\")\n",
    "        else:\n",
    "            print(\"   No high correlations found (all correlations < 0.8)\")\n",
    "\n",
    "# =============================\n",
    "# RECOMMENDATION SUMMARY\n",
    "# =============================\n",
    "\n",
    "print(f\"\\nüéÅ RECOMMENDATION SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not overall_results.empty:\n",
    "    # Best overall model\n",
    "    best_map5_model = overall_results.loc[overall_results['MAP@5'].idxmax(), 'Model']\n",
    "    best_map5_score = overall_results['MAP@5'].max()\n",
    "    \n",
    "    print(f\"üåü RECOMMENDED MODEL: {best_map5_model}\")\n",
    "    print(f\"   Primary Reason: Best MAP@5 score ({best_map5_score:.4f})\")\n",
    "    \n",
    "    # Additional insights\n",
    "    best_model_data = overall_results[overall_results['Model'] == best_map5_model].iloc[0]\n",
    "    \n",
    "    print(f\"\\nüìã Key Performance Metrics for {best_map5_model}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Show all available metrics for the best model\n",
    "    for k in [2, 3, 4, 5]:\n",
    "        metrics_line = []\n",
    "        if f'MAP@{k}' in best_model_data:\n",
    "            metrics_line.append(f\"MAP@{k}={best_model_data[f'MAP@{k}']:.4f}\")\n",
    "        if f'Precision@{k}' in best_model_data:\n",
    "            metrics_line.append(f\"Precision@{k}={best_model_data[f'Precision@{k}']:.4f}\")\n",
    "        if f'AUC@{k}' in best_model_data:\n",
    "            metrics_line.append(f\"AUC@{k}={best_model_data[f'AUC@{k}']:.4f}\")\n",
    "        if f'HitRate@{k}' in best_model_data:\n",
    "            metrics_line.append(f\"HitRate@{k}={best_model_data[f'HitRate@{k}']:.4f}\")\n",
    "        \n",
    "        if metrics_line:\n",
    "            print(f\"   k={k}: {', '.join(metrics_line)}\")\n",
    "    \n",
    "    print(f\"\\n‚ö° Performance: Accuracy={best_model_data['Test_Accuracy']:.4f}, \"\n",
    "          f\"Prediction Time={best_model_data['Prediction_Time']:.2f}s\")\n",
    "    \n",
    "    # Business impact estimation\n",
    "    total_customers = int(best_model_data['Total_Customers'])\n",
    "    if 'HitRate@5' in best_model_data:\n",
    "        hit_rate = best_model_data['HitRate@5']\n",
    "        estimated_customers_with_good_recommendations = int(total_customers * hit_rate)\n",
    "        print(f\"\\nüíº Business Impact Estimate:\")\n",
    "        print(f\"   Expected customers with good top-5 recommendations: {estimated_customers_with_good_recommendations:,} \"\n",
    "              f\"({hit_rate*100:.1f}% of {total_customers:,})\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ COMPREHENSIVE ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c950b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ COMPREHENSIVE METRICS VISUALIZATION DEMO\n",
      "============================================================\n",
      "\n",
      "    Usage:\n",
      "\n",
      "    # Visualize all metrics\n",
      "    visualize_metrics_comprehensive(\n",
      "        results_df=final_results_df,\n",
      "        save_path='comprehensive_metrics_dashboard.png'\n",
      "    )\n",
      "\n",
      "    # Create summary table\n",
      "    summary_table = create_metrics_summary_table(final_results_df)\n",
      "\n",
      "    Features:\n",
      "    üìà MAP@k trends across months and models\n",
      "    üìä AUC@k performance comparison  \n",
      "    üé≤ Hit Rate@k analysis\n",
      "    üéØ Precision@k evaluation\n",
      "    üèÜ Overall performance comparison\n",
      "    ‚ö° Speed vs accuracy analysis\n",
      "    üîó Metrics correlation heatmap\n",
      "    üìÖ Monthly trend analysis\n",
      "    üíº Business impact estimation\n",
      "    üìä Comprehensive summary table\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def visualize_metrics_comprehensive(results_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Comprehensive visualization of all recommendation metrics\n",
    "    \n",
    "    Parameters:\n",
    "    results_df : pd.DataFrame - Results dataframe with all metrics\n",
    "    save_path : str - Path to save the visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Filter data\n",
    "    monthly_data = results_df[results_df['Month'] != 'OVERALL'].copy()\n",
    "    overall_data = results_df[results_df['Month'] == 'OVERALL'].copy()\n",
    "    \n",
    "    if monthly_data.empty:\n",
    "        print(\"No monthly data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Create comprehensive dashboard\n",
    "    fig = plt.figure(figsize=(24, 20))\n",
    "    gs = GridSpec(4, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    fig.suptitle('üìä Comprehensive Recommendation System Performance Dashboard', \n",
    "                 fontsize=20, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # =============================\n",
    "    # 1. MAP@k COMPARISON (Top Left)\n",
    "    # =============================\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    map_cols = [col for col in monthly_data.columns if col.startswith('MAP@')]\n",
    "    \n",
    "    if map_cols:\n",
    "        for col in map_cols:\n",
    "            for model in monthly_data['Model'].unique():\n",
    "                model_data = monthly_data[monthly_data['Model'] == model]\n",
    "                ax1.plot(model_data['Month'], model_data[col], \n",
    "                        marker='o', linewidth=2, markersize=6,\n",
    "                        label=f\"{model} {col}\", alpha=0.8)\n",
    "        \n",
    "        ax1.set_title('üìà Mean Average Precision @k', fontweight='bold', fontsize=14)\n",
    "        ax1.set_xlabel('Month', fontweight='bold')\n",
    "        ax1.set_ylabel('MAP Score', fontweight='bold')\n",
    "        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        plt.setp(ax1.get_xticklabels(), rotation=45)\n",
    "        ax1.set_ylim(0, max(monthly_data[map_cols].max()) * 1.1)\n",
    "    \n",
    "    # =============================\n",
    "    # 2. AUC@k COMPARISON (Top Middle)\n",
    "    # =============================\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    auc_cols = [col for col in monthly_data.columns if col.startswith('AUC@')]\n",
    "    \n",
    "    if auc_cols:\n",
    "        for col in auc_cols:\n",
    "            for model in monthly_data['Model'].unique():\n",
    "                model_data = monthly_data[monthly_data['Model'] == model]\n",
    "                ax2.plot(model_data['Month'], model_data[col], \n",
    "                        marker='s', linewidth=2, markersize=6,\n",
    "                        label=f\"{model} {col}\", alpha=0.8)\n",
    "        \n",
    "        ax2.set_title('üìä Area Under Curve @k', fontweight='bold', fontsize=14)\n",
    "        ax2.set_xlabel('Month', fontweight='bold')\n",
    "        ax2.set_ylabel('AUC Score', fontweight='bold')\n",
    "        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        plt.setp(ax2.get_xticklabels(), rotation=45)\n",
    "        ax2.set_ylim(0, 1)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'AUC@k metrics\\nnot available', \n",
    "                ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "        ax2.set_title('üìä Area Under Curve @k', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # =============================\n",
    "    # 3. HIT RATE@k COMPARISON (Top Right)\n",
    "    # =============================\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    hitrate_cols = [col for col in monthly_data.columns if col.startswith('HitRate@')]\n",
    "    \n",
    "    if hitrate_cols:\n",
    "        for col in hitrate_cols:\n",
    "            for model in monthly_data['Model'].unique():\n",
    "                model_data = monthly_data[monthly_data['Model'] == model]\n",
    "                ax3.plot(model_data['Month'], model_data[col], \n",
    "                        marker='^', linewidth=2, markersize=6,\n",
    "                        label=f\"{model} {col}\", alpha=0.8)\n",
    "        \n",
    "        ax3.set_title('üé≤ Hit Rate @k', fontweight='bold', fontsize=14)\n",
    "        ax3.set_xlabel('Month', fontweight='bold')\n",
    "        ax3.set_ylabel('Hit Rate', fontweight='bold')\n",
    "        ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        plt.setp(ax3.get_xticklabels(), rotation=45)\n",
    "        ax3.set_ylim(0, 1)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Hit Rate@k metrics\\nnot available', \n",
    "                ha='center', va='center', transform=ax3.transAxes, fontsize=12)\n",
    "        ax3.set_title('üé≤ Hit Rate @k', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # =============================\n",
    "    # 4. PRECISION@k COMPARISON (Second Row Left)\n",
    "    # =============================\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    precision_cols = [col for col in monthly_data.columns if col.startswith('Precision@')]\n",
    "    \n",
    "    if precision_cols:\n",
    "        for col in precision_cols:\n",
    "            for model in monthly_data['Model'].unique():\n",
    "                model_data = monthly_data[monthly_data['Model'] == model]\n",
    "                ax4.plot(model_data['Month'], model_data[col], \n",
    "                        marker='d', linewidth=2, markersize=6,\n",
    "                        label=f\"{model} {col}\", alpha=0.8)\n",
    "        \n",
    "        ax4.set_title('üéØ Precision @k', fontweight='bold', fontsize=14)\n",
    "        ax4.set_xlabel('Month', fontweight='bold')\n",
    "        ax4.set_ylabel('Precision Score', fontweight='bold')\n",
    "        ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        plt.setp(ax4.get_xticklabels(), rotation=45)\n",
    "        ax4.set_ylim(0, max(monthly_data[precision_cols].max()) * 1.1)\n",
    "    \n",
    "    # =============================\n",
    "    # 5. OVERALL PERFORMANCE COMPARISON (Second Row Middle)\n",
    "    # =============================\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    if not overall_data.empty:\n",
    "        models = overall_data['Model'].values\n",
    "        x_pos = np.arange(len(models))\n",
    "        \n",
    "        # Create grouped bar chart for key metrics\n",
    "        width = 0.15\n",
    "        metrics_to_plot = []\n",
    "        \n",
    "        # Get available metrics\n",
    "        for metric in ['MAP@5', 'Precision@5', 'AUC@5', 'HitRate@5']:\n",
    "            if metric in overall_data.columns:\n",
    "                metrics_to_plot.append(metric)\n",
    "        \n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(metrics_to_plot)))\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            values = overall_data[metric].values\n",
    "            ax5.bar(x_pos + i*width, values, width, \n",
    "                   label=metric, alpha=0.8, color=colors[i])\n",
    "        \n",
    "        ax5.set_title('üèÜ Overall Performance @k=5', fontweight='bold', fontsize=14)\n",
    "        ax5.set_xlabel('Models', fontweight='bold')\n",
    "        ax5.set_ylabel('Score', fontweight='bold')\n",
    "        ax5.set_xticks(x_pos + width * (len(metrics_to_plot)-1) / 2)\n",
    "        ax5.set_xticklabels(models, rotation=45)\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3, axis='y')\n",
    "        ax5.set_ylim(0, 1)\n",
    "    \n",
    "    # =============================\n",
    "    # 6. PERFORMANCE vs SPEED ANALYSIS (Second Row Right)\n",
    "    # =============================\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    \n",
    "    if not overall_data.empty and 'MAP@5' in overall_data.columns:\n",
    "        x_data = overall_data['Prediction_Time']\n",
    "        y_data = overall_data['MAP@5']\n",
    "        models = overall_data['Model']\n",
    "        \n",
    "        scatter = ax6.scatter(x_data, y_data, s=100, alpha=0.7, c=range(len(models)), cmap='viridis')\n",
    "        \n",
    "        # Add model labels\n",
    "        for i, model in enumerate(models):\n",
    "            ax6.annotate(model, (x_data.iloc[i], y_data.iloc[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "        \n",
    "        ax6.set_title('‚ö° Performance vs Speed', fontweight='bold', fontsize=14)\n",
    "        ax6.set_xlabel('Prediction Time (seconds)', fontweight='bold')\n",
    "        ax6.set_ylabel('MAP@5 Score', fontweight='bold')\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # =============================\n",
    "    # 7. METRICS CORRELATION HEATMAP (Third Row Left)\n",
    "    # =============================\n",
    "    ax7 = fig.add_subplot(gs[2, 0])\n",
    "    \n",
    "    if not overall_data.empty:\n",
    "        # Get numerical columns for correlation\n",
    "        numerical_cols = []\n",
    "        for k in [2, 3, 4, 5]:\n",
    "            for metric_type in ['MAP', 'Precision', 'AUC', 'HitRate']:\n",
    "                col_name = f'{metric_type}@{k}'\n",
    "                if col_name in overall_data.columns:\n",
    "                    numerical_cols.append(col_name)\n",
    "        \n",
    "        if len(numerical_cols) >= 2:\n",
    "            corr_matrix = overall_data[numerical_cols].corr()\n",
    "            \n",
    "            # Create heatmap\n",
    "            sns.heatmap(corr_matrix, annot=True, cmap='RdYlBu_r', center=0, \n",
    "                       square=True, ax=ax7, cbar_kws={'shrink': 0.8},\n",
    "                       fmt='.2f', annot_kws={'size': 8})\n",
    "            \n",
    "            ax7.set_title('üîó Metrics Correlation Matrix', fontweight='bold', fontsize=14)\n",
    "            plt.setp(ax7.get_xticklabels(), rotation=45)\n",
    "            plt.setp(ax7.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    # =============================\n",
    "    # 8. K-VALUE PERFORMANCE ANALYSIS (Third Row Middle)\n",
    "    # =============================\n",
    "    ax8 = fig.add_subplot(gs[2, 1])\n",
    "    \n",
    "    if not overall_data.empty:\n",
    "        # Analyze how metrics change with k\n",
    "        k_values = [2, 3, 4, 5]\n",
    "        \n",
    "        for model in overall_data['Model'].values:\n",
    "            model_data = overall_data[overall_data['Model'] == model].iloc[0]\n",
    "            \n",
    "            # MAP@k progression\n",
    "            map_scores = []\n",
    "            for k in k_values:\n",
    "                if f'MAP@{k}' in model_data:\n",
    "                    map_scores.append(model_data[f'MAP@{k}'])\n",
    "                else:\n",
    "                    map_scores.append(np.nan)\n",
    "            \n",
    "            if not all(np.isnan(map_scores)):\n",
    "                ax8.plot(k_values, map_scores, marker='o', linewidth=2, \n",
    "                        label=f'{model} MAP@k', alpha=0.8)\n",
    "        \n",
    "        ax8.set_title('üìà MAP@k Progression', fontweight='bold', fontsize=14)\n",
    "        ax8.set_xlabel('k Value', fontweight='bold')\n",
    "        ax8.set_ylabel('MAP Score', fontweight='bold')\n",
    "        ax8.set_xticks(k_values)\n",
    "        ax8.legend()\n",
    "        ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # =============================\n",
    "    # 9. BUSINESS IMPACT VISUALIZATION (Third Row Right)\n",
    "    # =============================\n",
    "    ax9 = fig.add_subplot(gs[2, 2])\n",
    "    \n",
    "    if not overall_data.empty and 'HitRate@5' in overall_data.columns:\n",
    "        models = overall_data['Model'].values\n",
    "        hit_rates = overall_data['HitRate@5'].values\n",
    "        total_customers = overall_data['Total_Customers'].iloc[0] if 'Total_Customers' in overall_data.columns else 1000\n",
    "        \n",
    "        # Calculate estimated customers with good recommendations\n",
    "        estimated_customers = hit_rates * total_customers\n",
    "        \n",
    "        # Create bar chart\n",
    "        bars = ax9.bar(range(len(models)), estimated_customers, alpha=0.8, color='lightblue')\n",
    "        \n",
    "        # Add percentage labels on bars\n",
    "        for i, (bar, hit_rate) in enumerate(zip(bars, hit_rates)):\n",
    "            height = bar.get_height()\n",
    "            ax9.text(bar.get_x() + bar.get_width()/2., height + total_customers*0.01,\n",
    "                    f'{hit_rate:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax9.set_title('üíº Business Impact (Top-5 Recommendations)', fontweight='bold', fontsize=14)\n",
    "        ax9.set_xlabel('Models', fontweight='bold')\n",
    "        ax9.set_ylabel('Customers with Good Recommendations', fontweight='bold')\n",
    "        ax9.set_xticks(range(len(models)))\n",
    "        ax9.set_xticklabels(models, rotation=45)\n",
    "        ax9.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # =============================\n",
    "    # 10. MONTHLY TREND SUMMARY (Bottom Row - Full Width)\n",
    "    # =============================\n",
    "    ax10 = fig.add_subplot(gs[3, :])\n",
    "    \n",
    "    if not monthly_data.empty:\n",
    "        # Create summary trend chart\n",
    "        months = monthly_data['Month'].unique()\n",
    "        models = monthly_data['Model'].unique()\n",
    "        \n",
    "        x_pos = np.arange(len(months))\n",
    "        width = 0.8 / len(models)\n",
    "        \n",
    "        # Use MAP@5 as primary metric\n",
    "        if 'MAP@5' in monthly_data.columns:\n",
    "            for i, model in enumerate(models):\n",
    "                model_monthly = []\n",
    "                for month in months:\n",
    "                    month_data = monthly_data[(monthly_data['Month'] == month) & \n",
    "                                            (monthly_data['Model'] == model)]\n",
    "                    if not month_data.empty:\n",
    "                        model_monthly.append(month_data['MAP@5'].iloc[0])\n",
    "                    else:\n",
    "                        model_monthly.append(0)\n",
    "                \n",
    "                ax10.bar(x_pos + i*width, model_monthly, width, \n",
    "                        label=model, alpha=0.8)\n",
    "            \n",
    "            ax10.set_title('üìÖ Monthly Performance Comparison (MAP@5)', fontweight='bold', fontsize=16)\n",
    "            ax10.set_xlabel('Month', fontweight='bold')\n",
    "            ax10.set_ylabel('MAP@5 Score', fontweight='bold')\n",
    "            ax10.set_xticks(x_pos + width * (len(models)-1) / 2)\n",
    "            ax10.set_xticklabels(months, rotation=45)\n",
    "            ax10.legend()\n",
    "            ax10.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"üìä Visualization saved to: {save_path}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_metrics_summary_table(results_df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary table of all metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    overall_data = results_df[results_df['Month'] == 'OVERALL'].copy()\n",
    "    \n",
    "    if overall_data.empty:\n",
    "        print(\"No overall data available for summary table\")\n",
    "        return None\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    \n",
    "    for _, row in overall_data.iterrows():\n",
    "        model_summary = {'Model': row['Model']}\n",
    "        \n",
    "        # Add all available metrics\n",
    "        for k in [2, 3, 4, 5]:\n",
    "            for metric_type in ['MAP', 'Precision', 'AUC', 'HitRate']:\n",
    "                col_name = f'{metric_type}@{k}'\n",
    "                if col_name in row:\n",
    "                    model_summary[col_name] = row[col_name]\n",
    "        \n",
    "        # Add other metrics\n",
    "        if 'Test_Accuracy' in row:\n",
    "            model_summary['Accuracy'] = row['Test_Accuracy']\n",
    "        if 'Prediction_Time' in row:\n",
    "            model_summary['Pred_Time'] = row['Prediction_Time']\n",
    "        \n",
    "        summary_data.append(model_summary)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Style the table\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"üìä COMPREHENSIVE METRICS SUMMARY TABLE\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Display with proper formatting\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "    \n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "\n",
    "demo_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c02022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 'Random Forest' to models\\random_forest_20250820-101435.joblib\n",
      "‚ÑπÔ∏è  Metadata         ‚Üí models\\random_forest_20250820-101435.json\n",
      "‚úÖ Saved 'XGBoost' to models\\xgboost_20250820-101436.joblib\n",
      "‚ÑπÔ∏è  Metadata         ‚Üí models\\xgboost_20250820-101436.json\n",
      "‚úÖ Saved 'LightGBM' to models\\lightgbm_20250820-101436.joblib\n",
      "‚ÑπÔ∏è  Metadata         ‚Üí models\\lightgbm_20250820-101436.json\n",
      "\n",
      "üì¶ Saved models summary:\n",
      " - Random Forest: models\\random_forest_20250820-101435.joblib\n",
      " - XGBoost: models\\xgboost_20250820-101436.joblib\n",
      " - LightGBM: models\\lightgbm_20250820-101436.joblib\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "import sklearn\n",
    "\n",
    "# =============================\n",
    "# SAVE TRAINED PIPELINES\n",
    "# =============================\n",
    "\n",
    "MODELS_DIR = Path(\"models\")\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_pipeline(pipeline, model_name, y_columns=None, extra_meta=None):\n",
    "    \"\"\"Save a fitted sklearn Pipeline and a metadata JSON next to it.\"\"\"\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    safe_name = model_name.replace(\" \", \"_\").lower()\n",
    "    stem = f\"{safe_name}_{ts}\"\n",
    "    model_path = MODELS_DIR / f\"{stem}.joblib\"\n",
    "    meta_path = MODELS_DIR / f\"{stem}.json\"\n",
    "\n",
    "    joblib.dump(pipeline, model_path)\n",
    "\n",
    "    meta = {\n",
    "        \"model_name\": model_name,\n",
    "        \"created_at\": ts,\n",
    "        \"sklearn_version\": sklearn.__version__,\n",
    "        \"targets\": list(y_columns) if y_columns is not None else None,\n",
    "    }\n",
    "    if extra_meta:\n",
    "        meta.update(extra_meta)\n",
    "\n",
    "    meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2))\n",
    "\n",
    "    print(f\"‚úÖ Saved '{model_name}' to {model_path}\")\n",
    "    print(f\"‚ÑπÔ∏è  Metadata         ‚Üí {meta_path}\")\n",
    "    return str(model_path), str(meta_path)\n",
    "\n",
    "# Save all trained pipelines (from training_results)\n",
    "saved_artifacts = {}\n",
    "for model_name, info in training_results.items():\n",
    "    pipeline_fitted = info['pipeline']\n",
    "    train_acc = float(info.get('train_accuracy', 0.0))\n",
    "    train_time = float(info.get('training_time', 0.0))\n",
    "\n",
    "    model_path, meta_path = save_pipeline(\n",
    "        pipeline_fitted,\n",
    "        model_name,\n",
    "        y_columns=y_train.columns,\n",
    "        extra_meta={\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"training_time_s\": train_time\n",
    "        }\n",
    "    )\n",
    "    saved_artifacts[model_name] = {\"model\": model_path, \"meta\": meta_path}\n",
    "\n",
    "print(\"\\nüì¶ Saved models summary:\")\n",
    "for k, v in saved_artifacts.items():\n",
    "    print(f\" - {k}: {v['model']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4fc944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading data\\processed\\test_1_2016.csv ...\n",
      "üßπ Preprocessing to match training...\n",
      "Mem. usage decreased to 987.76 Mb (56.2% reduction)\n",
      "üßπ CLEANING DATASET...\n",
      "========================================\n",
      "1Ô∏è‚É£ Handling missing values...\n",
      "   ‚úÖ Filled payroll_final_label: 0 missing ‚Üí 0\n",
      "   ‚úÖ Filled pensions_2_final_label: 0 missing ‚Üí 0\n",
      "\n",
      "2Ô∏è‚É£ Creating customer tenure feature...\n",
      "   ‚úÖ Created 'customer_tenure_days' feature\n",
      "   üìä Range: -3 to 7682 days\n",
      "   üóëÔ∏è Dropped 'registration_date' column\n",
      "\n",
      "3Ô∏è‚É£ Converting last_primary_date to binary indicator...\n",
      "   ‚úÖ Created 'was_primary_customer' binary feature\n",
      "   üìä 6,151,862 nulls ‚Üí 0, 13,246 dates ‚Üí 1\n",
      "   üóëÔ∏è Dropped 'last_primary_date' column\n",
      "\n",
      "4Ô∏è‚É£ Removing constant and duplicate columns...\n",
      "   üóëÔ∏è Dropped 'address_type' (constant value)\n",
      "   üóëÔ∏è Dropped 'province_code' (duplicate of province_name)\n",
      "\n",
      "5Ô∏è‚É£ Cleaning numeric columns...\n",
      "   ‚úÖ Cleaned 'age': int16 ‚Üí numeric (0 nulls)\n",
      "   ‚ö†Ô∏è Converted 14 negative seniority values to NaN\n",
      "   ‚úÖ Cleaned 'seniority': int32 ‚Üí numeric (14 nulls)\n",
      "\n",
      "üìä CLEANUP SUMMARY:\n",
      "----------------------------------------\n",
      "   Final shape: (6165108, 46)\n",
      "   Memory usage: 3958.1 MB\n",
      "   Null values: 7,899,556\n",
      "\n",
      "üìã DATA TYPES AFTER CLEANUP:\n",
      "   int8: 24 columns\n",
      "   object: 12 columns\n",
      "   float16: 3 columns\n",
      "   int64: 2 columns\n",
      "   datetime64[ns]: 1 columns\n",
      "   int32: 1 columns\n",
      "   int16: 1 columns\n",
      "   float64: 1 columns\n",
      "   float32: 1 columns\n",
      "T·ªïng s·ªë kh√°ch h√†ng: 1,633,273\n",
      "Kh√°ch h√†ng KH√îNG s·ªü h·ªØu t√†i kho·∫£n thanh to√°n n√†o: 1,633,273\n",
      "\n",
      "üìä Distribution check:\n",
      "  current_accounts_final_label: [0 1]\n",
      "  payroll_accounts_final_label: [0 1]\n",
      "  junior_accounts_final_label: [0 1]\n",
      "  more_particular_accounts_final_label: [0 1]\n",
      "  particular_accounts_final_label: [0 1]\n",
      "  particular_plus_accounts_final_label: [0 1]\n",
      "  home_account_final_label: [0 1]\n",
      "  payroll_final_label: [0 1]\n",
      "  e_account_final_label: [0 1]\n",
      "üîß CREATING ENHANCED FEATURES...\n",
      "==================================================\n",
      "üìã Available columns: 25\n",
      "üìÖ Creating time-based features...\n",
      "üë• Creating demographic features...\n",
      "üè¶ Creating banking relationship features...\n",
      "üîó Creating interaction features...\n",
      "üéØ Creating behavioral features...\n",
      "‚öñÔ∏è Creating risk and stability features...\n",
      "‚úÖ Feature engineering completed!\n",
      "Original features: 25\n",
      "Enhanced features: 58\n",
      "New features added: 33\n",
      "üì¶ Loading latest model ‚Üí models\\lightgbm_20250820-100955.joblib\n",
      "\n",
      "üîÆ Generating top-2 recommendations for 1,633,273 users...\n",
      " - Done users 0 ‚Üí 30,000\n",
      " - Done users 30,000 ‚Üí 60,000\n",
      " - Done users 60,000 ‚Üí 90,000\n",
      " - Done users 90,000 ‚Üí 120,000\n",
      " - Done users 120,000 ‚Üí 150,000\n",
      " - Done users 150,000 ‚Üí 180,000\n",
      " - Done users 180,000 ‚Üí 210,000\n",
      " - Done users 210,000 ‚Üí 240,000\n",
      " - Done users 240,000 ‚Üí 270,000\n",
      " - Done users 270,000 ‚Üí 300,000\n",
      " - Done users 300,000 ‚Üí 330,000\n",
      " - Done users 330,000 ‚Üí 360,000\n",
      " - Done users 360,000 ‚Üí 390,000\n",
      " - Done users 390,000 ‚Üí 420,000\n",
      " - Done users 420,000 ‚Üí 450,000\n",
      " - Done users 450,000 ‚Üí 480,000\n",
      " - Done users 480,000 ‚Üí 510,000\n",
      " - Done users 510,000 ‚Üí 540,000\n",
      " - Done users 540,000 ‚Üí 570,000\n",
      " - Done users 570,000 ‚Üí 600,000\n",
      " - Done users 600,000 ‚Üí 630,000\n",
      " - Done users 630,000 ‚Üí 660,000\n",
      " - Done users 660,000 ‚Üí 690,000\n",
      " - Done users 690,000 ‚Üí 720,000\n",
      " - Done users 720,000 ‚Üí 750,000\n",
      " - Done users 750,000 ‚Üí 780,000\n",
      " - Done users 780,000 ‚Üí 810,000\n",
      " - Done users 810,000 ‚Üí 840,000\n",
      " - Done users 840,000 ‚Üí 870,000\n",
      " - Done users 870,000 ‚Üí 900,000\n",
      " - Done users 900,000 ‚Üí 930,000\n",
      " - Done users 930,000 ‚Üí 960,000\n",
      " - Done users 960,000 ‚Üí 990,000\n",
      " - Done users 990,000 ‚Üí 1,020,000\n",
      " - Done users 1,020,000 ‚Üí 1,050,000\n",
      " - Done users 1,050,000 ‚Üí 1,080,000\n",
      " - Done users 1,080,000 ‚Üí 1,110,000\n",
      " - Done users 1,110,000 ‚Üí 1,140,000\n",
      " - Done users 1,140,000 ‚Üí 1,170,000\n",
      " - Done users 1,170,000 ‚Üí 1,200,000\n",
      " - Done users 1,200,000 ‚Üí 1,230,000\n",
      " - Done users 1,230,000 ‚Üí 1,260,000\n",
      " - Done users 1,260,000 ‚Üí 1,290,000\n",
      " - Done users 1,290,000 ‚Üí 1,320,000\n",
      " - Done users 1,320,000 ‚Üí 1,350,000\n",
      " - Done users 1,350,000 ‚Üí 1,380,000\n",
      " - Done users 1,380,000 ‚Üí 1,410,000\n",
      " - Done users 1,410,000 ‚Üí 1,440,000\n",
      " - Done users 1,440,000 ‚Üí 1,470,000\n",
      " - Done users 1,470,000 ‚Üí 1,500,000\n",
      " - Done users 1,500,000 ‚Üí 1,530,000\n",
      " - Done users 1,530,000 ‚Üí 1,560,000\n",
      " - Done users 1,560,000 ‚Üí 1,590,000\n",
      " - Done users 1,590,000 ‚Üí 1,620,000\n",
      " - Done users 1,620,000 ‚Üí 1,633,273\n",
      "\n",
      "‚úÖ Saved top-2 recommendations ‚Üí predictions\\test_1_2016_top2.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# =============================\n",
    "# TOP-2 RECOMMENDATIONS PER USER FOR processed/test_1_2016.csv (SELF-CONTAINED)\n",
    "# =============================\n",
    "\n",
    "PRED_DIR = Path('predictions')\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEST_FILE = Path('data/processed/test_1_2016.csv')\n",
    "OUT_FILE = PRED_DIR / 'test_1_2016_top2.csv'\n",
    "\n",
    "# Helper to fetch latest saved model\n",
    "\n",
    "def get_latest_model_path(model_prefix=None, models_dir=\"models\"):\n",
    "    md = Path(models_dir)\n",
    "    if not md.exists():\n",
    "        raise FileNotFoundError(\"models directory not found. Train and save a model first.\")\n",
    "    candidates = list(md.glob(\"*.joblib\"))\n",
    "    if model_prefix:\n",
    "        prefix = model_prefix.lower().replace(\" \", \"_\")\n",
    "        candidates = [p for p in candidates if p.name.lower().startswith(prefix)]\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\"No saved .joblib models found.\")\n",
    "    latest = max(candidates, key=lambda p: p.stat().st_mtime)\n",
    "    return latest\n",
    "\n",
    "assert TEST_FILE.exists(), f\"Not found: {TEST_FILE}\"\n",
    "\n",
    "print(f\"üì• Loading {TEST_FILE} ...\")\n",
    "test_df = pd.read_csv(TEST_FILE)\n",
    "\n",
    "# Preprocess to match training\n",
    "print(\"üßπ Preprocessing to match training...\")\n",
    "test_df = reduce_memory_usage(test_df)\n",
    "test_df = clean_dataset(test_df)\n",
    "test_df = filter_data(test_df)\n",
    "test_df = create_enhanced_features(test_df)\n",
    "\n",
    "# Build X directly ensuring required columns exist\n",
    "_needed_num = list(available_num_cols)\n",
    "_needed_cat = list(available_cat_cols)\n",
    "for col in _needed_num:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = 0\n",
    "for col in _needed_cat:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = 'Unknown'\n",
    "X_all = test_df[_needed_num + _needed_cat]\n",
    "\n",
    "# Keep user ids\n",
    "customer_ids = test_df.loc[X_all.index, 'customer_id'] if 'customer_id' in test_df.columns else pd.Series(np.arange(len(X_all)), index=X_all.index)\n",
    "\n",
    "# Ensure model is loaded\n",
    "try:\n",
    "    loaded_pipeline\n",
    "except NameError:\n",
    "    latest_model = get_latest_model_path(None, models_dir='models')\n",
    "    print(f\"üì¶ Loading latest model ‚Üí {latest_model}\")\n",
    "    loaded_pipeline = joblib.load(latest_model)\n",
    "\n",
    "# Predict probabilities in batches and produce top-2\n",
    "chunk_size = 30000\n",
    "num_rows = len(X_all)\n",
    "print(f\"\\nüîÆ Generating top-2 recommendations for {num_rows:,} users...\")\n",
    "\n",
    "recs_rec1 = []\n",
    "recs_rec2 = []\n",
    "rec_indices = []\n",
    "\n",
    "def top2_from_proba(y_proba_batch, tar_cols):\n",
    "    n_rows = y_proba_batch[0].shape[0]\n",
    "    out = []\n",
    "    for i in range(n_rows):\n",
    "        scores = []\n",
    "        for j, product in enumerate(tar_cols):\n",
    "            proba_ij = y_proba_batch[j][i]\n",
    "            p1 = float(proba_ij[1]) if getattr(proba_ij, 'shape', None) and len(proba_ij) > 1 else float(proba_ij[0])\n",
    "            scores.append((product, p1))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top2 = [p for p, _ in scores[:2]]\n",
    "        if len(top2) < 2:\n",
    "            top2 += [''] * (2 - len(top2))\n",
    "        out.append((top2[0], top2[1]))\n",
    "    return out\n",
    "\n",
    "for start in range(0, num_rows, chunk_size):\n",
    "    end = min(start + chunk_size, num_rows)\n",
    "    idx = X_all.index[start:end]\n",
    "    X_batch = X_all.loc[idx]\n",
    "\n",
    "    try:\n",
    "        y_proba_batch = loaded_pipeline.predict_proba(X_batch)\n",
    "        top2_batch = top2_from_proba(y_proba_batch, tar_cols)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è predict_proba unavailable for rows {start}:{end}: {e}. Falling back to labels.\")\n",
    "        y_pred_batch = loaded_pipeline.predict(X_batch)\n",
    "        top2_batch = []\n",
    "        for row in y_pred_batch:\n",
    "            ones = [tar_cols[i] for i, v in enumerate(row) if v == 1]\n",
    "            zeros = [tar_cols[i] for i, v in enumerate(row) if v == 0]\n",
    "            ranked = (ones + zeros + ['',''])[:2]\n",
    "            top2_batch.append((ranked[0], ranked[1]))\n",
    "\n",
    "    rec1, rec2 = zip(*top2_batch)\n",
    "    recs_rec1.extend(rec1)\n",
    "    recs_rec2.extend(rec2)\n",
    "    rec_indices.extend(list(idx))\n",
    "    print(f\" - Done users {start:,} ‚Üí {end:,}\")\n",
    "\n",
    "# Assemble output\n",
    "out_df = pd.DataFrame({\n",
    "    'customer_id': customer_ids.loc[rec_indices].values,\n",
    "    'rec1': recs_rec1,\n",
    "    'rec2': recs_rec2,\n",
    "}, index=pd.Index(rec_indices, name='index')).reset_index(drop=True)\n",
    "\n",
    "out_df.to_csv(OUT_FILE, index=False)\n",
    "print(f\"\\n‚úÖ Saved top-2 recommendations ‚Üí {OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7692257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
